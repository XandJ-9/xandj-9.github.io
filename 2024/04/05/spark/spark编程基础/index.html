<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="1. RDD介绍​		RDD是Spark数据的基础单元，Spark编程是围绕着在RDD上创建和执行操作来进行的，它们是跨集群进行分区的不可变集合，如果某个分区丢失，这些分区可以重建(通过重新计算)。  RDD的特征 12345678* Internally, each RDD is characterized by five main properties:**  - A list of part">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2024/04/05/spark/spark%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="1. RDD介绍​		RDD是Spark数据的基础单元，Spark编程是围绕着在RDD上创建和执行操作来进行的，它们是跨集群进行分区的不可变集合，如果某个分区丢失，这些分区可以重建(通过重新计算)。  RDD的特征 12345678* Internally, each RDD is characterized by five main properties:**  - A list of part">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/image-20210623235829982.png">
<meta property="og:image" content="http://example.com/images/image-20210629001401157.png">
<meta property="og:image" content="http://example.com/images/image-20211026011823666.png">
<meta property="article:published_time" content="2024-04-05T14:53:22.935Z">
<meta property="article:modified_time" content="2024-04-05T14:53:22.935Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20210623235829982.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-spark/spark编程基础" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/spark/spark%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.935Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-RDD介绍"><a href="#1-RDD介绍" class="headerlink" title="1. RDD介绍"></a>1. RDD介绍</h1><p>​		RDD是Spark数据的基础单元，Spark编程是围绕着在RDD上创建和执行操作来进行的，它们是跨集群进行分区的不可变集合，如果某个分区丢失，这些分区可以重建(通过重新计算)。 </p>
<p>RDD的特征</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">* Internally, each RDD is characterized by five main properties:</span><br><span class="line">*</span><br><span class="line">*  - A list of partitions</span><br><span class="line">*  - A function for computing each split</span><br><span class="line">*  - A list of dependencies on other RDDs</span><br><span class="line">*  - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</span><br><span class="line">*  - Optionally, a list of preferred locations to compute each split on (e.g. block locations for</span><br><span class="line">*    an HDFS file)</span><br></pre></td></tr></table></figure>

<ol>
<li><p>分区的列表：</p>
<p> Partition(分区)是数据的基本组成单位，对于RDD来说，每个分区都会被一个计算任务来处理，用户可以在创建RDD时指定分区的个数，如果没有指定则采用默认值。</p>
</li>
<li><p>对每个分区都有计算函数对数据进行处理：</p>
<p> Spark中RDD的计算是以分区为单位的，每个RDD都会实现compute函数，compute函数会对迭代器进行组合，不需要保存每次计算的结果。</p>
</li>
<li><p>多个RDD之间存在依赖的关系，构成RDD之间的依赖列表：</p>
<p> RDD的每一次转换都会生成一个新的RDD，RDD之间形成类似流水线一样的前后依赖关系，在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p>
</li>
<li><p>key-values类型的RDD存在相应的分区器</p>
<p> 对于key-value的RDD而言，可能存在分区器(Partitioner)， Spark实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另一个是基于范围了RangePartitioner，只有key-value的RDD才有可能有Partitioner，非key-value类型的RDD的Partitioner值为None。Partitioner函数决定了RDD本身的分区数量，也决定了parent RDD Shuffle输出的分片数量。</p>
</li>
<li><p>优先计算的数据位置列表：</p>
<p> Spark任务调度时优先将计算移动到数据存储的位置。</p>
</li>
</ol>
<p>RDD的特点</p>
<ol>
<li><p>分区</p>
<p> RDD逻辑上分区，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。</p>
<p> 如果RDD是通过文件系统构建的，则compute函数是读取指定文件系统中的数据；</p>
<p> 如果RDD是通过其他的RDD转换而来，则comoute函数是执行转换逻辑将其他RDD的数据进行转换。</p>
</li>
<li><p>只读</p>
<p> RDD是只读的， 不可变的集合，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。</p>
<p> RDD通过丰富的转换算子操作来转换成另一个RDD，</p>
<p> RDD的算子包括两类:</p>
<ul>
<li>transformation 用来对RDD进行转换，该类算子是延迟执行(Lazy)</li>
<li>action 用来触发RDD的计算，得到相关计算结果或者将RDD保存在文件系统中。</li>
</ul>
</li>
<li><p>依赖关系</p>
<p> RDD通过操作算子进行转换，转换得到的新RDD包含了从其他RDD衍生所必须的信息， RDD之间维护者这种依赖的血缘关系(lineage)，也称之为依赖， 依赖包括两种：</p>
<ul>
<li>窄依赖 RDD之间分区是n:1或n:1的</li>
<li>宽依赖  子RDD的每个分区与父RDD的每个分区都有关，是n:m的关系， 一般是有shuffle的过程</li>
</ul>
</li>
</ol>
<p><img src="/./images/image-20210623235829982.png" alt="image-20210623235829982"></p>
<ol start="4">
<li><p>缓存</p>
<p> 可以控制存储的级别(内存，磁盘) 来进行缓存。</p>
<p> 应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到RDD的时候，会直接从缓存处获取而不用再根据血缘关系计算，这样就加速后期的重用。</p>
<p> <img src="/./images/image-20210629001401157.png" alt="image-20210629001401157"></p>
</li>
<li><p>checkpoint</p>
<p> ​		RDD的血缘关系使得它可以实现容错，当RDD的某个分区数据失败或缺失时，可以通过血缘关系来重建。但是对于长时间迭代的应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代中出错，那么需要通过非常长的血缘关系去重建，势必影响性能。</p>
<p> ​	RDD中支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道父代RDD了，可以直接从checkpoint处拿到数据。</p>
</li>
</ol>
<p>RDD与IO之间的关系</p>
<h1 id="2-Spark编程模型"><a href="#2-Spark编程模型" class="headerlink" title="2. Spark编程模型"></a>2. Spark编程模型</h1><p><img src="/./images/image-20211026011823666.png" alt="image-20211026011823666"></p>
<p>图上左边的各类数据源通过sparkContext连接，经过RDD的方法，转换算子和动作算子的计算最终转换成新的RDD或将结果输出到外部数据源。</p>
<ul>
<li>RDD表示数据对象</li>
<li>通过对象上的方法调用来对RDD进行转换</li>
<li>最终显示结果 或 将结果输出到外部数据源</li>
<li>RDD转换算子称为Transformation是Lazy的（延迟执行）</li>
<li>只有遇到Action算子，才会执行RDD的转换操作</li>
</ul>
<h1 id="3-RDD的创建方式"><a href="#3-RDD的创建方式" class="headerlink" title="3. RDD的创建方式"></a>3. RDD的创建方式</h1><ol>
<li>集合创建RDD</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 使用parallelize方法来创建</span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">3</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line"># 使用makeRDD创建</span><br><span class="line">scala&gt; sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">res0: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at &lt;console&gt;:<span class="number">25</span></span><br></pre></td></tr></table></figure>



<ol start="2">
<li><p>从文件创建</p>
</li>
<li><p>从其他RDD创建</p>
<p> ​	</p>
</li>
<li><p>直接new一个新的RDD对象</p>
</li>
</ol>
<h2 id="2-1-变换算子-Transformation-和动作算子-Action"><a href="#2-1-变换算子-Transformation-和动作算子-Action" class="headerlink" title="2.1 变换算子(Transformation)和动作算子(Action)"></a>2.1 变换算子(Transformation)和动作算子(Action)</h2><p>​		变换会根据当前的RDD来定义新的RDD；动作则是从从RDD返回值。</p>
<h3 id="Transformation算子"><a href="#Transformation算子" class="headerlink" title="Transformation算子"></a>Transformation算子</h3><p>​		每一次Transformation操作都会产生新的RDD，功下一个转换使用; 转换得到的RDD是惰性求值的，整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，只有遇到Action操作时，才会发生真正的计算，从血缘关系的源头开始，进行物理的转换操作。</p>
<p>**map(func)**：对数据集中的每个元素都使用func，然后返回一个新的RDD<br>**filter(func)**：对数据集中的每个元素都使用func，然后返回一个包含使func为true的元素构成的RDD<br>**flatMap(func)**：与 map 类似，每个输入元素被映射为0或多个输出元素<br>**mapPartitions(func)**：和map很像，但是map是将func作用在每个元素上，而mapPartitions是func作用在整个分区上。假设一个RDD有N个元素，M个分区（N &gt;&gt; M），那么map的函数将被调用N次，而mapPartitions中的函数仅被调用M次，一次处理一个分区中的所有元素<br>**mapPartitionsWithIndex(func)**：与 mapPartitions 类似，多了分区的索引值的信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val rdd1=sc.parallelize(1 to 10)</span></span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val rdd2=rdd1.map(_*2)</span></span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at &lt;console&gt;:25</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val rdd3=rdd2.filter(_&gt;10)</span></span><br><span class="line">rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at filter at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// RDD 是分区，rdd1有几个区，每个分区有哪些元素 </span><br><span class="line">rdd1.getNumPartitions</span><br><span class="line">rdd1.partitions.length</span><br><span class="line"></span><br><span class="line">// 对分区中每个分区中的元素组合成列表</span><br><span class="line">rdd1.mapPartitions&#123;iter=&gt;Iterator(s&quot;$&#123;iter.toList&#125;&quot;)&#125;.collect</span><br><span class="line">rdd1.mapPartitions&#123;iter =&gt;Iterator(s&quot;$&#123;iter.toArray.mkString(&quot;-&quot;)&#125;&quot;) &#125;.collect</span><br></pre></td></tr></table></figure>

<p>以上都是transformation操作，没有执行， 需要引入Action算子才会执行。</p>
<ul>
<li><p>map：每次处理一条数据</p>
</li>
<li><p>mapPartitions：每次处理一个分区的数据，分区的数据处理完成后，数据才能释放，资源不足时容易导致OOM</p>
<p>  最佳实践：当内存资源充足时，建议使用mapPartitions，以提高处理效率</p>
</li>
</ul>
<h3 id="聚合操作"><a href="#聚合操作" class="headerlink" title="聚合操作"></a>聚合操作</h3><p>**groupBy(func)**：按照传入函数的返回值进行分组。将key相同的值放入一个迭代器<br>**glom()**：将每一个分区形成一个数组，形成新的RDD类型 RDD[Array[T]]<br>**sample(withReplacement, fraction, seed)**：采样算子。以指定的随机种子(seed)随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样<br>**distinct([numTasks]))**：对RDD元素去重后，返回一个新的RDD。可传入numTasks参数改变RDD分区数<br>**coalesce(numPartitions)**：缩减分区数，无shuffle<br>**repartition(numPartitions)**：增加或减少分区数，有shuffle<br>**sortBy(func, [ascending], [numTasks])**：使用 func 对数据进行处理，对处理后的结果进行排序</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">// 将 RDD 中的元素按照3的余数分组 </span><br><span class="line">val rdd = sc.parallelize(1 to 10) </span><br><span class="line">val group = rdd.groupBy(_%3) </span><br><span class="line">group.collect </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 将 RDD 中的元素每10个元素分组 </span><br><span class="line">val rdd = sc.parallelize(1 to 101) </span><br><span class="line">// 分别对每个分区做map的映射操作</span><br><span class="line">rdd.glom.map(_.sliding(10, 10).toArray) </span><br><span class="line"></span><br><span class="line">// 对数据采样。fraction采样的百分比，近似数 </span><br><span class="line">// 有放回的采样，使用固定的种子 </span><br><span class="line">rdd.sample(true, 0.2, 2).collect </span><br><span class="line">// 无放回的采样，使用固定的种子 </span><br><span class="line">rdd.sample(false, 0.2, 2).collect </span><br><span class="line">// 有放回的采样，不设置种子 </span><br><span class="line">rdd.sample(false, 0.2).collect</span><br><span class="line"></span><br><span class="line">// 数据去重 </span><br><span class="line">val random = scala.util.Random </span><br><span class="line">val arr:Vector[Int] = (1 to 20).map(x =&gt; random.nextInt(10))</span><br><span class="line">// 将Vector转换成RDD</span><br><span class="line">val rdd = sc.makeRDD(arr)</span><br><span class="line">rdd.distinct.collect</span><br><span class="line"></span><br><span class="line">// RDD重分区 </span><br><span class="line">val rdd1 = sc.range(1, 10000, numSlices=10)</span><br><span class="line">val rdd2 = rdd1.filter(_%2==0)</span><br><span class="line">rdd2.getNumPartitions</span><br><span class="line"></span><br><span class="line">// 减少分区数；都生效了 </span><br><span class="line">val rdd3 = rdd2.repartition(5)</span><br><span class="line">rdd3.getNumPartitions</span><br><span class="line">val rdd4 = rdd2.coalesce(5)</span><br><span class="line">rdd4.getNumPartitions</span><br><span class="line"> </span><br><span class="line">// 增加分区数</span><br><span class="line">val rdd5 = rdd2.repartition(20)</span><br><span class="line">rdd5.getNumPartitions</span><br><span class="line"> </span><br><span class="line">// 增加分区数，这样使用没有效果</span><br><span class="line">val rdd6 = rdd2.coalesce(20)</span><br><span class="line">rdd6.getNumPartitions</span><br><span class="line"> </span><br><span class="line">// 增加分区数的正确用法</span><br><span class="line">val rdd6 = rdd2.coalesce(20, true)</span><br><span class="line">rdd6.getNumPartitions</span><br><span class="line"> </span><br></pre></td></tr></table></figure>

<ul>
<li>repartition：增大或减少分区数；有shuffle</li>
<li>coalesce：一般用于减少分区数（此时无shuffle）</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// RDD元素排序 </span><br><span class="line">val random = scala.util.Random</span><br><span class="line">val arr = (1 to 20).map(x =&gt; random.nextInt(10))</span><br><span class="line">val rdd = sc.makeRDD(arr)</span><br><span class="line">rdd.collect</span><br><span class="line"> </span><br><span class="line">// 数据全局有序，默认升序 </span><br><span class="line">rdd.sortBy(x=&gt;x).collect</span><br><span class="line">// 降序</span><br><span class="line">rdd.sortBy(x=&gt;x,false).collect</span><br></pre></td></tr></table></figure>



<h3 id="RDD之间交、并、差的算子"><a href="#RDD之间交、并、差的算子" class="headerlink" title="RDD之间交、并、差的算子"></a>RDD之间交、并、差的算子</h3><p>intersection(otherRDD) ： 交集</p>
<p>union(otherRDD) ： 并集</p>
<p>subtract (otherRDD) ： 差集</p>
<p>cartesian(otherRDD)：笛卡尔积</p>
<p>zip(otherRDD)  : 两个RDD组合成key-value形式的RDD，默认两个RDD的partition数量以及元素数量都相同，否则抛出异常</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.range(1,21)</span><br><span class="line">val rdd2 = sc.range(10,31)</span><br><span class="line"></span><br><span class="line">// 计算交集</span><br><span class="line">rdd1.intersection(rdd2).sortBy(x=&gt;x).collect</span><br><span class="line">// 元素求并集，不去重 </span><br><span class="line">rdd1.union(rdd2).sortBy(x=&gt;x).collect </span><br><span class="line">// 差集</span><br><span class="line">rdd1.subtract(rdd2).sortBy(x=&gt;x).collect </span><br><span class="line"></span><br><span class="line">// 检查分区数 </span><br><span class="line">rdd1.intersection(rdd2).getNumPartitions</span><br><span class="line">rdd1.union(rdd2).getNumPartitions </span><br><span class="line">rdd1.subtract(rdd2).getNumPartitions </span><br><span class="line"> </span><br><span class="line">// 笛卡尔积 </span><br><span class="line">val rdd1 = sc.range(1, 5) </span><br><span class="line">val rdd2 = sc.range(6, 10) </span><br><span class="line">rdd1.cartesian(rdd2).collect </span><br><span class="line">// 检查分区数 </span><br><span class="line">rdd1.cartesian(rdd2).getNumPartitions </span><br><span class="line"> </span><br><span class="line">// 拉链操作 </span><br><span class="line">rdd1.zip(rdd2).collect </span><br><span class="line">rdd1.zip(rdd2).getNumPartitions </span><br><span class="line"> </span><br><span class="line">// zip操作要求：两个RDD的partition数量以及元素数量都相同，否则会抛出异常 </span><br><span class="line">val rdd2 = sc.range(6, 20) </span><br><span class="line">rdd1.zip(rdd2).collect </span><br></pre></td></tr></table></figure>

<ul>
<li>union是窄依赖。得到的RDD分区数为：两个RDD分区数之和</li>
<li>cartesian也是窄依赖。得到的RDD分区数为：两个RDD分区数之积；得到RDD的数据量是两个RDD数据量的积。会有很严重的数据膨胀，慎用</li>
</ul>
<h3 id="Action算子"><a href="#Action算子" class="headerlink" title="Action算子"></a>Action算子</h3><p>Action触发Job。一个Spark程序(Driver程序)包含了多少 Action 算子，那么就有多少Job。</p>
<p>典型的Action算子：collect&#x2F;count</p>
<p>collect算子代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return an array that contains all of the elements in this RDD.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @note This method should only be used if the resulting array is expected to be small, as</span></span><br><span class="line"><span class="comment"> * 所有数据结果均存放在内存中。 (all the data is loaded into the driver&#x27;s memory.)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collect</span></span>(): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    <span class="comment">// 启动一个runJob</span></span><br><span class="line">  <span class="keyword">val</span> results = sc.runJob(<span class="keyword">this</span>, (iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; iter.toArray)</span><br><span class="line">  <span class="type">Array</span>.concat(results: _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>first()：Return the first element in this RDD<br>take(n)：Take the first num elements of the RDD<br>top(n)：按照默认（降序）或者指定的排序规则，返回前num个元素。<br>takeSample(withReplacement, num, [seed])：返回采样的数据<br>foreach(func) &#x2F; foreachPartition(func)：与map、mapPartitions类似，区别是 foreach 是 Action<br>saveAsTextFile(path) &#x2F; saveAsSequenceFile(path) &#x2F; saveAsObjectFile(path)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">// 返回统计信息。仅能作用 RDD[Double] 类型上调用 </span><br><span class="line">val rdd1 = sc.range(1, 101) </span><br><span class="line">rdd1.stats </span><br><span class="line"> </span><br><span class="line">val rdd2 = sc.range(1, 101)</span><br><span class="line"> </span><br><span class="line">// count在各种类型的RDD上，均能调用 </span><br><span class="line">rdd1.zip(rdd2).count </span><br><span class="line"> </span><br><span class="line">// 聚合操作 </span><br><span class="line">val rdd = sc.makeRDD(1 to 10, 2) </span><br><span class="line">rdd.reduce(_+_) </span><br><span class="line">rdd.fold(0)(_+_) </span><br><span class="line">rdd.fold(1)(_+_) </span><br><span class="line">rdd.fold(1)((x, y) =&gt; &#123; </span><br><span class="line">  println(s&quot;x=$x, y=$y&quot;) </span><br><span class="line">  x+y </span><br><span class="line">&#125;) </span><br><span class="line">rdd.aggregate(0)(_+_, _+_) </span><br><span class="line">rdd.aggregate(1)(_+_, _+_) </span><br><span class="line">rdd.aggregate(1)( </span><br><span class="line">(a, b) =&gt; &#123; </span><br><span class="line">  println(s&quot;a=$a, b=$b&quot;) </span><br><span class="line">  a+b </span><br><span class="line">&#125;, </span><br><span class="line">(x, y) =&gt; &#123; </span><br><span class="line">  println(s&quot;x=$x, y=$y&quot;) </span><br><span class="line">  x+y </span><br><span class="line">&#125;) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// first / take(n) / top(n) ：获取RDD中的元素。多用于测试 </span><br><span class="line">rdd.first </span><br><span class="line">rdd.take(10) </span><br><span class="line">rdd.top(10) </span><br><span class="line"> </span><br><span class="line">// 采样并返回结果 </span><br><span class="line">rdd.takeSample(false, 5) </span><br><span class="line"> </span><br><span class="line">// 保存文件到指定路径(rdd有多少分区，就保存为多少文件，保存文件时注意小文件问题) </span><br><span class="line">rdd.saveAsTextFile(&quot;data/t1&quot;) </span><br></pre></td></tr></table></figure>



<h2 id="2-2-Key-Value-RDD"><a href="#2-2-Key-Value-RDD" class="headerlink" title="2.2 Key-Value RDD"></a>2.2 Key-Value RDD</h2><p>键值对RDD通常用来进行聚合计算。</p>
<p>key-value类型的RDD操作都在PairRDDFunctions.scala文件中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">Array</span>((<span class="string">&quot;spark&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;hadoop&quot;</span>, <span class="number">26</span>), (<span class="string">&quot;hadoop&quot;</span>, <span class="number">23</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">15</span>), (<span class="string">&quot;scala&quot;</span>, <span class="number">26</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">25</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">23</span>), (<span class="string">&quot;hadoop&quot;</span>, <span class="number">16</span>), (<span class="string">&quot;scala&quot;</span>, <span class="number">24</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">16</span>))) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rdd.aggregateByKey((<span class="number">0</span>,<span class="number">0</span>))(</span><br><span class="line">    (x,y) =&gt; &#123;println(<span class="string">s&quot;x=<span class="subst">$x</span>, y=<span class="subst">$y</span>&quot;</span>); (x._1 + y, x._2 + <span class="number">1</span>)&#125;,</span><br><span class="line">    (a,b) =&gt; &#123;println(<span class="string">s&quot;a=<span class="subst">$a</span>, b=<span class="subst">$b</span>&quot;</span>); (a._1 + b._1, a._2 + b._2)&#125;</span><br><span class="line">).mapValues(x=&gt;x._1.toDouble/x._2).collect</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rdd.aggregateByKey(scala.collection.mutable.<span class="type">ArrayBuffer</span>[<span class="type">Int</span>]())(</span><br><span class="line">    (x,y) =&gt; &#123;x.append(y);x&#125;,</span><br><span class="line">    (a,b) =&gt; &#123;a ++ b&#125;  <span class="comment">// ArrayBuffer相加</span></span><br><span class="line">).collect</span><br><span class="line"></span><br></pre></td></tr></table></figure>




      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/spark/spark%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/" data-id="clumsd6xb000vabr782105ltm" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2024/04/05/spark/spark%E5%8E%9F%E7%90%86%E5%88%A8%E6%9E%90/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2024/04/05/scala/Scala%E7%AC%94%E8%AE%B0/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">April 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/04/05/%E6%95%B0%E4%BB%93%E7%BB%8F%E9%AA%8C/%E9%9D%A2%E8%AF%95%E9%A2%98/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/04/05/zookeeper/Zookeeper/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/04/05/zookeeper/%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA(HA)/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/04/05/spark/spark%E7%BC%96%E7%A8%8B-RDD%E9%AB%98%E9%98%B6/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/04/05/spark/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>