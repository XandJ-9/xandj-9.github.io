<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-数仓经验/面试题" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/%E6%95%B0%E4%BB%93%E7%BB%8F%E9%AA%8C/%E9%9D%A2%E8%AF%95%E9%A2%98/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.943Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="数仓如何划分主题域？"><a href="#数仓如何划分主题域？" class="headerlink" title="数仓如何划分主题域？"></a>数仓如何划分主题域？</h1><p>设计数据仓库的主题域需要考虑多个因素，包括业务需求、数据来源、数据集成、查询需求等。下面是一些常见的设计原则和步骤来帮助您设计数据仓库的主题域：</p>
<ol>
<li>了解业务需求：与业务用户和相关利益相关者合作，深入了解他们的需求和关注点。明确数据仓库的目标，确定要解决的业务问题和目标。</li>
<li>进行数据源分析：评估和分析可用的数据源，包括内部系统、外部数据源和第三方数据。了解数据源的结构、内容和可用性。</li>
<li>确定主题域：基于业务需求和数据源分析，确定数据仓库中的主题域。主题域是根据业务领域或功能领域来组织数据的逻辑集合。例如，销售、客户、库存、财务等。</li>
<li>定义主题域的粒度：确定每个主题域的数据粒度，即数据的最小可操作单元。粒度的选择应基于业务需求和数据分析的目标。</li>
<li>建立实体-关系模型：使用实体-关系建模方法来定义主题域内的实体和它们之间的关系。标识实体的属性和键，并建立实体之间的关联关系。</li>
<li>设计维度模型：对于每个主题域，设计相应的维度模型。维度模型是基于维度和事实表的模式，用于组织和描述业务数据。确定维度和指标，并定义维度之间的层次结构和关系。</li>
<li>确定ETL流程：设计和实现ETL（抽取、转换、加载）流程，用于从数据源中抽取、清洗和转换数据，并加载到数据仓库中的相应主题域。</li>
<li>验证和优化设计：对设计的主题域进行验证和测试，确保满足业务需求。根据实际使用情况和性能要求，进行性能优化和调整。</li>
<li>持续维护和演化：数据仓库的主题域设计是一个持续的过程。随着业务需求的变化和新的数据来源的引入，需要不断进行维护和演化，确保数据仓库的有效性和可用性。</li>
</ol>
<p>需要注意的是，数据仓库的主题域设计是一个复杂且需要灵活性的过程，因此在设计之前，建议与业务用户、数据专家和相关技术人员进行充分的沟通和讨论，以确保设计能够满足业务需求，并符合数据仓库的目标和原则。</p>
<h1 id="数仓处理过程中的缓存表如何处理？"><a href="#数仓处理过程中的缓存表如何处理？" class="headerlink" title="数仓处理过程中的缓存表如何处理？"></a>数仓处理过程中的缓存表如何处理？</h1><p>在数据仓库处理过程中，临时表或缓存表通常用于存储中间结果或提高查询性能。以下是一些常见的处理方法：</p>
<ol>
<li>命名规范：对于临时表或缓存表，可以采用特定的命名规范，以便清楚地标识其用途和临时性质。例如，在表名中添加前缀或后缀，如”tmp_”或”_cache”。</li>
<li>生命周期管理：临时表或缓存表的生命周期需要进行管理。在数据处理流程结束后，及时清理和删除不再需要的临时表或缓存表，以释放存储空间。</li>
<li>表结构定义：定义临时表或缓存表的结构，包括表名、列名、数据类型等。确保表结构与数据处理过程中涉及的数据一致，并遵循数据模型设计原则。</li>
<li>数据清理策略：对于临时表或缓存表中的数据，可以根据需求制定相应的清理策略。例如，可以基于时间戳或数据的过期规则定期清理数据，以保持表的数据量合理。</li>
<li>索引和优化：根据临时表或缓存表的查询需求，考虑在表中添加适当的索引，以提高查询性能。根据数据访问模式和查询频率，进行性能优化，如使用适当的缓存策略。</li>
<li>内存管理：对于临时表或缓存表，可以考虑将其存储在内存中，以加快查询速度。使用内存数据库或缓存技术，如Redis或Memcached，可以提高查询性能和响应时间。</li>
<li>安全性考虑：对于包含敏感数据的临时表或缓存表，确保适当的安全措施，如访问控制、加密和数据脱敏，以保护数据的安全性。</li>
<li>数据一致性：在处理过程中，确保临时表或缓存表与源数据和目标数据之间的一致性。及时更新或清理临时表或缓存表，以确保数据的准确性。</li>
</ol>
<p>综上所述，处理临时表或缓存表需要考虑命名规范、生命周期管理、表结构定义、数据清理策略、索引和优化、内存管理、安全性和数据一致性等方面。根据具体的数据仓库架构和业务需求，可以选择适合的处理方法。</p>
<h1 id="Spark-SQL执行过程，-例如group-by-count-1-这样的sql语句如何转换成底层的执行任务？"><a href="#Spark-SQL执行过程，-例如group-by-count-1-这样的sql语句如何转换成底层的执行任务？" class="headerlink" title="Spark SQL执行过程， 例如group by count(1) 这样的sql语句如何转换成底层的执行任务？"></a>Spark SQL执行过程， 例如group by count(1) 这样的sql语句如何转换成底层的执行任务？</h1><p>在Spark SQL中，SQL语句的执行过程通常涉及以下几个步骤，以group by count(1)为例：</p>
<ol>
<li>解析（Parsing）：Spark SQL首先对SQL语句进行解析，将其转换为逻辑计划（Logical Plan）。解析阶段将识别SQL语句的语法结构和关键字，并构建相应的语法树。</li>
<li>语义分析（Semantic Analysis）：在语义分析阶段，Spark SQL将执行语义检查，确保SQL语句的语义正确。这包括验证表和列的存在性、检查数据类型的一致性以及解析SQL语句中的函数和表达式等。</li>
<li>逻辑优化（Logical Optimization）：逻辑优化阶段对逻辑计划进行优化，以提高执行效率。这包括重新排序操作的顺序、合并多个相邻的操作等。</li>
<li>物理计划生成（Physical Plan Generation）：在物理计划生成阶段，Spark SQL将逻辑计划转换为物理计划（Physical Plan）。物理计划描述了如何在分布式环境中执行操作。</li>
<li>执行计划优化（Execution Plan Optimization）：在执行计划优化阶段，Spark SQL会进一步优化物理计划，以提高执行性能。这可能涉及到数据分区、数据倾斜处理、动态过滤等优化技术。</li>
<li>执行任务（Task Execution）：最后，Spark SQL将物理计划转化为一系列的任务（Tasks），并在集群上执行这些任务。每个任务负责在分布式环境中执行具体的计算操作，例如读取数据、执行聚合操作、进行分组等。</li>
</ol>
<p>对于group by count(1)这样的SQL语句，底层的执行任务通常包括以下步骤：</p>
<ol>
<li>数据读取：Spark SQL会根据查询条件读取相应的数据，可以是从文件系统、数据库或其他数据源中读取。</li>
<li>数据分区：读取的数据可能会根据分区规则进行划分，以便并行处理。Spark SQL会根据数据的键（group by的字段）将数据进行分组，将具有相同键的数据分配到同一个分区中。</li>
<li>局部聚合（Local Aggregation）：在每个分区内，Spark SQL会执行局部聚合操作，计算每个分区内各个键的数量。这一步可以在每个分区上并行进行。</li>
<li>全局聚合（Global Aggregation）：在局部聚合完成后，Spark SQL将各个分区的局部聚合结果合并为全局聚合结果。这涉及将具有相同键的结果合并在一起，并计算总的数量。</li>
<li>结果返回：最后，Spark SQL将计算得到的聚合结果返回给用户。</li>
</ol>
<h1 id="数据集成任务如何避免内存溢出？"><a href="#数据集成任务如何避免内存溢出？" class="headerlink" title="数据集成任务如何避免内存溢出？"></a>数据集成任务如何避免内存溢出？</h1><p>在数据集成任务中，为了避免内存溢出，可以考虑以下几个方面的优化措施：</p>
<ol>
<li>数据分批处理：将大规模的数据集拆分为较小的批次进行处理，而不是一次性加载整个数据集。这样可以减少每个批次所占用的内存，并降低内存溢出的风险。</li>
<li>调整内存配置：根据具体的任务需求和可用的系统资源，合理配置内存参数，如JVM堆内存大小、Spark Executor的内存分配等。适当增加可用内存可以减少内存溢出的概率。</li>
<li>数据压缩和编码：对于数据集成过程中的大量数据，考虑使用压缩和编码技术来减少数据的内存占用。例如，可以使用压缩算法对数据进行压缩，或者使用更紧凑的数据编码方式。</li>
<li>懒加载和分页加载：延迟加载数据，只在需要时才加载数据到内存中。可以使用类似于Spark的惰性求值机制，避免一次性加载大量数据，而是按需加载和处理数据。此外，可以使用分页加载的方式，逐页加载数据，减少内存压力。</li>
<li>内存管理和垃圾回收：合理管理内存资源，及时释放不再使用的内存对象。可以使用适当的内存管理策略和垃圾回收机制，如调整垃圾回收器参数、设置合理的对象生命周期等。</li>
<li>数据预处理和过滤：在数据集成之前，进行数据预处理和过滤，只选择需要的数据进行处理。可以使用过滤条件、列裁剪等方法来减少数据量，从而降低内存消耗。</li>
<li>分布式处理和并行执行：考虑使用分布式计算框架，如Spark、Hadoop等，在集群环境中进行数据集成任务的并行处理。这样可以将任务分散到多个节点上进行处理，减少单节点的内存压力。</li>
<li>数据持久化和持久化存储：对于大规模的数据集，可以考虑将部分数据进行持久化存储，以释放内存空间。可以将数据写入磁盘、存储到数据库或者使用分布式存储系统，如HDFS。</li>
</ol>
<p>通过以上的优化措施，可以有效减少内存占用，降低内存溢出的风险，并提高数据集成任务的执行效率和稳定性。根据具体的场景和需求，可以根据实际情况采取相应的优化策略。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/%E6%95%B0%E4%BB%93%E7%BB%8F%E9%AA%8C/%E9%9D%A2%E8%AF%95%E9%A2%98/" data-id="clumsd6xc000yabr7gs7odhr3" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-zookeeper/Zookeeper" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/zookeeper/Zookeeper/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.942Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p><img src="/Zookeeper.assets/Zookeeper.png" alt="Zookeeper"></p>
<h1 id="Zookeeper介绍"><a href="#Zookeeper介绍" class="headerlink" title="Zookeeper介绍"></a>Zookeeper介绍</h1><p>​		Zookeeper 是一个分布式协调服务的开源框架。 主要用来解决分布式集群中应用系统的一致性问题，例如怎样避免同时操作同一数据造成脏读的问题。分布式系统中数据存在一致性的问题！！</p>
<ul>
<li>ZooKeeper 本质上是一个分布式的小文件存储系统。 提供基于类似于文件系统的目录树方式的数据存储，并且可以对树中的节点进行有效管理。</li>
<li>ZooKeeper 提供给客户端监控存储在zk内部数据的功能，从而可以达到基于数据的集群管理。 诸如： 统一命名服务(dubbo)、分布式配置管理(solr的配置集中管理)、分布式消息队列（sub&#x2F;pub）、分布式锁、分布式协调等功能。</li>
</ul>
<p><img src="/Zookeeper.assets/image-20210308.PNG" alt="image-20210308"></p>
<h1 id="Zookeeper-特点"><a href="#Zookeeper-特点" class="headerlink" title="Zookeeper 特点"></a>Zookeeper 特点</h1><ol>
<li>Zookeeper：一个领导者（leader:老大），多个跟随者（follower:小弟）组成的集群。</li>
<li>Leader负责进行投票的发起和决议，更新系统状态(内部原理)</li>
<li>Follower用于接收客户请求并向客户端返回结果, 以及在选举Leader过程中参与投票</li>
<li>集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。</li>
<li><strong>全局数据一致</strong>：每个server保存一份相同的数据副本，Client无论连接到哪个server，数据都是一<br>致的。</li>
<li>更新请求顺序进行(内部原理)</li>
<li>数据更新原子性，一次数据更新要么成功，要么失败。</li>
</ol>
<h1 id="ZooKeeper数据模型Znode"><a href="#ZooKeeper数据模型Znode" class="headerlink" title="ZooKeeper数据模型Znode"></a>ZooKeeper数据模型Znode</h1><p><img src="/Zookeeper.assets/image-20210110214521460.png" alt="image-20210110214521460"></p>
<p><strong>Znode</strong> 节点</p>
<p>​	在Zookeeprr中，数据信息被保存在一个个数据节点中，这些节点被称为ZNode.</p>
<p>Zookeeper节点累心可以分为三大类:</p>
<p>持久性节点（Persistent）</p>
<p>临时性节点（Ephemeral）</p>
<p>顺序行节点（Sequential）</p>
<p><strong>持久节点</strong>：Zookeeper中最常见的一种节点类型，节点创建后会一直存在服务器，直到删除操作主动清除。</p>
<p><strong>持久顺序节点</strong>：有顺序的持久节点，有持久节点的特性，顺序特性是指在创建节点的时候会在节点后面加上一个数字后缀，来表示其顺序。</p>
<p><strong>临时节点</strong>：会被自动清理的节点， 其生命周期与客户端绑定，客户端会话结束，节点会被删除掉。与持久节点不同的是，临时节点不能创建子节点。</p>
<p><strong>临时顺序节点</strong>：有顺序的临时节点，和持久顺序节点一样，创建是会带上数字序号。</p>
<p>​		Zookeeper中的事务是指能够改变ZK服务器状态的操作，包括数据节点的创建与删除，数据节点内容更新等操作，对于每一个事务请求，Zookeeper都会为其分配一个<strong>全局唯一</strong>的事务ID，用<strong>ZXID</strong>来表示，通常是一个64位的数字。每一个ZXID对应一次更新操作，从这些ZXID中可以间接的识别出Zookeeper处理这些更新操作的全局顺序。</p>
<p><em>查看节点状态</em></p>
<p>.&#x2F;zkCli.sh进入交互操作窗口</p>
<p><img src="/Zookeeper.assets/image-20210110223053102.png" alt="image-20210110223053102"></p>
<p><img src="/Zookeeper.assets/image-20210110223115564.png" alt="image-20210110223115564"></p>
<p>每次修改znode中的数据时，事务的变更序号就会发生改变，如下mzxid两次的变化：</p>
<p><img src="/Zookeeper.assets/image-20210328230845342.png" alt="image-20210328230845342"></p>
<h1 id="Watcher监听机制"><a href="#Watcher监听机制" class="headerlink" title="Watcher监听机制"></a>Watcher监听机制</h1><p>​		Zookeeper使用Watcher机制实现分布式数据的发布&#x2F;订阅功能。一个典型的发布&#x2F;订阅模型系统定义了一种 一对多的订阅关系，能够让多个订阅者同时监听某一个主题对象，当这个主题对象自身状态变化时，会通知所有订阅者，使它们能够做出相应的处理。</p>
<p>​		在 ZooKeeper 中，引入了 Watcher 机制来实现这种分布式的通知功能。ZooKeeper 允许客户端向服务端注册一个 Watcher 监听，当服务端的一些指定事件触发了这个 Watcher，那么Zk就会向指定客户端发送一个事件通知来实现分布式的通知功能。</p>
<p><img src="/Zookeeper.assets/image-20210328155711023.png" alt="image-20210328155711023"></p>
<p>Zookeeper使用Watcher机制实现的分布式数据的发布&#x2F;订阅功能。</p>
<p>Watcher机制的主要角色包括: 客户端线程，客户端WatcherManager, Zookeeper服务器三部分。</p>
<p>具体工作流程：</p>
<ul>
<li>客户端在像Zookeeper服务器注册的同时，在客户端的WatcherManager中存储一个Watcher对象</li>
<li>当Zookeeper服务器触发Watcher事件后，回向客户端发送通知</li>
<li>客户端从WatcherManager中取出对应的Watcher对象来执行回调逻辑</li>
</ul>
<p>利用zk的watcher节点监听机制，可以实现简单的分布式锁。</p>
<h1 id="Zookeeper基本使用"><a href="#Zookeeper基本使用" class="headerlink" title="Zookeeper基本使用"></a>Zookeeper基本使用</h1><h2 id="命令行操作"><a href="#命令行操作" class="headerlink" title="命令行操作"></a>命令行操作</h2><p>通过zkCli.sh进入zookeeper客户端命令行</p>
<p>.&#x2F;zkCli.sh </p>
<p>.&#x2F;zkCli.sh -server ip:port    # 连接指定的服务器</p>
<p>连接成功的话，会输出zookeeper相关的环境及配置信息，使用help查看可以使用的命令。</p>
<img src="Zookeeper.assets/image-20210124151151244.png"/>







<h1 id="ZK内部原理"><a href="#ZK内部原理" class="headerlink" title="ZK内部原理"></a>ZK内部原理</h1><h2 id="一-Leader选举机制"><a href="#一-Leader选举机制" class="headerlink" title="一. Leader选举机制"></a>一. Leader选举机制</h2><p>选举机制特点</p>
<ul>
<li>半数原则：集群中<strong>半数以上</strong>机器存活，集群可用。所以Zookeeper适合安装奇数台服务器。</li>
<li>Zookeeper工作时，有一个节点为Leader，其它为Follower，Leader是通过内部的选举机制产生的。</li>
</ul>
<ol>
<li>集群首次启动时，zk选举机制：</li>
</ol>
<p><img src="/Zookeeper.assets/image-20210328160134216.png" alt="image-20210328160134216"></p>
<p>例如5台机器的zk集群初次启动时，有以下个过程：</p>
<p>（1）服务器1启动，此时只有它一台服务器启动了，它发出去的报文没有任何响应，所以它的选举状态一直是LOOKING状态。<br>（2）服务器2启动，它与最开始启动的服务器1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以id值较大的服务器2胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3)，所以服务器1、2还是继续保持LOOKING状态。<br>（3）服务器3启动，根据前面的理论分析，服务器3成为服务器1、2、3中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的Leader。<br>（4）服务器4启动，根据前面的分析，理论上服务器4应该是服务器1、2、3、4中最大的，但是由前<br>面已经有半数以上的服务器选举了服务器3，所以它只能接收当小弟的命了。</p>
<p>（5）服务器5启动，同4一样称为follower。</p>
<ol start="2">
<li>非首次启动zk集群</li>
</ol>
<p>非首次启动zk集群时，每个节点会参考自身的事务ID值(zxid)； 选举Leader时有限选择zxid最大的节点作为Leader。</p>
<h2 id="二-ZAB一致性协议"><a href="#二-ZAB一致性协议" class="headerlink" title="二. ZAB一致性协议"></a>二. ZAB一致性协议</h2><h1 id="zookeeper安装"><a href="#zookeeper安装" class="headerlink" title="zookeeper安装"></a>zookeeper安装</h1><p>上传压缩包并解压好之后。</p>
<p>修改配置文件，创建data与log目录</p>
<blockquote>
<p># 创建zk存储数据目录</p>
<p>mkdir -p &#x2F;opt&#x2F;lagou&#x2F;servers&#x2F;zookeeper-3.4.14&#x2F;data</p>
<p># 创建zk日志文件目录</p>
<p>mkdir -p &#x2F;opt&#x2F;lagou&#x2F;servers&#x2F;zookeeper-3.4.14&#x2F;data&#x2F;logs</p>
</blockquote>
<p>修改配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">数据目录</span></span><br><span class="line">dataDir=/opt/lagou/servers/zookeeper-3.4.14/data</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">日志目录</span></span><br><span class="line">dataLogDir=/opt/lagou/servers/zookeeper-3.4.14/data/logs</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">zk提供了自动清理事务日志和快照文件的功能，该参数指定清理频率</span></span><br><span class="line">autopurge.purgeInterval=1</span><br><span class="line"></span><br><span class="line">server.1=linux111:2888:3888</span><br><span class="line">server.2=linux112:2888:3888</span><br><span class="line">server.3=linux113:2888:3888</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>添加myid配置文件</p>
<p>在zookeeper的data目录下创建一个myid文件，内容为1，记录每个zookeeper服务器在集群中的id</p>
<blockquote>
<p>echo 1 &gt; myid</p>
</blockquote>
<p>zookeeper节点没有固定的主从之分，所以需要分别把每台机器上的zk进程启动。</p>
<p>linux111节点上</p>
<p><img src="/Zookeeper.assets/image-20210110210847889.png" alt="image-20210110210847889"></p>
<p>linux113节点上</p>
<p><img src="/Zookeeper.assets/image-20210110210919626.png" alt="image-20210110210919626"></p>
<p>可以看到这里linux113节点变成了leader, 其余节点都是follower, 这是zookeeper内部选择的节点角色。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/zookeeper/Zookeeper/" data-id="clumsd6xc000wabr7ghxrdvu3" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-zookeeper/高可用集群搭建(HA)" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/zookeeper/%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA(HA)/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.937Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Hadoop-HA-搭建"><a href="#Hadoop-HA-搭建" class="headerlink" title="Hadoop HA 搭建"></a>Hadoop HA 搭建</h1><p>​		使用zookeeper可以进一步搭建一个高可用的分布式集群，hadoop支持HA模式的集群，当集群的主节点(Master)发生异常时，会自动切换到备节点(Standby),这样就可以在不影响对外提供服务的情况下继续提供hadoop的集群服务。</p>
<h2 id="hdfs配置"><a href="#hdfs配置" class="headerlink" title="hdfs配置"></a>hdfs配置</h2><p>配置hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lagoucluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.lagoucluster<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.lagoucluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux111:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.lagoucluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux112:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.lagoucluster.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux111:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.lagoucluster.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux112:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://linux111:8485;linux112:8485;linux113:8485/lagou<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.lagoucluster<span class="tag">&lt;<span class="name">name</span>&gt;</span></span><br><span class="line">   	<span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;<span class="name">name</span>&gt;</span></span><br><span class="line">   	<span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;<span class="name">name</span>&gt;</span></span><br><span class="line">   	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;<span class="name">name</span>&gt;</span></span><br><span class="line">   	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;<span class="name">name</span>&gt;</span></span><br><span class="line">   	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/journalnode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;<span class="name">name</span>&gt;</span></span><br><span class="line">   	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        </span><br></pre></td></tr></table></figure>

<p>配置core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;<span class="name">name</span>&gt;</span></span><br><span class="line">   	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://lagoucluster<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;<span class="name">name</span>&gt;</span></span><br><span class="line">   	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/lagou/servers/ha/hadoop-2.9.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;<span class="name">name</span>&gt;</span></span><br><span class="line">   	<span class="tag">&lt;<span class="name">value</span>&gt;</span>linux111:2181,linux112:2191,linux113:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>





<ol>
<li><p>先启动zk集群</p>
</li>
<li><p>启动HDFS-HA集群，在每个节点上执行</p>
</li>
</ol>
<blockquote>
<p>&#x2F;opt&#x2F;lagou&#x2F;servers&#x2F;ha&#x2F;hadoop-2.9.2&#x2F;sbin&#x2F;hadoop-daemon.sh start journalnode</p>
</blockquote>
<ol start="3">
<li>在nn1节点上对其格式化， <strong>首次安装时才需要格式化</strong></li>
</ol>
<blockquote>
<p>&#x2F;opt&#x2F;lagou&#x2F;servers&#x2F;ha&#x2F;hadoop-2.9.2&#x2F;bin&#x2F;hdfs namenode -format</p>
</blockquote>
<ol start="4">
<li>在nn1上启动namenode</li>
</ol>
<blockquote>
<p>&#x2F;opt&#x2F;lagou&#x2F;servers&#x2F;ha&#x2F;hadoop-2.9.2&#x2F;sbin&#x2F;hadoop-daemon.sh start namenode</p>
</blockquote>
<ol start="5">
<li>在nn2上，同步nn1的元数据信息</li>
</ol>
<blockquote>
<p>&#x2F;opt&#x2F;lagou&#x2F;servers&#x2F;ha&#x2F;hadoop-2.9.2&#x2F;bin&#x2F;hdfs namenode -bootstrapStandby</p>
</blockquote>
<ol start="6">
<li>在nn1节点上初始化zkfc</li>
</ol>
<blockquote>
<p>&#x2F;opt&#x2F;lagou&#x2F;servers&#x2F;ha&#x2F;hadoop-2.9.2&#x2F;bin&#x2F;hdfs zkfc -formatZK</p>
</blockquote>
<ol start="7">
<li>在nn1节点上，启动集群</li>
</ol>
<blockquote>
<p>&#x2F;opt&#x2F;lagou&#x2F;servers&#x2F;ha&#x2F;hadoop-2.9.2&#x2F;sbin&#x2F;start-dfs.sh</p>
</blockquote>
<p>遇到的问题</p>
<p>在格式化时报错:</p>
<p>is in an inconsistent state: Can’t format the storage directory because the current directory is not empty. </p>
<p>实际情况是对 &#x2F;opt&#x2F;lagou&#x2F;servers&#x2F;ha&#x2F;hadoop-2.9.2&#x2F;data&#x2F;tmp 数据目录没有删除干净，导致格式化失败， 所以在首次安装时，要确保存放元数据的目录为空。</p>
<p>jps显示异常的进程号</p>
<img src="Zookeeper.assets/image-20210126231702506.png"/>

<p>rm -rf &#x2F;tmp&#x2F;hsperfdata_* </p>
<h2 id="yarn配置"><a href="#yarn配置" class="headerlink" title="yarn配置"></a>yarn配置</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 启用resourcemanager ha --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster-yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux112<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux113<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux111:2181,linux112:2181,linux113:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>修改配置完后， 分别在指定的两台resourcemanager节点上启动yarn服务。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/zookeeper/%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA(HA)/" data-id="clumsd6xc000xabr794q0bd57" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-spark/spark编程-RDD高阶" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/spark/spark%E7%BC%96%E7%A8%8B-RDD%E9%AB%98%E9%98%B6/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.936Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>变换和动作</p>
<p>序列化</p>
<p>​		在实际开发中我们往往需要自己定义一些对于RDD的操作，那么此时需要考虑的主要问题是，初始化工作是在Driver端进行的，而实际运行程序是在Executor端进行的，这就涉及到了跨进程通信，是需要序列化的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/spark/spark%E7%BC%96%E7%A8%8B-RDD%E9%AB%98%E9%98%B6/" data-id="clumsd6xb000rabr71d0p327u" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-spark/spark系统架构与集群部署模式" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/spark/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.936Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Spark系统架构"><a href="#Spark系统架构" class="headerlink" title="Spark系统架构:"></a>Spark系统架构:</h1><ul>
<li>Cluster Manager</li>
<li>Worker Node</li>
<li>Driver</li>
<li>Executor</li>
</ul>
<ol>
<li><p>Cluster Manager是集群资源的管理者;</p>
</li>
<li><p>Worker Node 工作节点管理本地资源；</p>
</li>
<li><p>Driver Program 运行应用的main方法并且创建了SparkContext, 由ClusterManager分配资源，SparkContext发送Task到Executor上执行。</p>
</li>
<li><p>Executor 在工作节点上运行，执行Driver发送给Task, 并向Driver汇报计算结果。</p>
</li>
</ol>
<p><img src="/images/image-20211015162049385.png" alt="image-20211015162049385"></p>
<h1 id="Spark的集群管理类型"><a href="#Spark的集群管理类型" class="headerlink" title="Spark的集群管理类型"></a>Spark的集群管理类型</h1><p>Cluster Manager Types</p>
<p>The system currently supports several cluster managers:</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/spark-standalone.html">Standalone</a> – a simple cluster manager included with Spark that makes it easy to set up a cluster.</li>
<li><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/running-on-mesos.html">Apache Mesos</a> – a general cluster manager that can also run Hadoop MapReduce and service applications. (Deprecated)</li>
<li><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/running-on-yarn.html">Hadoop YARN</a> – the resource manager in Hadoop 2.</li>
<li><a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/running-on-kubernetes.html">Kubernetes</a> – an open-source system for automating deployment, scaling, and management of containerized applications.</li>
</ul>
<p>参考： <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/cluster-overview.html">http://spark.apache.org/docs/latest/cluster-overview.html</a></p>
<p>​	1) . Standalone 本地模式</p>
<p>​		独立模式，自带完整的服务，可单独部署到一个集群中，无需依赖任何其他资源管理系统；</p>
<p>​		Cluster Manager角色：Master</p>
<p>​		Worker Node角色: Worker</p>
<p>​		仅支持粗粒度的资源分配方式</p>
<p>参考链接(学习版本2.4.5): <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.4.5/spark-standalone.html">http://spark.apache.org/docs/2.4.5/spark-standalone.html</a></p>
<p>​	2). Spark on Yarn 模式</p>
<p>​		国内运用最广的部署模式。</p>
<p>​		Spark on Yarn 支持两种模式</p>
<ul>
<li><p>yarn-cluster：用于生产环境</p>
</li>
<li><p>yarn-client：用于交互调式</p>
<p>  Cluster Manager:  ResourceManager</p>
<p>  Worker Node: NodeManager</p>
<p>  也是粗粒度的资源调度方式。</p>
</li>
</ul>
<p>​    </p>
<p>​	3). Spark on Mesos 模式</p>
<p>​	官方推荐的模式，Spark运行在Mesos上会比运行在Yarn上更加灵活自然。</p>
<p>​	Cluster Manager: Mesos Master</p>
<p>​	Worker Node: Mesos Slave</p>
<p>​	即支持粗粒度也支持细粒度的资源调度方式。</p>
<p><strong>粗粒度模式（Coarse-grained Mode）</strong>：每个应用程序的运行环境由一个Dirver和若干个Executor组成，其中每个Executor占用若干资源，内部可运行多个Task。应用程序的各个任务正式运行之前，需要将运行环境中的资源都申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。</p>
<p><strong>细粒度模式（Fine-grained Mode）</strong>：鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos还提供了另外一调度模式：细粒度模式，这种模式类似于现在的云计算，核心思想是按需分配。</p>
<p>相关术语：</p>
<p>Application： 用户提交的Spark应用程序，由集群中的一个Driver和多个Executor组成；</p>
<p>Application jar： 包含Spark应用程序的jar文件；</p>
<p>Driver： spark应用程序main方法开始的部分，由它创建并初始SparkContext；</p>
<p>Cluster Manager： 资源管理服务，即 standalone，yarn，mesos等；</p>
<p>Worker Node： 运行应用程序的节点；</p>
<p>Executor： 运行应用程序Task和保存数据，每个应用成勋都有自己的executors，并且相互独立；</p>
<p>Task： executor应用程序的最小单元；</p>
<p>Job：在用户程序中，每次调用Action都会产生一个新的job；</p>
<p>Stage：一个job又会被分成多个Stage,每个Stage是一系列Task的集合。</p>
<h1 id="Spark-安装"><a href="#Spark-安装" class="headerlink" title="Spark 安装"></a>Spark 安装</h1><p>官网地址：<a target="_blank" rel="noopener" href="http://spark.apache.org/">http://spark.apache.org/</a><br>文档地址：<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/">http://spark.apache.org/docs/latest/</a><br>下载地址：<a target="_blank" rel="noopener" href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a></p>
<ol>
<li><p>下载spark版本上传到服务器上</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/lagou/software/</span><br><span class="line">tar zxvf spark-2.4.5-bin-without-hadoop-scala-2.12.tgz</span><br><span class="line">mv spark-2.4.5-bin-without-hadoop-scala-2.12/ ../servers/spark-2.4.5/</span><br></pre></td></tr></table></figure>

</li>
<li><p>配置环境变量</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line">export SPARK_HOME=/opt/lagou/servers/spark-2.4.5</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>


</li>
<li><p>修改spark配置文件</p>
<p> 配置文件所在目录：$SPARK_HOME&#x2F;conf</p>
<p> 1.配置slaves节点，在slaves配置文件中添加spark集群节点</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vi slaves</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">###  实验节点的hostname</span></span></span><br><span class="line">linux111</span><br><span class="line">linux112</span><br><span class="line">linux113</span><br></pre></td></tr></table></figure>

<p> 2.修改配置文件spark-default.conf</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Default system properties included when running spark-submit.</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">This is useful <span class="keyword">for</span> setting default environmental settings.</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Example:</span></span><br><span class="line">spark.master                     spark://linux111:7077</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">开启eventLog日志</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">spark.eventLog.enabled           <span class="literal">true</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定日志在hdfs上的位置，如果目录不存在需要手动创建好</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">spark.eventLog.<span class="built_in">dir</span>               hdfs://linux111:9000/spark-eventLog</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定序列化器</span></span><br><span class="line">spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">定义driver的内存大小</span></span><br><span class="line">spark.driver.memory              512m</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">executor运行时的JVM参数</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=<span class="string">&quot;one two three&quot;</span></span></span><br></pre></td></tr></table></figure>

<p> 3.修改spark-env.sh， spark启动时的一些环境变量</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/lagou/servers/jdk1.8.0_231</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">spark处理hadoop上的文件时需要配置</span></span><br><span class="line">export HADOOP_HOME=/opt/lagou/servers/hadoop-2.9.2</span><br><span class="line">export HADOOP_CONF_DIR=/opt/lagou/servers/hadoop-2.9.2/etc/hadoop</span><br><span class="line">export SPARK_DIST_CLASSPATH=$(/opt/lagou/servers/hadoop-2.9.2/bin/hadoop classpath)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">master节点的hostname</span></span><br><span class="line">export SPARK_MASTER_HST=linux111</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">端口</span></span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br></pre></td></tr></table></figure>

<p> 在spark-env.sh配置文件中还有一些可选的配置项如下:</p>
 <table class="table">
   <tr><th style="width:21%">Environment Variable</th><th>Meaning</th></tr>
   <tr>
     <td><code>SPARK_MASTER_HOST</code></td>
     <td>Bind the master to a specific hostname or IP address, for example a public one.</td>
   </tr>
   <tr>
     <td><code>SPARK_MASTER_PORT</code></td>
     <td>Start the master on a different port (default: 7077).</td>
   </tr>
   <tr>
     <td><code>SPARK_MASTER_WEBUI_PORT</code></td>
     <td>Port for the master web UI (default: 8080).</td>
   </tr>
   <tr>
     <td><code>SPARK_MASTER_OPTS</code></td>
     <td>Configuration properties that apply only to the master in the form "-Dx=y" (default: none). See below for a list of possible options.</td>
   </tr>
   <tr>
     <td><code>SPARK_LOCAL_DIRS</code></td>
     <td>
     Directory to use for "scratch" space in Spark, including map output files and RDDs that get
     stored on disk. This should be on a fast, local disk in your system. It can also be a
     comma-separated list of multiple directories on different disks.
     </td>
   </tr>
   <tr>
     <td><code>SPARK_WORKER_CORES</code></td>
     <td>Total number of cores to allow Spark applications to use on the machine (default: all available cores).</td>
   </tr>
   <tr>
     <td><code>SPARK_WORKER_MEMORY</code></td>
     <td>Total amount of memory to allow Spark applications to use on the machine, e.g. <code>1000m</code>, <code>2g</code> (default: total memory minus 1 GiB); note that each application's <i>individual</i> memory is configured using its <code>spark.executor.memory</code> property.</td>
   </tr>
   <tr>
     <td><code>SPARK_WORKER_PORT</code></td>
     <td>Start the Spark worker on a specific port (default: random).</td>
   </tr>
   <tr>
     <td><code>SPARK_WORKER_WEBUI_PORT</code></td>
     <td>Port for the worker web UI (default: 8081).</td>
   </tr>
   <tr>
     <td><code>SPARK_WORKER_DIR</code></td>
     <td>Directory to run applications in, which will include both logs and scratch space (default: SPARK_HOME/work).</td>
   </tr>
   <tr>
     <td><code>SPARK_WORKER_OPTS</code></td>
     <td>Configuration properties that apply only to the worker in the form "-Dx=y" (default: none). See below for a list of possible options.</td>
   </tr>
   <tr>
     <td><code>SPARK_DAEMON_MEMORY</code></td>
     <td>Memory to allocate to the Spark master and worker daemons themselves (default: 1g).</td>
   </tr>
   <tr>
     <td><code>SPARK_DAEMON_JAVA_OPTS</code></td>
     <td>JVM options for the Spark master and worker daemons themselves in the form "-Dx=y" (default: none).</td>
   </tr>
   <tr>
     <td><code>SPARK_DAEMON_CLASSPATH</code></td>
     <td>Classpath for the Spark master and worker daemons themselves (default: none).</td>
   </tr>
   <tr>
     <td><code>SPARK_PUBLIC_DNS</code></td>
     <td>The public DNS name of the Spark master and workers (default: none).</td>
   </tr>
 </table>
 
 
 
 
 
<p> 4.将配置好的spark目录分发到其他集群节点上</p>
<p> ​		使用scp命令将配置好的spark目录分发到其他集群节点上。</p>
<p> 5.启动集群</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $SPARK_HOME/sbin</span><br><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure></li>
</ol>
<p><img src="/images/image-20211020000610163.png" alt="image-20211020000610163"></p>
<p><img src="/images/image-20211020000701679.png" alt="image-20211020000701679"></p>
<p>​     6.测试安装结果</p>
<p>在$SPARK_HOME&#x2F;bin目录下执行下列测试样例。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">测试计算pi的命令</span></span><br><span class="line">run-example SparkPi 10</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入spark-shell环境下</span></span><br><span class="line">./spark-shell</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">由于测试的是Spark集群的standalone运行模式，因此文件读取来自hdfs  ??</span></span><br><span class="line">var lines = sc.textFile(&quot;/input/wc.txt&quot;)</span><br><span class="line">lines.flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_+_).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p><img src="/images/image-20211020001631767.png" alt="image-20211020001631767"></p>
<h1 id="Spark三种应用运行模式"><a href="#Spark三种应用运行模式" class="headerlink" title="Spark三种应用运行模式"></a>Spark三种应用运行模式</h1><p>​		这里说的三种运行模式是指</p>
<h2 id="Spark的本地模式-local"><a href="#Spark的本地模式-local" class="headerlink" title="Spark的本地模式 local"></a>Spark的本地模式 local</h2><p>​		本地模式部署在单机，用于测试和实验。它是最简单的运行模式，所有的进程都运行在一台机器的JVM中，该模式下用多个线程模拟Spark的分布式计算，通常用来验证开发的应用程序。</p>
<p>​		本地模式只需要把Spark安装包解压后修改常用配置即可使用，不需要启动Spark的Master，Worker守护进程，也不需要Hadoop服务(除非用到了HDFS)。</p>
<ol>
<li>关闭其他服务</li>
</ol>
<p>​         关闭hadoop集群相关的服务，保证单机上没有其他的服务干扰(用于验证)。</p>
<ol start="2">
<li><p>使用spark-shell查看spark的后台交互窗口</p>
<blockquote>
<p>spark-shell –master local</p>
</blockquote>
<pre><code> local：在本地启动一个线程来运行作业；
 local[N]：启动了N个线程；
 local[*]：使用了系统中所有的核；
 local[N,M]：第一个参数表示用到核的个数；第二个参数表示容许作业失败的次数
</code></pre>
</li>
</ol>
<p>​		使用jps检查，会发现只有一个SparkSubmit进程</p>
<p><img src="/images/image-20211020232305691.png" alt="image-20211020232305691"></p>
<p>​		这里一个SparkSubmit进程包揽了上述spark运行中的所有进程角色。</p>
<p><img src="/images/image-20211020232737782.png" alt="image-20211020232737782"></p>
<p>​		3. 使用spark-submit脚本提交本地模式下的应用:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master local --class org.apache.spark.examples.SparkPi /opt/lagou/servers/spark-2.4.5/examples/jars/spark-examples_2.12-2.4.5.jar 4000</span><br></pre></td></tr></table></figure>



<h2 id="Spark伪分布式-local-cluster"><a href="#Spark伪分布式-local-cluster" class="headerlink" title="Spark伪分布式 local-cluster"></a>Spark伪分布式 local-cluster</h2><p>​		伪分布模式是指在一台机器中模拟集群运行，相关的进程都在一台机器上。</p>
<p>​		它不用启动资源调度服务。</p>
<blockquote>
<p>spark-shell –master local-cluster[2,4]</p>
</blockquote>
<pre><code>local-cluster[N,cores,memory]
N模拟集群的 Slave（或worker）节点个数
cores模拟集群中各个Slave节点上的内核数
memory模拟集群的各个Slave节点上的内存大小
</code></pre>
<p>​		使用jps查看运行情况</p>
<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211022164944398.png" alt="image-20211022164944398"></p>
<p>​		除了 SparkSubmit 进程外，还有2个 CoarseGrainedExecutorBackend 进程，SparkSubmit依然充当全能角色，又是Client进程，又是Driver程序，还有资源管理的作用。2个CoarseGrainedExecutorBackend模拟伪分布式集群中的两个slave节点。</p>
<p>提交应用运行:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --master local-cluster[4,2,1024] --class org.apache.spark.examples.SparkPi $SPARK_HOME/examples/jars/spark-examples_2.11-2.4.5.jar 10 </span><br></pre></td></tr></table></figure>

<p>注意：伪分布式模式存在bug问题，实际上很少使用。</p>
<h2 id="Spark集群分布式应用运行"><a href="#Spark集群分布式应用运行" class="headerlink" title="Spark集群分布式应用运行"></a>Spark集群分布式应用运行</h2><h3 id="standalone模式"><a href="#standalone模式" class="headerlink" title="standalone模式"></a>standalone模式</h3><h4 id="启动standalone模式"><a href="#启动standalone模式" class="headerlink" title="启动standalone模式"></a>启动standalone模式</h4><p> 参考文档: <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/spark-standalone.html">http://spark.apache.org/docs/latest/spark-standalone.html</a></p>
<p>​		这里standalone模式与单机模式不同，需要先启动Spark的Master和worker节点，关闭yarn对应的服务。</p>
<p>启动standalone模式下的master节点：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-master.sh</span><br></pre></td></tr></table></figure>

<p>启动slave节点(Worker)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-slaves.sh</span><br></pre></td></tr></table></figure>

<p>master节点上可以看到进程如下:</p>
<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211023003248268.png" alt="image-20211023003248268"></p>
<p>其他从节点上的进程正常情况只出现Worker</p>
<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211023003451440.png" alt="image-20211023003451440"></p>
<p>此时可以通过sparkUI界面查看集群的情况:</p>
<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211023003549734.png" alt="image-20211023003549734"></p>
<p>使用spark-shell连接到集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">./bin/spark-shell --master spark://IP:PORT</span></span><br><span class="line">./bin/spark-shell --master spark://linux111:7077</span><br></pre></td></tr></table></figure>

<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211023004140212.png" alt="image-20211023004140212"></p>
<p>查看界面可以看到出现了提交的spark-shell应用作业</p>
<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211023004216783.png" alt="image-20211023004216783"></p>
<h4 id="standalone模式下的应用部署方式"><a href="#standalone模式下的应用部署方式" class="headerlink" title="standalone模式下的应用部署方式"></a>standalone模式下的应用部署方式</h4><p>​		使用spark-submit脚本提交应用程序到集群上时，有两种部署方式，分别为client&#x2F;cluster，它们的区别在与Driver运行的位置不同。</p>
<ul>
<li>client方式下，Driver运行在提交任务的客户端上，此时能够直接看到应用执行的结果，方便测试，验证， client模式是默认的部署方式。</li>
<li>cluster方式下，Driver会随机运行在Spark集群的worker节点上，在提交程序的客户端上看不到执行的结果，该方式适合在生产环境下使用。</li>
</ul>
<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211023005548428.png" alt="image-20211023005548428"></p>
<p><strong>client模式测试</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit --class org.apache.spark.examples.SparkPi $SPARK_HOME/examples/jars/spark-examples_2.12-2.4.5.jar 1000</span><br></pre></td></tr></table></figure>

<p>当上述命令执行中时，查看进程如下:</p>
<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211023005953962.png" alt="image-20211023005953962"></p>
<p>再次使用 jps 检查集群中的进程：<br><strong>Master</strong>进程做为cluster manager，管理集群资源<br><strong>Worker</strong> 管理节点资源<br><strong>SparkSubmit</strong> 做为Client端，运行 Driver 程序。Spark Application执行完成，进程终止<br><strong>CoarseGrainedExecutorBackend</strong>，executor角色，运行在Worker上，用来并发执行应用程序</p>
<p><strong>cluste模式测试试</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit --class org.apache.spark.examples.SparkPi --deploy-mode cluster $SPARK_HOME/examples/jars/spark-examples_2.12-2.4.5.jar 1000</span><br></pre></td></tr></table></figure>

<p>指定deploy-mode类型为cluster</p>
<p>查看集群的机器上的进程</p>
<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211023011840456.png" alt="image-20211023011840456"></p>
<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211023011901383.png" alt="image-20211023011901383"></p>
<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211023011932162.png" alt="image-20211023011932162"></p>
<p>在本次测试的三个集群机器上除了Master和Worker进程外的以下进程如下:</p>
<p><strong>SparkSubmit</strong> 进程会在应用程序提交给集群之后就退出,这里与</p>
<p><strong>CoarseGrainedExecutorBackend</strong>Worker节点上会启动 CoarseGrainedExecutorBackend用来执行程序</p>
<p><strong>DriverWrapper</strong> Master会在集群中选择一个 Worker 进程生成一个子进程 DriverWrapper 来启动 Driver 程序(在本次测试中是linux111机器)进程会占用 Worker 进程的一个core（缺省分配1个core，1G内存<br>        应用程序的结果，会在执行 Driver 程序的节点的 stdout 中输出，而不是打印在屏幕上，本测试中在linux111机器$SPARK_HOME&#x2F;work&#x2F;driver-20211023011422-0002目录下可以看到提交的jar文件，以及输出日志和错误日志。</p>
<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211023012524177.png" alt="image-20211023012524177"></p>
<h4 id="Spark-standalone集群的高可用配置"><a href="#Spark-standalone集群的高可用配置" class="headerlink" title="Spark standalone集群的高可用配置"></a>Spark standalone集群的高可用配置</h4><p>​		Spark standalone集群是Master-Slave架构的集群模式，同样存在着Master节点崩溃的问题，在Spark中提供了两种方案来解决Master节点崩溃所产生的问题:</p>
<p>(1). 基于zookeeper的StandBy Master, 适用于生产环境中，将Spark连接到zookeeper，利用zookeeper提供的选举和状态保存的功能，当一个Master节点处于Active状态时，其他Master处于StandBy状态。</p>
<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211024155544443.png" alt="image-20211024155544443"></p>
<p>(2). 基于文件系统的单点恢复。主要用于开发或者测试环境。将Spark Application 和 Worker 的注册信息保存在文件中，一旦Master发生故障，就可以重新启动Master进程，将系统恢复到之前的状态。</p>
<p>​		<img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211024232122054.png" alt="image-20211024232122054"></p>
<p>这里主要说明下基于Zookeeper的高可用配置方式。</p>
<p>​		文档参考 <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/spark-standalone.html#standby-masters-with-zookeeper">http://spark.apache.org/docs/latest/spark-standalone.html#standby-masters-with-zookeeper</a></p>
<p>​		要求先将zookeeper集群启动好，测试中zk集群节点分别为linux111、linux112、linux113。</p>
<ol>
<li><p>关闭spark standby集群的Master和Worker进程</p>
</li>
<li><p>进入$SPARK_HOME&#x2F;conf目录下修改spark-env.sh文件，并分发到spark集群的其他节点上</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">因为要启动多个Master节点，所以这里不用指定</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="built_in">export</span> SPARK_MASTER_HOST=linux111</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="built_in">export</span> SPARK_MASTER_PORT=7077</span></span><br><span class="line"></span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=linux111,linux112,linux113 -Dspark.deploy.zookeeper.dir=/spark&quot;</span><br></pre></td></tr></table></figure>

</li>
<li><p>修改玩个节点上的配置文件后启动集群</p>
<p> 分别启动linux111和linux112上的Master进程，启动所有的slave节点上的worker进程。</p>
<p> <img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211024162150817.png" alt="image-20211024162150817"></p>
</li>
<li><p>查看界面效果</p>
<p> <img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211024162302902.png" alt="image-20211024162302902"></p>
</li>
</ol>
<p>关闭linux111上的Master，观察linux112上的Master是否会由standby转换成Active状态。</p>
<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211024162729723.png" alt="image-20211024162729723"></p>
<ol start="5">
<li><p>查看zookeeper上的&#x2F;spark路径下的内容</p>
<p> <img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211024163128726.png" alt="image-20211024163128726"></p>
<p> 这里记录了master节点的状态信息。</p>
</li>
</ol>
<h4 id="History-Server配置"><a href="#History-Server配置" class="headerlink" title="History Server配置"></a>History Server配置</h4><p>​		历史记录服务需要在spark的配置文件中开启日志记录功能</p>
<p>spark-default.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark.master                     spark://linux111:7077</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">开启日志功能</span></span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">记录存储在HDFS集群上的目录</span></span><br><span class="line">spark.eventLog.dir               hdfs://linux111:9000/spark-eventLog</span><br><span class="line">spark.eventLog.compress          true</span><br><span class="line">spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line">spark.driver.memory              512m</span><br></pre></td></tr></table></figure>





<p>spark-env.sh修改</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=50 -Dspark.history.fs.logDirectory=hdfs://linux121:9000/spark-eventlog&quot; </span><br></pre></td></tr></table></figure>



<p>启动日志记录服务</p>
<ol>
<li><p>先启动HDFS服务，因为日志需要写道集群上</p>
</li>
<li><p>启动HistoryServer服务</p>
<p> $SPARK_HOME&#x2F;sbin&#x2F;start-history-server.sh</p>
</li>
<li><p>web地址查看: <a target="_blank" rel="noopener" href="http://linux111:18080/">http://linux111:18080/</a></p>
<p> <img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211025004224448.png" alt="image-20211025004224448"></p>
<p> 图中已有的两个任务记录对应在hdfs上指定的目录下</p>
<p> <img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211025004320237.png" alt="image-20211025004320237"></p>
</li>
</ol>
<h3 id="Spark-on-Yarn模式"><a href="#Spark-on-Yarn模式" class="headerlink" title="Spark on Yarn模式"></a>Spark on Yarn模式</h3><p>​		参考文档: <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/running-on-yarn.html">http://spark.apache.org/docs/latest/running-on-yarn.html</a></p>
<p>​		需要启动HDFS和yarn服务，同时关闭standalone模式下的Master和worker进程。</p>
<p>​		在yarn模式中也有两种提交运行方式</p>
<ul>
<li><p>yarn-client</p>
</li>
<li><p>yarn-cluster</p>
<p>  这两种的区别在与Driver执行的位置不同。</p>
</li>
</ul>
<p>要使用spark on yar模式需要在spark配置文件中包含以下配置 </p>
<ol>
<li><p>spark-env.sh</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hadoop相关的路径需要告诉spark</span></span><br><span class="line">export HADOOP_HOME=/opt/lagou/servers/hadoop-2.9.2</span><br><span class="line">export HADOOP_CONF_DIR=/opt/lagou/servers/hadoop-2.9.2/etc/hadoop</span><br></pre></td></tr></table></figure>
</li>
<li><p>spark-default.conf</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定spark应用在yarn上的historyServer地址， 这个地址与spark的历史记录服务地址要一致，这样才能从yarn的界面跳转到spark任务的执行记录界面。</span></span><br><span class="line">spark.yarn.historyServer.address linux111:18080</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">优化，事先将jar上传到hdfs集群路径</span></span><br><span class="line">spark.yarn.jars    hdfs:///spark-yarn/jars/*.jar</span><br></pre></td></tr></table></figure>

</li>
<li><p>应用部署方式</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">client</span>  </span><br><span class="line">./spark-submit --master yarn --class org.apache.spark.examples.SparkPi --deploy-mode client $SPARK_HOME/examples/jars/spark-examples_2.12-2.4.5.jar 1000 </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">cluster</span></span><br><span class="line">./bin/spark-submit --master yarn --class org.apache.spark.examples.SparkPi --deploy-mode cluster $SPARK_HOME/examples/jars/spark-examples_2.12-2.4.5.jar 1000</span><br></pre></td></tr></table></figure>

<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211025014823417.png" alt="image-20211025014823417"></p>
<p>使用spark on yarn的方式，部署的应用执行情况就可以在yarn的应用追踪界面查看了。</p>
<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211025015854747.png" alt="image-20211025015854747"></p>
<p>查看应用的执行记录。</p>
<p>点击History链接即可跳转到spark的history Server界面查看spark应用的执行过程。</p>
<p><img src="/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F.assets/image-20211025015959424.png" alt="image-20211025015959424"></p>
<p>这里是因为将spark的historyServer服务与yarn的historyServer服务整合到了一起。</p>
<p> 小结:</p>
<p>​		spark的yarn部署模式需要启动的服务包括：HDFS, YARN, spark HistoryServer， yarn jobHistoryServer</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/spark/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/" data-id="clumsd6xb000sabr71qbd2kcy" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-spark/spark编程-SQL&amp;DataFrame&amp;DataSet" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/spark/spark%E7%BC%96%E7%A8%8B-SQL&DataFrame&DataSet/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.936Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>SparkSQL处理结构化数据。</p>
<p>​		在Spark 0.x版的时候推出了Shark，Shark与Hive是紧密关联的，Shark底层很多东西还是依赖于Hive，修改了内存管理、物理计划、执行三个模块，底层使用Spark的基于内存的计算模型，性能上比Hive提升了很多倍。<br>​		Shark更多是对Hive的改造，替换了Hive的物理执行引擎，提高了执行速度。但Shark继承了大量的Hive代码，因此给优化和维护带来了大量的麻烦。<br>​		在Spark 1.x的时候Shark被淘汰。在2014 年7月1日的Spark Summit 上，Databricks宣布终止对Shark的开发，将重点放到 Spark SQL 上。</p>
<p>SparkSQL数据抽象</p>
<p>​		SparkSQL提供了两个新的抽象，分别是DataFrame和DataSet。</p>
<ol>
<li><p>DataFrame</p>
<p> 与RDD类似，DataFrame也是一个分布式数据集：</p>
<ul>
<li>DataFrame可以看做分布式 Row 对象的集合，提供了由列组成的详细模式信息，使其可以得到优化。</li>
<li>DataFrame 不仅有比RDD更多的算子，还可以进行执行计划的优化</li>
<li>DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema</li>
<li>DataFrame也支持嵌套数据类型（struct、array和map）</li>
<li>DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低</li>
<li>Dataframe的劣势在于在编译期缺少类型安全检查，导致运行时出错</li>
</ul>
</li>
</ol>
<p><img src="/./images/image-20210803183924243.png" alt="image-20210803183924243"></p>
<ol start="2">
<li><p>DataSet </p>
<p> 与RDD相比，保存更多的描述信息，概念上等同于关系型数据库中的二维表；</p>
<p> 与DataFrame相比，保存了类型信息， 是强类型的，提供编译时类型检查。</p>
<p> 调用DataSet的方法会先生成逻辑计划，然后Spark的优化器进行优化，最终生成物理计划，提交到集群中运行。</p>
</li>
</ol>
<p>三者之间转换</p>
<p>RDD, DataSet, DataFrame</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/spark/spark%E7%BC%96%E7%A8%8B-SQL&DataFrame&DataSet/" data-id="clumsd6xb000tabr70td15px8" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-spark/spark原理刨析" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/spark/spark%E5%8E%9F%E7%90%86%E5%88%A8%E6%9E%90/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.935Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>内存管理</p>
<p>执行spark应用时，Spark集群会启动Driver和Executor两种JVM进程。</p>
<p>堆内内存与堆外内存</p>
<p>Executor的内存管理建立在JVM的内存管理之上，Spark对JVM的内存进行了更为详细的分配，以充分利用内存。同时，Spark引入了堆外内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化内存的使用。</p>
<ol>
<li><p>队内内存</p>
<p> 堆内内存的大小是由Spark应用程序启动时的executor-memory或spark.executor.memory参数配置</p>
</li>
</ol>
<p>编码的优化</p>
<ol>
<li><p>RDD复用</p>
<p> 避免创建重复的RDD，在开发中，对于同一数据，只创建一份RDD</p>
</li>
<li><p>RDD缓存&#x2F;持久化</p>
<ul>
<li>当多次对同一RDD执行算子操作时，每一次都会对这个RDD之前的父RDD重新计算一次，这种情况是必须避免的</li>
<li>对多次使用的RDD进行持久化，通过持久化将多次使用的公共RDD数据缓存到内存或磁盘中</li>
<li>RDD的持久化可以进行序列化操作，当内存无法将RDD数据完整存放时，通过序列化的方式减小数据体积，将数据完整存储在内存中</li>
</ul>
</li>
<li><p>巧用filter</p>
<ul>
<li>尽可能早的执行filter操作，过滤掉无用数据</li>
<li>在filter过滤掉较多数据之后，使用coalesce对数据重分区</li>
</ul>
</li>
<li><p>使用高性能的算子</p>
<ul>
<li>避免使用groupByKey，更具场景使用reduceByKey，aggregateByKey</li>
<li>coalesce,repartition,选择没有shuffle的操作</li>
<li>foreachPartition优化操作</li>
<li>map,mapPartitions,选择合适的选择算子，mapPartitions性能更好，但面对大数量时容易发声OOM</li>
<li>用repartitionAndSortWithPartitions代替repartition+sort操作</li>
<li>合理使用cache、persist、checkpoint，选择合理的存储级别</li>
<li>减少对数据源的扫描操作</li>
</ul>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/spark/spark%E5%8E%9F%E7%90%86%E5%88%A8%E6%9E%90/" data-id="clumsd6xa000oabr7272m802f" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-spark/spark编程基础" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/spark/spark%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.935Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-RDD介绍"><a href="#1-RDD介绍" class="headerlink" title="1. RDD介绍"></a>1. RDD介绍</h1><p>​		RDD是Spark数据的基础单元，Spark编程是围绕着在RDD上创建和执行操作来进行的，它们是跨集群进行分区的不可变集合，如果某个分区丢失，这些分区可以重建(通过重新计算)。 </p>
<p>RDD的特征</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">* Internally, each RDD is characterized by five main properties:</span><br><span class="line">*</span><br><span class="line">*  - A list of partitions</span><br><span class="line">*  - A function for computing each split</span><br><span class="line">*  - A list of dependencies on other RDDs</span><br><span class="line">*  - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</span><br><span class="line">*  - Optionally, a list of preferred locations to compute each split on (e.g. block locations for</span><br><span class="line">*    an HDFS file)</span><br></pre></td></tr></table></figure>

<ol>
<li><p>分区的列表：</p>
<p> Partition(分区)是数据的基本组成单位，对于RDD来说，每个分区都会被一个计算任务来处理，用户可以在创建RDD时指定分区的个数，如果没有指定则采用默认值。</p>
</li>
<li><p>对每个分区都有计算函数对数据进行处理：</p>
<p> Spark中RDD的计算是以分区为单位的，每个RDD都会实现compute函数，compute函数会对迭代器进行组合，不需要保存每次计算的结果。</p>
</li>
<li><p>多个RDD之间存在依赖的关系，构成RDD之间的依赖列表：</p>
<p> RDD的每一次转换都会生成一个新的RDD，RDD之间形成类似流水线一样的前后依赖关系，在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p>
</li>
<li><p>key-values类型的RDD存在相应的分区器</p>
<p> 对于key-value的RDD而言，可能存在分区器(Partitioner)， Spark实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另一个是基于范围了RangePartitioner，只有key-value的RDD才有可能有Partitioner，非key-value类型的RDD的Partitioner值为None。Partitioner函数决定了RDD本身的分区数量，也决定了parent RDD Shuffle输出的分片数量。</p>
</li>
<li><p>优先计算的数据位置列表：</p>
<p> Spark任务调度时优先将计算移动到数据存储的位置。</p>
</li>
</ol>
<p>RDD的特点</p>
<ol>
<li><p>分区</p>
<p> RDD逻辑上分区，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。</p>
<p> 如果RDD是通过文件系统构建的，则compute函数是读取指定文件系统中的数据；</p>
<p> 如果RDD是通过其他的RDD转换而来，则comoute函数是执行转换逻辑将其他RDD的数据进行转换。</p>
</li>
<li><p>只读</p>
<p> RDD是只读的， 不可变的集合，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。</p>
<p> RDD通过丰富的转换算子操作来转换成另一个RDD，</p>
<p> RDD的算子包括两类:</p>
<ul>
<li>transformation 用来对RDD进行转换，该类算子是延迟执行(Lazy)</li>
<li>action 用来触发RDD的计算，得到相关计算结果或者将RDD保存在文件系统中。</li>
</ul>
</li>
<li><p>依赖关系</p>
<p> RDD通过操作算子进行转换，转换得到的新RDD包含了从其他RDD衍生所必须的信息， RDD之间维护者这种依赖的血缘关系(lineage)，也称之为依赖， 依赖包括两种：</p>
<ul>
<li>窄依赖 RDD之间分区是n:1或n:1的</li>
<li>宽依赖  子RDD的每个分区与父RDD的每个分区都有关，是n:m的关系， 一般是有shuffle的过程</li>
</ul>
</li>
</ol>
<p><img src="/./images/image-20210623235829982.png" alt="image-20210623235829982"></p>
<ol start="4">
<li><p>缓存</p>
<p> 可以控制存储的级别(内存，磁盘) 来进行缓存。</p>
<p> 应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到RDD的时候，会直接从缓存处获取而不用再根据血缘关系计算，这样就加速后期的重用。</p>
<p> <img src="/./images/image-20210629001401157.png" alt="image-20210629001401157"></p>
</li>
<li><p>checkpoint</p>
<p> ​		RDD的血缘关系使得它可以实现容错，当RDD的某个分区数据失败或缺失时，可以通过血缘关系来重建。但是对于长时间迭代的应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代中出错，那么需要通过非常长的血缘关系去重建，势必影响性能。</p>
<p> ​	RDD中支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道父代RDD了，可以直接从checkpoint处拿到数据。</p>
</li>
</ol>
<p>RDD与IO之间的关系</p>
<h1 id="2-Spark编程模型"><a href="#2-Spark编程模型" class="headerlink" title="2. Spark编程模型"></a>2. Spark编程模型</h1><p><img src="/./images/image-20211026011823666.png" alt="image-20211026011823666"></p>
<p>图上左边的各类数据源通过sparkContext连接，经过RDD的方法，转换算子和动作算子的计算最终转换成新的RDD或将结果输出到外部数据源。</p>
<ul>
<li>RDD表示数据对象</li>
<li>通过对象上的方法调用来对RDD进行转换</li>
<li>最终显示结果 或 将结果输出到外部数据源</li>
<li>RDD转换算子称为Transformation是Lazy的（延迟执行）</li>
<li>只有遇到Action算子，才会执行RDD的转换操作</li>
</ul>
<h1 id="3-RDD的创建方式"><a href="#3-RDD的创建方式" class="headerlink" title="3. RDD的创建方式"></a>3. RDD的创建方式</h1><ol>
<li>集合创建RDD</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 使用parallelize方法来创建</span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">3</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line"># 使用makeRDD创建</span><br><span class="line">scala&gt; sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">res0: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at &lt;console&gt;:<span class="number">25</span></span><br></pre></td></tr></table></figure>



<ol start="2">
<li><p>从文件创建</p>
</li>
<li><p>从其他RDD创建</p>
<p> ​	</p>
</li>
<li><p>直接new一个新的RDD对象</p>
</li>
</ol>
<h2 id="2-1-变换算子-Transformation-和动作算子-Action"><a href="#2-1-变换算子-Transformation-和动作算子-Action" class="headerlink" title="2.1 变换算子(Transformation)和动作算子(Action)"></a>2.1 变换算子(Transformation)和动作算子(Action)</h2><p>​		变换会根据当前的RDD来定义新的RDD；动作则是从从RDD返回值。</p>
<h3 id="Transformation算子"><a href="#Transformation算子" class="headerlink" title="Transformation算子"></a>Transformation算子</h3><p>​		每一次Transformation操作都会产生新的RDD，功下一个转换使用; 转换得到的RDD是惰性求值的，整个转换过程只是记录了转换的轨迹，并不会发生真正的计算，只有遇到Action操作时，才会发生真正的计算，从血缘关系的源头开始，进行物理的转换操作。</p>
<p>**map(func)**：对数据集中的每个元素都使用func，然后返回一个新的RDD<br>**filter(func)**：对数据集中的每个元素都使用func，然后返回一个包含使func为true的元素构成的RDD<br>**flatMap(func)**：与 map 类似，每个输入元素被映射为0或多个输出元素<br>**mapPartitions(func)**：和map很像，但是map是将func作用在每个元素上，而mapPartitions是func作用在整个分区上。假设一个RDD有N个元素，M个分区（N &gt;&gt; M），那么map的函数将被调用N次，而mapPartitions中的函数仅被调用M次，一次处理一个分区中的所有元素<br>**mapPartitionsWithIndex(func)**：与 mapPartitions 类似，多了分区的索引值的信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val rdd1=sc.parallelize(1 to 10)</span></span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val rdd2=rdd1.map(_*2)</span></span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at &lt;console&gt;:25</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">val rdd3=rdd2.filter(_&gt;10)</span></span><br><span class="line">rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at filter at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// RDD 是分区，rdd1有几个区，每个分区有哪些元素 </span><br><span class="line">rdd1.getNumPartitions</span><br><span class="line">rdd1.partitions.length</span><br><span class="line"></span><br><span class="line">// 对分区中每个分区中的元素组合成列表</span><br><span class="line">rdd1.mapPartitions&#123;iter=&gt;Iterator(s&quot;$&#123;iter.toList&#125;&quot;)&#125;.collect</span><br><span class="line">rdd1.mapPartitions&#123;iter =&gt;Iterator(s&quot;$&#123;iter.toArray.mkString(&quot;-&quot;)&#125;&quot;) &#125;.collect</span><br></pre></td></tr></table></figure>

<p>以上都是transformation操作，没有执行， 需要引入Action算子才会执行。</p>
<ul>
<li><p>map：每次处理一条数据</p>
</li>
<li><p>mapPartitions：每次处理一个分区的数据，分区的数据处理完成后，数据才能释放，资源不足时容易导致OOM</p>
<p>  最佳实践：当内存资源充足时，建议使用mapPartitions，以提高处理效率</p>
</li>
</ul>
<h3 id="聚合操作"><a href="#聚合操作" class="headerlink" title="聚合操作"></a>聚合操作</h3><p>**groupBy(func)**：按照传入函数的返回值进行分组。将key相同的值放入一个迭代器<br>**glom()**：将每一个分区形成一个数组，形成新的RDD类型 RDD[Array[T]]<br>**sample(withReplacement, fraction, seed)**：采样算子。以指定的随机种子(seed)随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样<br>**distinct([numTasks]))**：对RDD元素去重后，返回一个新的RDD。可传入numTasks参数改变RDD分区数<br>**coalesce(numPartitions)**：缩减分区数，无shuffle<br>**repartition(numPartitions)**：增加或减少分区数，有shuffle<br>**sortBy(func, [ascending], [numTasks])**：使用 func 对数据进行处理，对处理后的结果进行排序</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">// 将 RDD 中的元素按照3的余数分组 </span><br><span class="line">val rdd = sc.parallelize(1 to 10) </span><br><span class="line">val group = rdd.groupBy(_%3) </span><br><span class="line">group.collect </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 将 RDD 中的元素每10个元素分组 </span><br><span class="line">val rdd = sc.parallelize(1 to 101) </span><br><span class="line">// 分别对每个分区做map的映射操作</span><br><span class="line">rdd.glom.map(_.sliding(10, 10).toArray) </span><br><span class="line"></span><br><span class="line">// 对数据采样。fraction采样的百分比，近似数 </span><br><span class="line">// 有放回的采样，使用固定的种子 </span><br><span class="line">rdd.sample(true, 0.2, 2).collect </span><br><span class="line">// 无放回的采样，使用固定的种子 </span><br><span class="line">rdd.sample(false, 0.2, 2).collect </span><br><span class="line">// 有放回的采样，不设置种子 </span><br><span class="line">rdd.sample(false, 0.2).collect</span><br><span class="line"></span><br><span class="line">// 数据去重 </span><br><span class="line">val random = scala.util.Random </span><br><span class="line">val arr:Vector[Int] = (1 to 20).map(x =&gt; random.nextInt(10))</span><br><span class="line">// 将Vector转换成RDD</span><br><span class="line">val rdd = sc.makeRDD(arr)</span><br><span class="line">rdd.distinct.collect</span><br><span class="line"></span><br><span class="line">// RDD重分区 </span><br><span class="line">val rdd1 = sc.range(1, 10000, numSlices=10)</span><br><span class="line">val rdd2 = rdd1.filter(_%2==0)</span><br><span class="line">rdd2.getNumPartitions</span><br><span class="line"></span><br><span class="line">// 减少分区数；都生效了 </span><br><span class="line">val rdd3 = rdd2.repartition(5)</span><br><span class="line">rdd3.getNumPartitions</span><br><span class="line">val rdd4 = rdd2.coalesce(5)</span><br><span class="line">rdd4.getNumPartitions</span><br><span class="line"> </span><br><span class="line">// 增加分区数</span><br><span class="line">val rdd5 = rdd2.repartition(20)</span><br><span class="line">rdd5.getNumPartitions</span><br><span class="line"> </span><br><span class="line">// 增加分区数，这样使用没有效果</span><br><span class="line">val rdd6 = rdd2.coalesce(20)</span><br><span class="line">rdd6.getNumPartitions</span><br><span class="line"> </span><br><span class="line">// 增加分区数的正确用法</span><br><span class="line">val rdd6 = rdd2.coalesce(20, true)</span><br><span class="line">rdd6.getNumPartitions</span><br><span class="line"> </span><br></pre></td></tr></table></figure>

<ul>
<li>repartition：增大或减少分区数；有shuffle</li>
<li>coalesce：一般用于减少分区数（此时无shuffle）</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// RDD元素排序 </span><br><span class="line">val random = scala.util.Random</span><br><span class="line">val arr = (1 to 20).map(x =&gt; random.nextInt(10))</span><br><span class="line">val rdd = sc.makeRDD(arr)</span><br><span class="line">rdd.collect</span><br><span class="line"> </span><br><span class="line">// 数据全局有序，默认升序 </span><br><span class="line">rdd.sortBy(x=&gt;x).collect</span><br><span class="line">// 降序</span><br><span class="line">rdd.sortBy(x=&gt;x,false).collect</span><br></pre></td></tr></table></figure>



<h3 id="RDD之间交、并、差的算子"><a href="#RDD之间交、并、差的算子" class="headerlink" title="RDD之间交、并、差的算子"></a>RDD之间交、并、差的算子</h3><p>intersection(otherRDD) ： 交集</p>
<p>union(otherRDD) ： 并集</p>
<p>subtract (otherRDD) ： 差集</p>
<p>cartesian(otherRDD)：笛卡尔积</p>
<p>zip(otherRDD)  : 两个RDD组合成key-value形式的RDD，默认两个RDD的partition数量以及元素数量都相同，否则抛出异常</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.range(1,21)</span><br><span class="line">val rdd2 = sc.range(10,31)</span><br><span class="line"></span><br><span class="line">// 计算交集</span><br><span class="line">rdd1.intersection(rdd2).sortBy(x=&gt;x).collect</span><br><span class="line">// 元素求并集，不去重 </span><br><span class="line">rdd1.union(rdd2).sortBy(x=&gt;x).collect </span><br><span class="line">// 差集</span><br><span class="line">rdd1.subtract(rdd2).sortBy(x=&gt;x).collect </span><br><span class="line"></span><br><span class="line">// 检查分区数 </span><br><span class="line">rdd1.intersection(rdd2).getNumPartitions</span><br><span class="line">rdd1.union(rdd2).getNumPartitions </span><br><span class="line">rdd1.subtract(rdd2).getNumPartitions </span><br><span class="line"> </span><br><span class="line">// 笛卡尔积 </span><br><span class="line">val rdd1 = sc.range(1, 5) </span><br><span class="line">val rdd2 = sc.range(6, 10) </span><br><span class="line">rdd1.cartesian(rdd2).collect </span><br><span class="line">// 检查分区数 </span><br><span class="line">rdd1.cartesian(rdd2).getNumPartitions </span><br><span class="line"> </span><br><span class="line">// 拉链操作 </span><br><span class="line">rdd1.zip(rdd2).collect </span><br><span class="line">rdd1.zip(rdd2).getNumPartitions </span><br><span class="line"> </span><br><span class="line">// zip操作要求：两个RDD的partition数量以及元素数量都相同，否则会抛出异常 </span><br><span class="line">val rdd2 = sc.range(6, 20) </span><br><span class="line">rdd1.zip(rdd2).collect </span><br></pre></td></tr></table></figure>

<ul>
<li>union是窄依赖。得到的RDD分区数为：两个RDD分区数之和</li>
<li>cartesian也是窄依赖。得到的RDD分区数为：两个RDD分区数之积；得到RDD的数据量是两个RDD数据量的积。会有很严重的数据膨胀，慎用</li>
</ul>
<h3 id="Action算子"><a href="#Action算子" class="headerlink" title="Action算子"></a>Action算子</h3><p>Action触发Job。一个Spark程序(Driver程序)包含了多少 Action 算子，那么就有多少Job。</p>
<p>典型的Action算子：collect&#x2F;count</p>
<p>collect算子代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return an array that contains all of the elements in this RDD.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @note This method should only be used if the resulting array is expected to be small, as</span></span><br><span class="line"><span class="comment"> * 所有数据结果均存放在内存中。 (all the data is loaded into the driver&#x27;s memory.)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collect</span></span>(): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    <span class="comment">// 启动一个runJob</span></span><br><span class="line">  <span class="keyword">val</span> results = sc.runJob(<span class="keyword">this</span>, (iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; iter.toArray)</span><br><span class="line">  <span class="type">Array</span>.concat(results: _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>first()：Return the first element in this RDD<br>take(n)：Take the first num elements of the RDD<br>top(n)：按照默认（降序）或者指定的排序规则，返回前num个元素。<br>takeSample(withReplacement, num, [seed])：返回采样的数据<br>foreach(func) &#x2F; foreachPartition(func)：与map、mapPartitions类似，区别是 foreach 是 Action<br>saveAsTextFile(path) &#x2F; saveAsSequenceFile(path) &#x2F; saveAsObjectFile(path)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">// 返回统计信息。仅能作用 RDD[Double] 类型上调用 </span><br><span class="line">val rdd1 = sc.range(1, 101) </span><br><span class="line">rdd1.stats </span><br><span class="line"> </span><br><span class="line">val rdd2 = sc.range(1, 101)</span><br><span class="line"> </span><br><span class="line">// count在各种类型的RDD上，均能调用 </span><br><span class="line">rdd1.zip(rdd2).count </span><br><span class="line"> </span><br><span class="line">// 聚合操作 </span><br><span class="line">val rdd = sc.makeRDD(1 to 10, 2) </span><br><span class="line">rdd.reduce(_+_) </span><br><span class="line">rdd.fold(0)(_+_) </span><br><span class="line">rdd.fold(1)(_+_) </span><br><span class="line">rdd.fold(1)((x, y) =&gt; &#123; </span><br><span class="line">  println(s&quot;x=$x, y=$y&quot;) </span><br><span class="line">  x+y </span><br><span class="line">&#125;) </span><br><span class="line">rdd.aggregate(0)(_+_, _+_) </span><br><span class="line">rdd.aggregate(1)(_+_, _+_) </span><br><span class="line">rdd.aggregate(1)( </span><br><span class="line">(a, b) =&gt; &#123; </span><br><span class="line">  println(s&quot;a=$a, b=$b&quot;) </span><br><span class="line">  a+b </span><br><span class="line">&#125;, </span><br><span class="line">(x, y) =&gt; &#123; </span><br><span class="line">  println(s&quot;x=$x, y=$y&quot;) </span><br><span class="line">  x+y </span><br><span class="line">&#125;) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// first / take(n) / top(n) ：获取RDD中的元素。多用于测试 </span><br><span class="line">rdd.first </span><br><span class="line">rdd.take(10) </span><br><span class="line">rdd.top(10) </span><br><span class="line"> </span><br><span class="line">// 采样并返回结果 </span><br><span class="line">rdd.takeSample(false, 5) </span><br><span class="line"> </span><br><span class="line">// 保存文件到指定路径(rdd有多少分区，就保存为多少文件，保存文件时注意小文件问题) </span><br><span class="line">rdd.saveAsTextFile(&quot;data/t1&quot;) </span><br></pre></td></tr></table></figure>



<h2 id="2-2-Key-Value-RDD"><a href="#2-2-Key-Value-RDD" class="headerlink" title="2.2 Key-Value RDD"></a>2.2 Key-Value RDD</h2><p>键值对RDD通常用来进行聚合计算。</p>
<p>key-value类型的RDD操作都在PairRDDFunctions.scala文件中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">Array</span>((<span class="string">&quot;spark&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;hadoop&quot;</span>, <span class="number">26</span>), (<span class="string">&quot;hadoop&quot;</span>, <span class="number">23</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">15</span>), (<span class="string">&quot;scala&quot;</span>, <span class="number">26</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">25</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">23</span>), (<span class="string">&quot;hadoop&quot;</span>, <span class="number">16</span>), (<span class="string">&quot;scala&quot;</span>, <span class="number">24</span>), (<span class="string">&quot;spark&quot;</span>, <span class="number">16</span>))) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rdd.aggregateByKey((<span class="number">0</span>,<span class="number">0</span>))(</span><br><span class="line">    (x,y) =&gt; &#123;println(<span class="string">s&quot;x=<span class="subst">$x</span>, y=<span class="subst">$y</span>&quot;</span>); (x._1 + y, x._2 + <span class="number">1</span>)&#125;,</span><br><span class="line">    (a,b) =&gt; &#123;println(<span class="string">s&quot;a=<span class="subst">$a</span>, b=<span class="subst">$b</span>&quot;</span>); (a._1 + b._1, a._2 + b._2)&#125;</span><br><span class="line">).mapValues(x=&gt;x._1.toDouble/x._2).collect</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rdd.aggregateByKey(scala.collection.mutable.<span class="type">ArrayBuffer</span>[<span class="type">Int</span>]())(</span><br><span class="line">    (x,y) =&gt; &#123;x.append(y);x&#125;,</span><br><span class="line">    (a,b) =&gt; &#123;a ++ b&#125;  <span class="comment">// ArrayBuffer相加</span></span><br><span class="line">).collect</span><br><span class="line"></span><br></pre></td></tr></table></figure>




      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/spark/spark%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/" data-id="clumsd6xb000vabr782105ltm" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-scala/Scala笔记" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/scala/Scala%E7%AC%94%E8%AE%B0/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.921Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h1><p>scala是完全<strong>面向对象</strong>的<strong>函数式</strong>语言。</p>
<pre><code>    - 运行在JVM之上：Scala不仅利用了JVM的高性能，也可以使用Java中丰富的工具类及类库生态
    - 静态类型：Scala与Java相似，也是需要编译后才能在JVM中运行
    - 支持面向对象编程，Scala中一切都是对象，即使是数值类型
    - 函数式编程，Scala完全支持函数式编程，函数式编程被视为解决并发，大数据以及代码正确性问题的最佳工具，使用不可变值，被视为一等公民的函数，无副作用的函数，高阶函数以及函数集合，有助于编写简洁，强大又正确的代码
    - 复杂的类型系统， Scala对Java类型系统进行扩展，提供更灵活的范型以及一些有助于提高代码正确性的改进。通过使用类型推演，Scala编写的代码能够和动态类型语言编写的代码一样精简
    - 简洁、优雅、灵活的语法
</code></pre>
<ul>
<li>可扩展的架构， 使用Scala，能够编写简短的解释脚本，并将其粘合成大型的分布式应用。<ul>
<li>使用trait实现的混合结构</li>
<li>抽象类型成员和泛型</li>
<li>嵌套类</li>
<li>显示字类型（self type）</li>
</ul>
</li>
</ul>
<h1 id="2-基础语法"><a href="#2-基础语法" class="headerlink" title="2. 基础语法"></a>2. 基础语法</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">object 关键字， 声明一个单例对象， 它是同名类(HelloWorld)的伴生对象</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloWorld</span> </span>&#123;</span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  main 方法， 执行开始的地方.</span></span><br><span class="line"><span class="comment">  main方法需要定义在object声明的对象中</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// scala的打印方式</span></span><br><span class="line">    println(<span class="string">&quot;hello world&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用java中的方法</span></span><br><span class="line">    <span class="type">System</span>.out.println(<span class="string">&quot;hello world in java&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="2-1-伴生对象与伴生类"><a href="#2-1-伴生对象与伴生类" class="headerlink" title="2.1 伴生对象与伴生类"></a>2.1 伴生对象与伴生类</h2><p>java中定义一个对象</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Student</span> &#123;</span><br><span class="line">    <span class="comment">// 定义一个全局的静态属性</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="type">String</span> <span class="variable">school</span> <span class="operator">=</span> <span class="string">&quot;Peking&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> age;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Student</span><span class="params">(String name, <span class="type">int</span> age)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.name = name;</span><br><span class="line">        <span class="built_in">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">printInfo</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 这里获取school的属性是通过类来获取的，而不是由对象中取得</span></span><br><span class="line">        System.out.println(<span class="built_in">this</span>.name + <span class="string">&quot; &quot;</span> + <span class="built_in">this</span>.age + <span class="string">&quot; &quot;</span> + Student.school);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Student</span> <span class="variable">tom</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Student</span>(<span class="string">&quot;tom&quot;</span>, <span class="number">18</span>);</span><br><span class="line">        <span class="type">Student</span> <span class="variable">jack</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Student</span>(<span class="string">&quot;jack&quot;</span>, <span class="number">19</span>);</span><br><span class="line">        tom.printInfo();</span><br><span class="line">        jack.printInfo();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>scala示例,使用伴生对象</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">Student对象的伴生类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">printInfo</span></span>(): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="comment">// 这里school属性就是通过Student类的对象来获取</span></span><br><span class="line">    println(<span class="keyword">this</span>.name+<span class="string">&quot; &quot;</span> + <span class="keyword">this</span>.age + <span class="string">&quot; &quot;</span> + <span class="type">Student</span>.school)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">Student类的伴生对象</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Student</span></span>&#123;</span><br><span class="line">  <span class="keyword">val</span> school = <span class="string">&quot;Peking&quot;</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> tom = <span class="keyword">new</span> <span class="type">Student</span>(<span class="string">&quot;tom&quot;</span>,<span class="number">18</span>)</span><br><span class="line">    tom.printInfo()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>为什么使用伴生对象?</p>
<p>​    在Java中，定义类中可以使用static定义一个静态的全局属性，获取这个属性是通过类名引用或类方法获取的，这时他并不是一个具体的对象，在scala的面向对象概念中， 对于一个类型的全局属性，是写在它的伴生对象中，这样就是通过对象来获取这个属性，由于伴生对象是一个单例的对象，因此也保证了它是全局的。</p>
<h2 id="2-2-变量与常量"><a href="#2-2-变量与常量" class="headerlink" title="2.2 变量与常量"></a>2.2 变量与常量</h2><ul>
<li><p>java中声明变量与常量</p>
<p>  变量类型 变量名称 &#x3D; 初始值   int i &#x3D; 10</p>
<p>  final 变量类型 变量名称 &#x3D; 初始值   final int j &#x3D; 0</p>
</li>
<li><p>scala声明变量与常量</p>
<p>  var 变量名称 &#x3D; 初始值</p>
<p>  val 变量名称 &#x3D; 初始值</p>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// (1) 声明变量时，类型可以省略，编译器自动推导</span></span><br><span class="line"><span class="keyword">var</span> a1 = <span class="number">10</span></span><br><span class="line"><span class="keyword">val</span> b1 = <span class="number">23</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// (2) 类型确定后，就不能修改了，说明scala是强数据类型</span></span><br><span class="line"><span class="keyword">var</span> a2 = <span class="number">12</span></span><br><span class="line"><span class="comment">//  a2 = &quot;change&quot;   错误</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// (3) 声明变量时，如果没有给定数据类型，就必须有初始值, 如果有给出数据类型，那么也可以使用 _ 占位符，初始话为编译器给出的默认值</span></span><br><span class="line"><span class="keyword">val</span> a3 = <span class="string">&quot;hello&quot;</span></span><br><span class="line"><span class="keyword">val</span> a4: <span class="type">String</span> = _</span><br><span class="line"></span><br><span class="line"><span class="comment">// (4) 声明定义一个变量时，var修饰的变量可改变值，val修饰的变量不能修改</span></span><br><span class="line">a1 = <span class="number">20</span></span><br><span class="line"><span class="comment">//    b1 = 30  b1是val修饰的变量，这里不能再次修改值</span></span><br></pre></td></tr></table></figure>





<h2 id="2-3-数据类型"><a href="#2-3-数据类型" class="headerlink" title="2.3 数据类型"></a>2.3 数据类型</h2><p><img src="/Scala%E7%AC%94%E8%AE%B0.assets/unified-types-diagram.svg" alt="Scala Type Hierarchy"></p>
<p>Scala中的一切数据都是对象，是Any的子类；</p>
<p>低精度类型会向高精度类型自动转换；</p>
<p>查看scala中的String类型，可以看到它其实是Java的别名， 在scala中StringOps是Java中String的增强。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** The `String` type in Scala has methods that come either from the underlying</span></span><br><span class="line"><span class="comment"> *  Java String (see the documentation corresponding to your Java version, for</span></span><br><span class="line"><span class="comment"> *  example [[http://docs.oracle.com/javase/8/docs/api/java/lang/String.html]]) or</span></span><br><span class="line"><span class="comment"> *  are added implicitly through [[scala.collection.immutable.StringOps]].</span></span><br><span class="line"><span class="comment"> *  @group aliases</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">type</span> <span class="title">String</span>        </span>= java.lang.<span class="type">String</span></span><br></pre></td></tr></table></figure>





<h2 id="2-4-运算符"><a href="#2-4-运算符" class="headerlink" title="2.4 运算符"></a>2.4 运算符</h2><ol>
<li><p>数学运算符</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> result: <span class="type">Int</span> = <span class="number">10</span> / <span class="number">3</span></span><br><span class="line">println(result) <span class="comment">// 3</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">val</span> result2: <span class="type">Double</span> = <span class="number">10</span> / <span class="number">3</span> <span class="comment">// 结果是Int，然后向高精度转换</span></span><br><span class="line">println(result2) <span class="comment">// 3.0</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">val</span> result3: <span class="type">Double</span> = <span class="number">10.0</span> / <span class="number">3</span> </span><br><span class="line">println(result3) <span class="comment">// 3.333333333333333</span></span><br><span class="line">println(result3.formatted(<span class="string">&quot;%.2f&quot;</span>)) <span class="comment">// 保留2位小数</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">val</span> result4:<span class="type">Int</span> = <span class="number">10</span> % <span class="number">3</span></span><br><span class="line">println(result4) <span class="comment">// 1</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>关系运算符</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> s1: <span class="type">String</span> = <span class="string">&quot;hello&quot;</span></span><br><span class="line"><span class="keyword">val</span> s2: <span class="type">String</span> = <span class="keyword">new</span> <span class="type">String</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">println(s1 == s2) <span class="comment">// true</span></span><br><span class="line">println(s1.equals(s2)) <span class="comment">// true  这里如果是自定义的类型，如果没有重写equals方法，那么使用这种方法比较的对象是不行的</span></span><br><span class="line">println(s1.eq(s2)) <span class="comment">// false  scala中可以使用eq()方法来比较不同对象</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>逻辑运算符</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&amp;&amp; </span><br><span class="line">||</span><br><span class="line">!</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<p>def m(n: Int): Int &#x3D; {<br>      println(“m被调用”)<br>      n<br>    }<br>    val n &#x3D; 1<br>    println((4 &gt; 5) &amp;&amp; m(n) &gt; 0) &#x2F;&#x2F; false  前一个判断是假,那么就不会执行后面的逻辑, 这里就是使用&amp;的短路特点<br>    println((4 &lt; 5) &amp;&amp; m(n) &gt; 0) &#x2F;&#x2F; true<br>    <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">    </span><br><span class="line">4. 位运算符</span><br><span class="line"></span><br><span class="line">    ```scala</span><br><span class="line">    val a: Byte = 60 // 0011 1100</span><br><span class="line">    println(a &lt;&lt; 3) // 480  对应二进制表示0000 0001 1110 0000  Byte类型会转换成Int类型，因此不会溢出</span><br><span class="line">    val b: Short = -13 // 1111 1111 1111 1111 1111 1111 1111 0011</span><br><span class="line">    println(b &lt;&lt; 2) // -52  有符号左移2位 1111 1111 1111 1111 1111 1111 1100 1100</span><br><span class="line">    println(b &gt;&gt; 2) // -4   有符号右移2位 1111 1111 1111 1111 1111 1111 1111 1100</span><br><span class="line">    println(b &gt;&gt;&gt; 2) // 1073741820    无符号右移2位 0011 1111 1111 1111 1111 1111 1111 1100</span><br><span class="line">    val c = b &gt;&gt;&gt; 2</span><br><span class="line">    println(b.toBinaryString)</span><br><span class="line">    println(c.toBinaryString)</span><br></pre></td></tr></table></figure></p>
<pre><code>在scaa中运算符也是一个对象方法

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a+<span class="number">4</span>  -&gt; a.+(<span class="number">4</span>)</span><br><span class="line">a*<span class="number">3</span>  -&gt; a.*(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
</code></pre>
<h2 id="2-5-流程控制"><a href="#2-5-流程控制" class="headerlink" title="2.5 流程控制"></a>2.5 流程控制</h2><p>​		scala中同样可以使用if&#x2F;else, for，while等控制结构， 用法类似，其中scala对for有一些更方便的使用。</p>
<p>for循环</p>
<ol>
<li><p>for范围遍历</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 范围遍历, 这里to不是关键字，而是方法调用</span></span><br><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">10</span>)&#123;</span><br><span class="line">  println(i + <span class="string">&quot;. hello world&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1.</span>to(<span class="number">10</span>))&#123;</span><br><span class="line">  println(i + <span class="string">&quot;.hello world&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>不包含边界</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 不包含边界,下面两种方式都是不包含最后边界的</span></span><br><span class="line"><span class="keyword">for</span>( i &lt;- <span class="type">Range</span>(<span class="number">1</span>,<span class="number">10</span>))&#123;</span><br><span class="line">  println(i + <span class="string">&quot;.hello&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> until <span class="number">10</span>)&#123;</span><br><span class="line">  print(i+<span class="string">&quot;&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>对集合遍历</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="keyword">for</span> (elem &lt;- arr) &#123;</span><br><span class="line">  println(elem)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>循环守卫</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 下面两种方式得到的效果一样，在scala中可以使用后一种方式简化遍历</span></span><br><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span>)&#123;</span><br><span class="line">  <span class="keyword">if</span>(i!=<span class="number">2</span>)&#123;</span><br><span class="line">    println(i)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">3</span> <span class="keyword">if</span> i != <span class="number">2</span>)&#123;</span><br><span class="line">  println(i)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>步长循环</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定每轮循环的步长 </span></span><br><span class="line"><span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">10</span> by <span class="number">2</span>) &#123;</span><br><span class="line">  println(i)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 递减遍历</span></span><br><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">10</span> to <span class="number">1</span> by <span class="number">-2</span>)&#123;</span><br><span class="line">  println(i)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>嵌套循环</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">10</span>; j &lt;- <span class="number">1</span> to <span class="number">8</span>)&#123;</span><br><span class="line">  println(<span class="string">&quot;i = &quot;</span> + i + <span class="string">&quot;, j = &quot;</span> + j)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 实现9*9乘法表</span></span><br><span class="line"><span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">9</span>; j &lt;- <span class="number">1</span> to i)&#123;</span><br><span class="line">  print(<span class="string">s&quot;<span class="subst">$&#123;i&#125;</span> * <span class="subst">$&#123;j&#125;</span> = <span class="subst">$&#123;i * j&#125;</span> \t&quot;</span>)</span><br><span class="line">  <span class="keyword">if</span>(i == j)&#123;</span><br><span class="line">    println()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>获取for循环的返回值</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> b = <span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">10</span>) <span class="keyword">yield</span> i</span><br><span class="line">println(<span class="string">&quot;b = &quot;</span> + b) <span class="comment">// b = Vector(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</span></span><br></pre></td></tr></table></figure>

<p> 使用yield关键字可以将for循环中的每次遍历结果保存并返回。</p>
</li>
</ol>
<p>使用break中断循环</p>
<p>在scala中是break不是关键字，而是一个方法，使用是需要引入scala.util.control.Breaks包中的break()方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> result = <span class="number">0</span></span><br><span class="line"><span class="keyword">import</span> scala.util.control.<span class="type">Breaks</span>._</span><br><span class="line">breakable&#123;</span><br><span class="line">  <span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">10</span>)&#123;</span><br><span class="line">    <span class="keyword">if</span>(i == <span class="number">5</span>)&#123;</span><br><span class="line">      <span class="keyword">break</span>()</span><br><span class="line">    &#125;</span><br><span class="line">    result += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">println(<span class="string">s&quot;result = <span class="subst">$&#123;result&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>





<h1 id="3-函数式编程"><a href="#3-函数式编程" class="headerlink" title="3 函数式编程"></a>3 函数式编程</h1><p>函数定义</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">函数名</span></span>([参数: 参数类型,...]): 函数返回类型=&#123;函数体&#125;</span><br></pre></td></tr></table></figure>





<p>函数与方法的区别</p>
<p>1） 概念</p>
<p>​	（1）完成某一功能的程序语言集合，称之为函数。</p>
<p>​	（2）类中的函数也成为方法。</p>
<p>2）</p>
<p>​	（1）Scala中可以在任何语法结构中声明任何方法</p>
<p>​	（2）函数没有重载和重写的概念；方法可以进行重载或重写</p>
<p>​	（3）Scala中函数可以嵌套定义</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test01_FunctionAndMethod</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 在main方法中定义一个函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sayHi</span></span>(name: <span class="type">String</span>) = &#123;</span><br><span class="line">      println(<span class="string">&quot;hi, &quot;</span> + name)</span><br><span class="line">    &#125;</span><br><span class="line">    sayHi(<span class="string">&quot;alice&quot;</span>) <span class="comment">// 调用最近定义的函数</span></span><br><span class="line">    <span class="keyword">this</span>.sayHi(<span class="string">&quot;job&quot;</span>) <span class="comment">// 使用当前对象的方法</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 对象中的方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sayHi</span></span>(name: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">&quot;Hi, &quot;</span> + name)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="3-1-函数定义"><a href="#3-1-函数定义" class="headerlink" title="3.1 函数定义"></a>3.1 函数定义</h2><p>scala函数定义尽可能简化。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f0</span></span>(name: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">      <span class="comment">// 可以省略return，最后一行作为返回值</span></span><br><span class="line">      <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;=========================================&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 函数中只有一行，可以省略掉大括号</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f1</span></span>(name: <span class="type">String</span>): <span class="type">String</span> = name</span><br><span class="line"></span><br><span class="line">    println(f1(<span class="string">&quot;Tom&quot;</span>))</span><br><span class="line">    println(<span class="string">&quot;=========================================&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回值的类型也可以省略, 返回的变量为传入的参数时，可以省略返回类型</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f2</span></span>(name: <span class="type">String</span>) = name</span><br><span class="line"></span><br><span class="line">    println(f2(<span class="string">&quot;Jack&quot;</span>));</span><br><span class="line">    println(<span class="string">&quot;=========================================&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果写了return, 那么必须给出返回类型, 不能省略</span></span><br><span class="line">    <span class="comment">//    def f3(name:String) = &#123;</span></span><br><span class="line">    <span class="comment">//      return name</span></span><br><span class="line">    <span class="comment">//    &#125;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f3</span></span>(name: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">      <span class="comment">// 如果写了return, 那么必须给出返回类型</span></span><br><span class="line">      <span class="keyword">return</span> name</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果指定返回类型是Unit, 此时即使函数最后有返回值也不起作用</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f4</span></span>(name:<span class="type">String</span>):<span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">return</span> name</span><br><span class="line">    &#125;</span><br><span class="line">    println(f4(<span class="string">&quot;zhnagsan&quot;</span>)) <span class="comment">// 调用函数返回的是空类型</span></span><br><span class="line">    println(<span class="string">&quot;=========================================&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果函数是没有返回值的，则可以省略等号</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f5</span></span>(name:<span class="type">String</span>)&#123;</span><br><span class="line">      println(name)</span><br><span class="line">    &#125;</span><br><span class="line">    println(f5(<span class="string">&quot;无返回值&quot;</span>))</span><br><span class="line">    println(<span class="string">&quot;=========================================&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 函数无参数，那么调用时可以不加括号</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f6</span></span>() = &#123;</span><br><span class="line">      println(<span class="string">&quot;函数无参数&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    f6()</span><br><span class="line">    f6</span><br><span class="line">    println(<span class="string">&quot;=========================================&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果无参函数定义时就省略了小括号，那么调用时也要省略</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f7</span></span>():<span class="type">Unit</span> = &#123;</span><br><span class="line">      println(<span class="string">&quot;函数无参数,且省略小括号&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//f7()  定义时没有小括号</span></span><br><span class="line">    f7</span><br><span class="line">    println(<span class="string">&quot;=========================================&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果只关注函数的处理逻辑， 那么函数名都可以省略了</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f8</span></span>(name:<span class="type">String</span>) = &#123;</span><br><span class="line">      println(name)</span><br><span class="line">    &#125;</span><br><span class="line">        <span class="comment">// 匿名函数(Lambda表达式)</span></span><br><span class="line">    (name:<span class="type">String</span>) =&gt; println(name)</span><br></pre></td></tr></table></figure>

<h2 id="3-2-函数使用"><a href="#3-2-函数使用" class="headerlink" title="3.2 函数使用"></a>3.2 函数使用</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将函数赋值给变量</span></span><br><span class="line"> <span class="keyword">val</span> fun  = (name:<span class="type">String</span>) =&gt; &#123;println(name)&#125;</span><br><span class="line"> <span class="comment">//    val fun: String =&gt; Unit = (name:String) =&gt; &#123;println(name)&#125;</span></span><br><span class="line"> <span class="comment">// String =&gt; Unit 表示传入String类型参数返回Unit类型的变量</span></span><br><span class="line"> fun(<span class="string">&quot;Jacks&quot;</span>)</span><br><span class="line"> println(<span class="string">&quot;======================================&quot;</span>)</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 定义一个函数作为参数</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">f</span></span>(func:<span class="type">String</span> =&gt;<span class="type">Unit</span>):<span class="type">Unit</span> = &#123;</span><br><span class="line">   func(<span class="string">&quot;函数作为参数&quot;</span>)</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="comment">// 函数中是逻辑处理过程， 我们将处理的过程作为参数传递，使其对应到数据上操作</span></span><br><span class="line"> f(fun)</span><br><span class="line"> <span class="comment">// 直接传递lambda表达式</span></span><br><span class="line"> f((name:<span class="type">String</span>) =&gt; &#123;println(name)&#125;)</span><br><span class="line"> println(<span class="string">&quot;======================================&quot;</span>)</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 匿名函数的至简单原则</span></span><br><span class="line"> <span class="comment">// 1. 参数类型省略， 自动推到参数类型</span></span><br><span class="line"> f((name) =&gt;&#123;</span><br><span class="line">   println(name)</span><br><span class="line"> &#125;)</span><br><span class="line"> <span class="comment">// 参数只有一个，可以省略小括号</span></span><br><span class="line"> f(name =&gt; &#123;println(name)&#125;)</span><br><span class="line"> <span class="comment">// 函数内容只有一行，还可以也省略</span></span><br><span class="line"> f(name =&gt; println(name))</span><br><span class="line"> <span class="comment">// 函数参数只出现一次，name参数在函数体中只有一部操作，那么还能再省掉参数名，只用下划线占位</span></span><br><span class="line"> f(println(_))</span><br><span class="line"> <span class="comment">// 如果能够推断出传入的是一个函数体，而不是调用语句，可以直接省略下划线</span></span><br><span class="line"> f(println)</span><br><span class="line"> println(<span class="string">&quot;=======================================&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> <span class="comment">// 定义一个二元操作的函数，它的参数为一个函数</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">dualFunctionOneAndTwo</span></span>(fun:(<span class="type">Int</span>, <span class="type">Int</span>)=&gt;<span class="type">Int</span>):<span class="type">Int</span> = &#123;</span><br><span class="line">   fun(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 定义函数体</span></span><br><span class="line"> <span class="keyword">val</span> add = (a:<span class="type">Int</span>,b:<span class="type">Int</span>)=&gt;&#123;a+b&#125;</span><br><span class="line"> <span class="keyword">val</span> minus = (a:<span class="type">Int</span>, b:<span class="type">Int</span>)=&gt;&#123;a-b&#125;</span><br><span class="line"> <span class="comment">// 传入函数名称，这里把函数的处理逻辑传递给方法，即将函数当作参数传递</span></span><br><span class="line"> println(dualFunctionOneAndTwo(add))</span><br><span class="line"> println(dualFunctionOneAndTwo(minus))</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 函数匿名化, 参数类型也可以省略</span></span><br><span class="line"> <span class="comment">// (a,b)=&gt; a+b 匿名的方法直接作为参数传递给函数</span></span><br><span class="line"> println(dualFunctionOneAndTwo((a,b)=&gt; a+b))</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 参数名称也可以省略了，直接使用_替代</span></span><br><span class="line"> println(dualFunctionOneAndTwo(_+_))</span><br></pre></td></tr></table></figure>





<h2 id="3-3-高阶函数"><a href="#3-3-高阶函数" class="headerlink" title="3.3 高阶函数"></a>3.3 高阶函数</h2><p>​	使用函数时，一般都是先定义后调用。</p>
<p>函数的其他高阶用法:</p>
<h3 id="1-函数作为值来传递"><a href="#1-函数作为值来传递" class="headerlink" title="1. 函数作为值来传递"></a>1. 函数作为值来传递</h3><p>注意区分函数作为值传递和函数调用, 函数作为值传递，所传递的是该函数的引用，相当于函数别名</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 有参函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span></span>(n: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  println(<span class="string">&quot;f调用&quot;</span>)</span><br><span class="line">  n + <span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 无参函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span></span>():<span class="type">Int</span> = &#123;</span><br><span class="line">  println(<span class="string">&quot;fun调用&quot;</span>)</span><br><span class="line">  <span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> result = f(<span class="number">123</span>)</span><br><span class="line">println(result)</span><br><span class="line"></span><br><span class="line"><span class="comment">//1. 函数作为值来传递</span></span><br><span class="line"><span class="keyword">val</span> f1 = f _ <span class="comment">// 接收函数，而不是接收函数调用后的结果值，不是 f1 = f(123)</span></span><br><span class="line"><span class="keyword">val</span> f2:<span class="type">Int</span> =&gt; <span class="type">Int</span> = f <span class="comment">// 如果指定的类型， 那么可以省略下划线 _</span></span><br><span class="line"></span><br><span class="line">println(f1) <span class="comment">// f1函数的引用</span></span><br><span class="line">println(f1(<span class="number">12</span>)) <span class="comment">// f1函数调用</span></span><br><span class="line">println(f2)</span><br><span class="line">println(f2(<span class="number">12</span>))</span><br></pre></td></tr></table></figure>





<h3 id="2-函数作为参数传递"><a href="#2-函数作为参数传递" class="headerlink" title="2. 函数作为参数传递"></a>2. 函数作为参数传递</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dualEval</span></span>(op: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span>, a:<span class="type">Int</span>, b:<span class="type">Int</span>):<span class="type">Int</span> = &#123;</span><br><span class="line">  op(a, b)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(a:<span class="type">Int</span>, b:<span class="type">Int</span>):<span class="type">Int</span> =&#123;</span><br><span class="line">  a + b</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">println(dualEval(add,<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 传递的是一个匿名函数, 它的参数可以自动推断出来</span></span><br><span class="line">println(dualEval((_+_), <span class="number">1</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure>



<h3 id="3-函数作为返回类型"><a href="#3-函数作为返回类型" class="headerlink" title="3. 函数作为返回类型"></a>3. 函数作为返回类型</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定返回类型为函数，且该函数的类型为 Int =&gt; Unit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f5</span></span>():<span class="type">Int</span> =&gt; <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">f6</span></span>(n:<span class="type">Int</span>):<span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">&quot;f6调用&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  f6</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 或 使用 函数名 空格 下划线的方式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f7</span></span>()= &#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">f8</span></span>(n:<span class="type">Int</span>):<span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">&quot;f6调用&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  f8 _</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="4-函数作为参数"><a href="#4-函数作为参数" class="headerlink" title="4. 函数作为参数:"></a>4. 函数作为参数:</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 对数组中元素做操作, 第二个参数op为定义好类型的函数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">arrayOperation</span></span>(array: <span class="type">Array</span>[<span class="type">Int</span>], op: <span class="type">Int</span> =&gt; <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">Int</span>] =&#123;</span><br><span class="line">    <span class="keyword">for</span>(elem &lt;- array) <span class="keyword">yield</span> op(elem)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addOne</span></span>(elem:<span class="type">Int</span>):<span class="type">Int</span> = &#123;</span><br><span class="line">    elem + <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">  <span class="keyword">val</span> newArray:<span class="type">Array</span>[<span class="type">Int</span>] = arrayOperation(arr, addOne)</span><br><span class="line">  newArray.foreach(println)</span><br><span class="line">  <span class="keyword">val</span> array:<span class="type">Array</span>[<span class="type">Int</span>] = arrayOperation(arr, (_ + <span class="number">2</span>))</span><br><span class="line">  array.foreach(println)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<p>练习：</p>
<ol>
<li><p>定义匿名函数，将它作为值传递给变量fun, 函数有三个参数分别为Int, String, Char,返回类型为Boolean. 要求调用函数fun(0,””,’0’) 返回false,其他情况均返回true.</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fun = (i:<span class="type">Int</span>, s:<span class="type">String</span>, c:<span class="type">Char</span>)=&gt;&#123;<span class="keyword">if</span>(i == <span class="number">0</span> &amp;&amp; s ==<span class="string">&quot;&quot;</span> &amp;&amp; c == &#x27;<span class="number">0</span>&#x27;)<span class="literal">false</span> <span class="keyword">else</span> <span class="literal">true</span>&#125;</span><br><span class="line">println(fun(<span class="number">0</span>,<span class="string">&quot;&quot;</span>,&#x27;<span class="number">0</span>&#x27;))</span><br></pre></td></tr></table></figure>
</li>
<li><p>定义一个函数func, 它接收一个Int类型的参数，返回一个函数(记作f1),它返回的函数f1,接收一个String类型的参数，同样返回一个函数(记作f2),函数f2接收一个Char类型，返回一个Boolean的值。要求调用函数func(0)(“”)(‘0’)得到的返回值为false,其他情况均返回true。</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span></span>(i: <span class="type">Int</span>): <span class="type">String</span> =&gt; <span class="type">Char</span> =&gt; <span class="type">Boolean</span>  = &#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">f1</span></span>(s:<span class="type">String</span>): <span class="type">Char</span> =&gt; <span class="type">Boolean</span> =&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f2</span></span>(c:<span class="type">Char</span>):<span class="type">Boolean</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> ( i == <span class="number">0</span> &amp;&amp; s == <span class="string">&quot;&quot;</span> &amp;&amp; c == &#x27;<span class="number">0</span>&#x27;) <span class="literal">false</span> <span class="keyword">else</span> <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">    f2</span><br><span class="line">  &#125;</span><br><span class="line">  f1</span><br><span class="line">&#125;</span><br><span class="line">println(func(<span class="number">0</span>)(<span class="string">&quot;&quot;</span>)(&#x27;<span class="number">0</span>&#x27;)) </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment">// 简写</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func1</span></span>(i:<span class="type">Int</span>): <span class="type">String</span>=&gt;<span class="type">Char</span>=&gt;<span class="type">Boolean</span> = &#123;</span><br><span class="line">  s =&gt; c =&gt; <span class="keyword">if</span> (i == <span class="number">0</span> &amp;&amp; s == <span class="string">&quot;&quot;</span> &amp;&amp; c == &#x27;<span class="number">0</span>&#x27;) <span class="literal">false</span> <span class="keyword">else</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="5-函数传名"><a href="#5-函数传名" class="headerlink" title="5. 函数传名"></a>5. 函数传名</h3><p>传递一个代码块，每次执行代码块中的内容。 </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 值传递</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f0</span></span>(a: <span class="type">Int</span>) = () =&gt; &#123;</span><br><span class="line">      println(<span class="string">&quot;a: &quot;</span> + a)</span><br><span class="line">      println(<span class="string">&quot;a: &quot;</span> + a)</span><br><span class="line">      <span class="number">10</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    f0(<span class="number">23</span>)</span><br><span class="line">    println(<span class="string">&quot;=====================&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f1</span> </span>= &#123;</span><br><span class="line">      println(<span class="string">&quot;f1调用&quot;</span>)</span><br><span class="line">      <span class="number">23</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 传名参数, 传递不再是具体的值，而是代码块</span></span><br><span class="line">    <span class="comment">// 这里的参数a不在是一个值，而是传递进去的函数代码块</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f2</span></span>(a: =&gt; <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      println(<span class="string">&quot;a: &quot;</span> + a)</span><br><span class="line">      println(<span class="string">&quot;a: &quot;</span> + a)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    f2(<span class="number">23</span>)</span><br><span class="line">    println(<span class="string">&quot;=================&quot;</span>)</span><br><span class="line">    f2(f1)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;=================&quot;</span>)</span><br><span class="line">    f2(&#123;</span><br><span class="line">      println(<span class="string">&quot;这是一个代码块&quot;</span>)</span><br><span class="line">      <span class="number">12</span></span><br><span class="line">    &#125;)</span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">=====================</span><br><span class="line">a: <span class="number">23</span></span><br><span class="line">a: <span class="number">23</span></span><br><span class="line">=================</span><br><span class="line">f1调用</span><br><span class="line">a: <span class="number">23</span></span><br><span class="line">f1调用</span><br><span class="line">a: <span class="number">23</span></span><br><span class="line">=================</span><br><span class="line">这是一个代码块</span><br><span class="line">a: <span class="number">12</span></span><br><span class="line">这是一个代码块</span><br><span class="line">a: <span class="number">12</span></span><br></pre></td></tr></table></figure>





<h3 id="6-懒加载"><a href="#6-懒加载" class="headerlink" title="6. 懒加载"></a>6. 懒加载</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">lazy</span> <span class="keyword">val</span> result = sum(<span class="number">10</span>,<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;1. 函数调用&quot;</span>)</span><br><span class="line">    println(<span class="string">&quot;2. result = &quot;</span> + result)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sum</span> </span>(a:<span class="type">Int</span>, b:<span class="type">Int</span>): <span class="type">Int</span> =&#123;</span><br><span class="line">      println(<span class="string">&quot;3. sum执行&quot;</span>)</span><br><span class="line">      a+b</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line"><span class="number">1.</span> 函数调用</span><br><span class="line"><span class="number">3.</span> sum执行</span><br><span class="line"><span class="number">2.</span> result = <span class="number">22</span></span><br></pre></td></tr></table></figure>

<p>​		使用lazy声明的变量会在第一调用的时候才执行，样例中result只是先声明了，但并没有实际初始化，只有当后面引用了该变量时，才计算到它的值。</p>
<h2 id="3-4-函数柯里化-闭包"><a href="#3-4-函数柯里化-闭包" class="headerlink" title="3.4 函数柯里化&amp;闭包"></a>3.4 函数柯里化&amp;闭包</h2><p><strong>闭包</strong>: 如果一个函数访问到了它外部变量值，那么这个函数和它所处的环境成为闭包。</p>
<p><strong>函数柯里化</strong>: 把一个参数列表的多个参数，变成多个参数列表。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test09_ClosureAndCurrying</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> num = <span class="number">0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addNum</span></span>(a: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      a + num</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将上面函数放到一个函数中</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addBy</span></span>(): <span class="type">Int</span> =&gt; <span class="type">Int</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> a = <span class="number">0</span></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(b: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">        a + b</span><br><span class="line">      &#125;</span><br><span class="line">      add</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">makeIncreaser2</span></span>(more: <span class="type">Int</span>) = (x:<span class="type">Int</span>)=&gt;x + more</span><br><span class="line">    <span class="comment">// 或简化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">makeIncreaser</span></span>(more: <span class="type">Int</span>): <span class="type">Int</span> =&gt; <span class="type">Int</span> = _ + more</span><br><span class="line"></span><br><span class="line">    println(makeIncreaser(<span class="number">1</span>)(<span class="number">2</span>))</span><br><span class="line">    println(makeIncreaser(<span class="number">2</span>)(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 柯里化(curry)</span></span><br><span class="line">    <span class="comment">// 将上述函数以柯里化的方式编写</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addCurrying</span></span>(a: <span class="type">Int</span>)(b: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      a + b</span><br><span class="line">    &#125;</span><br><span class="line">    println(addCurrying(<span class="number">5</span>)(<span class="number">10</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>







<h1 id="4-类与对象"><a href="#4-类与对象" class="headerlink" title="4. 类与对象"></a>4. 类与对象</h1><p>类和无</p>
<p>参构造器</p>
<p>Scala类如果没有指定构造器，会默认有一个无参的构造器。</p>
<p>类字段必须进行初始化，Scala编译器会根据初始化值的类型自动计算字段变量的类型，同时字段的类型可以省略</p>
<p>类字段的定义声明方式:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">    <span class="comment">// 声明变量时初始化， 编译器根据具体的字面值判断字段的类型</span></span><br><span class="line">    <span class="keyword">var</span> name=<span class="string">&quot;xiaoming0&quot;</span></span><br><span class="line">    <span class="comment">// 指定的数据类型， 使用_占位符, 编译器会根据指定的类型选择初始化值</span></span><br><span class="line">    <span class="keyword">var</span> age:<span class="type">Int</span> = _</span><br><span class="line">    <span class="comment">// val修饰的变量不能使用占位符</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ClassDemo</span></span>&#123;</span><br><span class="line">    <span class="keyword">val</span> person= <span class="keyword">new</span> <span class="type">Person</span>()</span><br><span class="line">    println(<span class="string">s&quot;<span class="subst">$&#123;person.name&#125;</span> <span class="subst">$&#123;person.age&#125;</span>&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>如果没有定义构造器，Scala类中会有一个默认的无参构造器；</p>
<h1 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>1）Scala的集合有三大类: 序列Seq, 集 Set, 映射 Map, 所有的集合都扩展自Iterable特质。</p>
<p>2）对于几乎所有的集合类，Scala都提供了可变和不可变的版本，分别位于以下两个包:</p>
<p>​		不可变集合：scala.collection.immutable</p>
<p>​		可变集合：scala.collection.mutable</p>
<p>3）Scala不可变集合是指该集合对象不可修改，每次修改就会返回一个新对象，而不会对原对象进行修改。</p>
<p>4）可变集合，就是这个集合可以直接对原对象进行修改，而不会返回新的对象。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/scala/Scala%E7%AC%94%E8%AE%B0/" data-id="clumsd6xa000pabr75kcbfgpt" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-redis/Redis入门" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/redis/Redis%E5%85%A5%E9%97%A8/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.912Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Redis持久化"><a href="#Redis持久化" class="headerlink" title="Redis持久化"></a>Redis持久化</h1><p>Redis是一个内存的数据存储，当服务器宕机后，数据会立即消失，因此为了在Redis重启后能快速恢复数据，必须将内存中的数据持久化到磁盘上。</p>
<p>参考链接: <a target="_blank" rel="noopener" href="https://baijiahao.baidu.com/s?id=1654694618189745916&wfr=spider&for=pc">https://baijiahao.baidu.com/s?id=1654694618189745916&amp;wfr=spider&amp;for=pc</a></p>
<h2 id="RDB机制"><a href="#RDB机制" class="headerlink" title="RDB机制"></a>RDB机制</h2><p>RDB其实就是把数据以快照的形式保存在磁盘上。什么是快照呢，你可以理解成把当前时刻的数据拍成一张照片保存下来。</p>
<p>RDB持久化是指在指定的时间间隔内将内存中的数据集快照写入磁盘。也是默认的持久化方式，这种方式是就是将内存中数据以快照的方式写入到二进制文件中,默认的文件名为dump.rdb。</p>
<p>RDB机制是通过把某个时刻的所有数据生成一个快照来保存，那么就应该有一种触发机制，是实现这个过程。对于RDB来说，提供了三种机制：save、bgsave、自动化。</p>
<p>快照持久化参数的配置redis.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定60s内，至少执行了1000条命令，则执行一次快照存储</span></span><br><span class="line">save 60 1000</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建快照失败后是否继续执行写命令</span></span><br><span class="line">stop-writes-on-bgsave-error no</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">是否对快照压缩</span></span><br><span class="line">rdbcompression yes</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">存储文件名称</span></span><br><span class="line">dbfilename dump.rdb</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">存储文件保存的位置， 是RDB和AOF共享的配置项</span></span><br><span class="line">dir ./</span><br></pre></td></tr></table></figure>



<h3 id="持久化触发方式"><a href="#持久化触发方式" class="headerlink" title="持久化触发方式"></a>持久化触发方式</h3><ol>
<li>save触发方式</li>
</ol>
<p>该命令会阻塞当前Redis服务器，执行save命令期间，Redis不能处理其他命令，直到RDB过程完成为止。</p>
<ol start="2">
<li>bgsave触发方式</li>
</ol>
<p>该命令执行时，Redis会在后台异步进行快照操作，快照同时还可以响应客户端请求。</p>
<p><img src="/Redis.assets/image-20210221132348232.png" alt="image-20210221132348232"></p>
<p>具体操作是Redis进程执行fork操作创建子进程，RDB持久化过程由子进程负责，完成后自动结束。阻塞只发生在fork阶段，一般时间很短。基本上 Redis 内部所有的RDB操作都是采用 bgsave 命令。</p>
<h3 id="RDB机制的优势："><a href="#RDB机制的优势：" class="headerlink" title="RDB机制的优势："></a>RDB机制的优势：</h3><p>1). 一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。</p>
<p>2). 对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。</p>
<p>3). 性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。</p>
<p>4). 相比于AOF机制，如果数据集很大，RDB的启动效率会更高。</p>
<p>RDB又存在哪些劣势呢：</p>
<p>1). 如果你想保证数据的高可用性，即最大限度的避免数据丢失，那么RDB将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。</p>
<p>2). 由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟。</p>
<p>摘录《Redis实战》</p>
<p><img src="/Redis.assets/image-20210221170404652.png" alt="image-20210221170404652"></p>
<h2 id="AOF机制"><a href="#AOF机制" class="headerlink" title="AOF机制"></a>AOF机制</h2><p>AOF（Append only file）是Redis的另一种持久话方式， 采用日志记录的方式，将更新操作命令记录到AOF文件中。</p>
<p>这样Redis重启时只需要按照顺序重新执行这些命令就可以恢复到原始状态。</p>
<p>也就是说 AOF记录更新过程， RDB只管结果。</p>
<h3 id="AOF持久化过程"><a href="#AOF持久化过程" class="headerlink" title="AOF持久化过程"></a>AOF持久化过程</h3><p>配置redis.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">开启aof</span></span><br><span class="line">appendonly yes</span><br><span class="line"></span><br><span class="line">dir ./</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">The name of the append only file (default: <span class="string">&quot;appendonly.aof&quot;</span>)</span></span><br><span class="line">appendfilename &quot;appendonly.aof&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>AOF文件中存储的是redis的命令，同步命令到AOF文件的整个过程可分为三个阶段:</p>
<p>1）命令传播： Redis将执行完的命令，参数，参数的个数等信息发送大AOF程序中：</p>
<p>当客户端需要执行命令时，它通过网络连接，将协议文本发送给Redis服务器，服务器在接到客户端的请求后，会根据协议文本的内容，选择适当的命令内容，并将各个参数从字符串文本转换为Redis字符串对象(StringObject)。每当命令函数成功执行之后，命令参数都会被传播到AOF程序。</p>
<p>2）缓存追加： AOF程序根据接收到的命令数据，将命令转换为网络通讯协议的格式，然后将协议内容追加到服务器的AOF缓存中：</p>
<p>AOF程序接收到命令后，程序会根据命令以及命令的参数，将命令从字符串对象转换回原来的协议文本，协议文本生成之后，它会被追加到redis.h&#x2F;redisServer结构的aof_buf末尾。</p>
<p>3）文件写入和保存：AOF缓存中的内容被写入到AOF文件末尾，如果设定的AOF保存条件被满足的话，fsync函数或者fdatasync函数会被调用，将写入的内容正真正地保存到磁盘中。</p>
<h3 id="保存-同步-模式"><a href="#保存-同步-模式" class="headerlink" title="保存(同步)模式"></a>保存(同步)模式</h3><p>AOF_FSYNC_NO 不保存</p>
<p>​	这种模式下，每次调用flushAppendOnlyFile函数，WRITE都会被执行，但SAVE会被略过。</p>
<p>SAVE命令只有在一下情况才会执行：</p>
<p>Redis被关闭</p>
<p>AOF功能被关闭</p>
<p>系统的写缓存被刷新(可能是缓存已经被写满了，或者定期保存操作被执行)</p>
<p>以上三种情况发生时，SAVE操作都会导致Redis主进程阻塞。</p>
<p>AOF_FSYNC_EVERYSEC 每秒保存一次（默认）</p>
<p>​	这种模式下，SAVE原则上是每隔一秒执行一次，由后台子线程（fork）调用，所以不会印期服务器主进程阻塞。</p>
<p>AOF_FSYNC_ALWAYS 每执行一个米革命保存一次 （不推荐）</p>
<p>​	SAVE命令由主进程执行， 在SAVE执行期间，主进程会被阻塞，不能接收命令请求。</p>
<h3 id="AOF持久化存在的问题"><a href="#AOF持久化存在的问题" class="headerlink" title="AOF持久化存在的问题"></a>AOF持久化存在的问题</h3><p>Redis不断的将被执行的命令记录到AOF文件中，导致文件的体积不断变大，同时Redis重启时，如果AOF文件过大，记录的写操作太多，那么还原操作占用的时间就可能会非常长。</p>
<p>客户端可以向Redis发送BGREWRITEAOF命令，这个命令通过移除AOF文件中的冗余命令来重写AOF文件，是AOF文件体积尽可能小。</p>
<p>重写AOF文件时会创建一个子进程，然后由子进程负责对AOF文件进行重写。</p>
<h3 id="AOF重写"><a href="#AOF重写" class="headerlink" title="AOF重写"></a>AOF重写</h3><p>AOF的重写不需要对原有的AOF文件进行任何写入和读取，它针对的是数据库中键的当前值。</p>
<p>Redis主进程会创建子进程进行重写，这样主进程可以继续处理客户端的命令。</p>
<ul>
<li><p>为什么是新创建子进程而不是创建线程？</p>
<p>  子进程带有主进程的数据副本，可以在避免锁的情况下，保证数据的安全性。使用线程就需要保证重写时，被重写的数据不能同时在被需改，即需要加锁控制。</p>
</li>
<li><p>在重写期间，主进程继续处理的命令如何保证能被重写进程访问到，并写入新AOF文件中？</p>
<p>  Redis增加了AOF重写缓存，在fork出子进程之后就会开始启用一个重写缓存，Redis主进程在接到新的写命令之后，除了会将写命令内容追加到现有的AOF文件之外，还会追加到这个重写缓存中， 当重写子进程完成之后，会通知主进程， 主进程收到完成信号后再将重写缓存中的内容也写到临时AOF文件中，这样新的AOF文件就和现有的AOF文件一致了，最后用新的AOF文件替换现有的就文件即完成了整个重写过程。</p>
</li>
</ul>
<p><img src="/Redis.assets/image-20210221235500551.png" alt="image-20210221235500551"></p>
<h3 id="重写的触发方式"><a href="#重写的触发方式" class="headerlink" title="重写的触发方式"></a>重写的触发方式</h3><ol>
<li>触发配置</li>
</ol>
<p>在redis.conf中配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">表示当前aof文件超过上一次aof文件的百分之夺嫂的时候会进行重写，如果之前没有重写过，则以启动时aof文件大小为准</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">百分比设置为0关闭重写功能</span></span><br><span class="line">auto-aof-rewrite-percentage 100</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">限制允许重写最小aof文件大小，也就是文件大小小于64mb的时候，不需要进行优化</span></span><br><span class="line">auto-aof-rewrite-min-size 64mb</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>执行bgrewriteaof命令强制执行重写</li>
</ol>
<blockquote>
<p>127.0.0.1:6379&gt; bgrewriteaof<br>Background append only file rewriting started</p>
</blockquote>
<h3 id="混合持久化"><a href="#混合持久化" class="headerlink" title="混合持久化"></a>混合持久化</h3><p>RDB和AOF各有优缺点，Redis4.0开始支持两种方式混合持久化，如果混合持久化开启后，aof rewrite的时候就直接把rdb的内容写道aof文件开头。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aof-use-rdb-preamble yes</span><br></pre></td></tr></table></figure>

<p>这种方式会在持久化的文件中记录RDB格式的文件头和AOF格式的命令。</p>
<h1 id="发布订阅"><a href="#发布订阅" class="headerlink" title="发布订阅"></a>发布订阅</h1><p>Redis的发布订阅功能，用于消息的传输。类似将redis作为一个消息队列使用。</p>
<p>包括消息的发布publisher， 订阅subscriber，通道channel</p>
<p>发布者和订阅者都是Redis客户端， 通道Channel则为服务器端。</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>SUBSCRIBE</td>
<td>SUBSCRIBE channel [channel …]  — 订阅给定的一个或多个频道</td>
</tr>
<tr>
<td>UNSUBSCRIBE</td>
<td>UNSUBSCRIBE [channel [channel …]] — 退订给定的一个或多个频道,  如果没有指定频道，则退订所有的频道</td>
</tr>
<tr>
<td>PUBLISH</td>
<td>PUBLISH channel message  — 给指定频道发送消息</td>
</tr>
<tr>
<td>PSUBSCRIBE</td>
<td>PSUBSCRIBE pattern [pattern …] —订阅与给定模式匹配的所有频道</td>
</tr>
<tr>
<td>PUNSUBSCRIBE</td>
<td>PUNSUBSCRIBE [pattern [pattern ….]]  – 退订给定的模式，如果没给定任何模式，那么退订所有模式</td>
</tr>
</tbody></table>
<p><img src="/Redis.assets/image-20210222235734554.png" alt="image-20210222235734554"></p>
<p><img src="/Redis.assets/image-20210222235655866.png" alt="image-20210222235655866"></p>
<p>这里需要先发布消息，才能由订阅者接收到，并且订阅者只能收到发送者此刻推送的消息。</p>
<h1 id="Redis事务"><a href="#Redis事务" class="headerlink" title="Redis事务"></a>Redis事务</h1><p>通过multi, exec, discard, watch 这四个命令来完成；</p>
<p>单个命令都是原子性的，需要确保事务性的对象是命令集合；</p>
<p>Redis将命令集合序列化并确保处于同一事务的的命令集合连续且不被打断的执行；</p>
<p>Redis不支持回滚操作。</p>
<p>multi 表示事务标志开启，之后执行的命令都不会被立刻执行，而是放到一个命令队列的集合里， 当收到exec命令时才会把命令集合中的命令提交。</p>
<p>multi 开启事务</p>
<p>exec 执行命令</p>
<p>discard 清除队列</p>
<p>watch  监控某个key</p>
<blockquote>
<p>127.0.0.1:6379&gt; get name:1<br>“111”<br>127.0.0.1:6379&gt; get name:2<br>“222”<br>127.0.0.1:6379&gt; watch name:1  # 在multi命令之前<br>OK<br>127.0.0.1:6379&gt; multi<br>OK<br>127.0.0.1:6379&gt; set name:2 two<br>QUEUED<br>127.0.0.1:6379&gt; exec  # 在exec命令执行之前，如果其他客户端修改了name:1的内容，那么命令队列中的内容都会清除</p>
<p>(nil)   # exec执行的结果失败， 并没有执行在事务里的操作 set name:2 two</p>
</blockquote>
<p><em>用户使用WATCH命令对键key进行监视之后，直到用户执行EXEC命令的这段时间，如果有其他客户端抢先对任何被监视的键进行替换，更新或删除等操作，那么当用户执行EXEC命令的时候，事务将失败并返回一个错误（之后用户可以选择重试事务或放弃事务）。</em></p>
<p>UNWATCH命令可以在WATCH命令之后，MULTI命令之前对连接重置；</p>
<p>DISCARD命令则可以在MULTI命令之后，EXEC命令之前执行，用来清除命令队列中的命令，也会将WATCH的作用取消。</p>
<h1 id="Redis源码编译及安装"><a href="#Redis源码编译及安装" class="headerlink" title="Redis源码编译及安装"></a>Redis源码编译及安装</h1><ol>
<li><p>准备编译依赖的环境</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y gcc-c++</span><br><span class="line">yum install -y wget</span><br></pre></td></tr></table></figure>

</li>
<li><p>下载redis-5.0.5.tar.gz，解压</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://download.redis.io/releases/redis-5.0.5.tar.gz</span><br></pre></td></tr></table></figure>


</li>
<li><p>解压文件，编译源码</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd redis-5.0.5/src</span><br><span class="line">make</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装Redis</p>
<p> 通过PREFIX指定安装路径</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建要安装的路径</span></span><br><span class="line">mkdir /opt/redis -p</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">安装redis</span></span><br><span class="line">make install PREFIX=/opt/redis/</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动redis-server</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入到/opt/redis/bin</span></span><br><span class="line">./redis-server</span><br></pre></td></tr></table></figure>

<p> <img src="/Redis.assets/image-20211108105441577.png" alt="image-20211108105441577"></p>
<p> 这种方式启动的服务端不能关闭窗口,直接使用ctrl+c停止服务。</p>
<p> <strong>后台启动服务端</strong></p>
<ol>
<li>拷贝配置文件redis.conf到redis的安装目录下</li>
</ol>
<p> <img src="/Redis.assets/image-20211108105740717.png" alt="image-20211108105740717"></p>
<ol start="2">
<li><p>修改redis.conf文件</p>
<p> vim redis.conf</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">JUST COMMENT THE FOLLOWING LINE.</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">注释绑定ip的配置，允许所有的外部连接</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="built_in">bind</span> 127.0.0.1</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Protected mode is a layer of security protection, <span class="keyword">in</span> order to avoid that</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Redis instances left open on the internet are accessed and exploited.</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash"><span class="comment"># When protected mode is on and if:</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash"><span class="comment"># 1) The server is not binding explicitly to a set of addresses using the</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">   <span class="string">&quot;bind&quot;</span> directive.</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2) No password is configured.</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash"><span class="comment"># The server only accepts connections from clients connecting from the</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">sockets.</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash"><span class="comment"># By default protected mode is enabled. You should disable it only if</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">you are sure you want clients from other hosts to connect to Redis</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">even <span class="keyword">if</span> no authentication is configured, nor a specific <span class="built_in">set</span> of interfaces</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">are explicitly listed using the <span class="string">&quot;bind&quot;</span> directive.</span></span><br><span class="line">protected-mode no</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">By default Redis does not run as a daemon. Use <span class="string">&#x27;yes&#x27;</span> <span class="keyword">if</span> you need it.</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Note that Redis will write a pid file <span class="keyword">in</span> /var/run/redis.pid when daemonized.</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">作为后台程序运行</span></span><br><span class="line">daemonize yes</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>启动服务</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./redis-server redis.conf </span><br></pre></td></tr></table></figure>
</li>
<li><p>客户端连接</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">客户端连接</span></span><br><span class="line">./redis-cli -h 127.0.0.1 -p 6379</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<p>​	<strong>关闭后台运行</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./redis-cli shutdown</span><br></pre></td></tr></table></figure>



<h1 id="Redis-集群搭建"><a href="#Redis-集群搭建" class="headerlink" title="Redis 集群搭建"></a>Redis 集群搭建</h1><p>集群准备</p>
<p>创建每个redis实例目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir /var/redis-cluster/7001</span><br><span class="line">mkdir /var/redis-cluster/7002</span><br><span class="line">mkdir /var/redis-cluster/7003</span><br><span class="line">mkdir /var/redis-cluster/7004</span><br><span class="line">mkdir /var/redis-cluster/7005</span><br><span class="line">mkdir /var/redis-cluster/7006</span><br></pre></td></tr></table></figure>

<p>由源码编译，输出到&#x2F;var&#x2F;redis-cluster&#x2F;7001目录下</p>
<p>make install PREFIX&#x3D;&#x2F;var&#x2F;redis-cluster&#x2F;7001</p>
<p>拷贝redis.conf文件到编译好的目标目录下</p>
<p>配置redis.conf</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一般配置</span></span><br><span class="line"><span class="comment"># 注释掉ip绑定</span></span><br><span class="line"><span class="comment">#bind 127.0.0.1</span></span><br><span class="line"><span class="attr">port</span> <span class="string">7001</span></span><br><span class="line"><span class="attr">daemonize</span> <span class="string">yes</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># redis集群配置</span></span><br><span class="line"><span class="attr">cluster-enabled</span> <span class="string">yes</span></span><br></pre></td></tr></table></figure>

<p>将7001目录下的redis配置拷贝到其他的实例目录下,并分别配置每个目录下redis.conf文件中的port端口号</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cp -r 7001/* 7002/</span><br><span class="line">cp -r 7001/* 7003/</span><br><span class="line">cp -r 7001/* 7004/</span><br><span class="line">cp -r 7001/* 7005/</span><br><span class="line">cp -r 7001/* 7006/</span><br></pre></td></tr></table></figure>

<p>切换到实例目录的上一级</p>
<p>编辑启动脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">cd 7001/bin</span><br><span class="line">./redis-server redis.conf</span><br><span class="line">cd -</span><br><span class="line"></span><br><span class="line">cd 7002/bin</span><br><span class="line">./redis-server redis.conf</span><br><span class="line">cd -</span><br><span class="line"></span><br><span class="line">cd 7003/bin</span><br><span class="line">./redis-server redis.conf</span><br><span class="line">cd -</span><br><span class="line"></span><br><span class="line">cd 7004/bin</span><br><span class="line">./redis-server redis.conf</span><br><span class="line">cd -</span><br><span class="line"></span><br><span class="line">cd 7005/bin</span><br><span class="line">./redis-server redis.conf</span><br><span class="line">cd -</span><br><span class="line"></span><br><span class="line">cd 7006/bin</span><br><span class="line">./redis-server redis.conf</span><br><span class="line">cd -</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>目录下文件如图</p>
<p><img src="/Redis.assets/image-20210224235542334.png" alt="image-20210224235542334"></p>
<p>启动redis实例</p>
<p><img src="/Redis.assets/image-20210224235946648.png" alt="image-20210224235946648"></p>
<p>查看进程</p>
<p><img src="/Redis.assets/image-20210225000029402.png" alt="image-20210225000029402"></p>
<p>可以看到每个实例都以cluster模式启动起来了。</p>
<p>创建集群</p>
<p>进入到一个实例的bin目录下，执行命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./redis-cli --cluster create 192.168.80.100:7001 192.168.80.100:7002 192.168.80.100:7003 192.168.80.100:7004 192.168.80.100:7005 192.168.80.100:7006 --cluster-replicas 1</span><br></pre></td></tr></table></figure>

<p>指定主节点所拥有的从节点个数为1， 这样就创建出了3主3从的集群。</p>
<p>上述命令执行后显示如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost bin]# ./redis-cli --cluster create 192.168.80.100:7001 192.168.80.100:7002 192.168.80.100:7003 192.168.80.100:7004 192.168.80.100:7005 192.168.80.100:7006 --cluster-replicas 1</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; Performing <span class="built_in">hash</span> slots allocation on 6 nodes...</span></span><br><span class="line">Master[0] -&gt; Slots 0 - 5460</span><br><span class="line">Master[1] -&gt; Slots 5461 - 10922</span><br><span class="line">Master[2] -&gt; Slots 10923 - 16383</span><br><span class="line">Adding replica 192.168.80.100:7005 to 192.168.80.100:7001</span><br><span class="line">Adding replica 192.168.80.100:7006 to 192.168.80.100:7002</span><br><span class="line">Adding replica 192.168.80.100:7004 to 192.168.80.100:7003</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; Trying to optimize slaves allocation <span class="keyword">for</span> anti-affinity</span></span><br><span class="line">[WARNING] Some slaves are in the same host as their master</span><br><span class="line">M: 876a8d0800cc746b819afe50141aa0e13ad2dbab 192.168.80.100:7001</span><br><span class="line">   slots:[0-5460] (5461 slots) master</span><br><span class="line">M: d582dd67ab9e2b09585b4a06f57d46ba32c1fcd2 192.168.80.100:7002</span><br><span class="line">   slots:[5461-10922] (5462 slots) master</span><br><span class="line">M: 1d4f27b78dbdbe3edcad80f2952a6fdafb2659c7 192.168.80.100:7003</span><br><span class="line">   slots:[10923-16383] (5461 slots) master</span><br><span class="line">S: e5e9e15d9c7ca749e445ba88a0a0c5a17132b74c 192.168.80.100:7004</span><br><span class="line">   replicates d582dd67ab9e2b09585b4a06f57d46ba32c1fcd2</span><br><span class="line">S: 7175a28aad5667349d28cd4d49696d941dfec28e 192.168.80.100:7005</span><br><span class="line">   replicates 1d4f27b78dbdbe3edcad80f2952a6fdafb2659c7</span><br><span class="line">S: d004be81457595e12748bb4fa9096422a4df12d6 192.168.80.100:7006</span><br><span class="line">   replicates 876a8d0800cc746b819afe50141aa0e13ad2dbab</span><br><span class="line">Can I set the above configuration? (type &#x27;yes&#x27; to accept): yes  # 输入yes允许自动配置主从节点</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; Nodes configuration updated</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; Assign a different config epoch to each node</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; Sending CLUSTER MEET messages to <span class="built_in">join</span> the cluster</span></span><br><span class="line">Waiting for the cluster to join</span><br><span class="line">..</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; Performing Cluster Check (using node 192.168.80.100:7001)</span></span><br><span class="line">M: 876a8d0800cc746b819afe50141aa0e13ad2dbab 192.168.80.100:7001</span><br><span class="line">   slots:[0-5460] (5461 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">S: 7175a28aad5667349d28cd4d49696d941dfec28e 192.168.80.100:7005</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates 1d4f27b78dbdbe3edcad80f2952a6fdafb2659c7</span><br><span class="line">M: 1d4f27b78dbdbe3edcad80f2952a6fdafb2659c7 192.168.80.100:7003</span><br><span class="line">   slots:[10923-16383] (5461 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">S: e5e9e15d9c7ca749e445ba88a0a0c5a17132b74c 192.168.80.100:7004</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates d582dd67ab9e2b09585b4a06f57d46ba32c1fcd2</span><br><span class="line">M: d582dd67ab9e2b09585b4a06f57d46ba32c1fcd2 192.168.80.100:7002</span><br><span class="line">   slots:[5461-10922] (5462 slots) master</span><br><span class="line">   1 additional replica(s)</span><br><span class="line">S: d004be81457595e12748bb4fa9096422a4df12d6 192.168.80.100:7006</span><br><span class="line">   slots: (0 slots) slave</span><br><span class="line">   replicates 876a8d0800cc746b819afe50141aa0e13ad2dbab</span><br><span class="line">[OK] All nodes agree about slots configuration.</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; Check <span class="keyword">for</span> open slots...</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; Check slots coverage...</span></span><br><span class="line">[OK] All 16384 slots covered.</span><br></pre></td></tr></table></figure>



<p>添加一个主节点后一个从节点</p>
<p>添加端口为7007的主节点和端口为7008的从节点</p>
<ol>
<li><p>添加主节点</p>
<p> 将7007端口对应的redis实例添加到7001所连接的集群上</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">redis-cli --cluster add-node [新添加的节点ip:port] [已连接在集群中的任意正常的Maste节点ip:port]</span></span><br><span class="line">redis-cli --cluster add-node 127.0.0.1:7007 127.0.0.1:7001</span><br></pre></td></tr></table></figure>

<p>​		添加指定节点连接到集群中的任意的一台节点上。</p>
<p>​		然后给新添加的Master节点分配slot</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">./redis-cli --cluster reshard 127.0.0.1:7007</span><br></pre></td></tr></table></figure>



<ol start="2">
<li><p>添加从节点</p>
<p> 添加7008端口对应的节点</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">redis-cli --cluster add-node [新添加的节点ip:port] [已连接在集群中的任意正常的节点ip:port] --cluster-slave --cluster-master-id [ID]</span> </span><br><span class="line">redis-cli --cluster add-node 127.0.0.1:7008 127.0.0.1:7001 --cluster-slave --cluster-master-id 3a0f87eecfb810fe79c4715ecc701431d6f4b9a1</span><br></pre></td></tr></table></figure>

<p>添加参数**–cluster-slave** , 表示添加的节点为一个从节点</p>
<p><strong>–cluster-master-id [ID]</strong> , 指定所添加的从节点对应的主节点ID, master的ID可以通过。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/redis/Redis%E5%85%A5%E9%97%A8/" data-id="clumsd6xa000mabr73edxesit" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">April 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/04/05/%E6%95%B0%E4%BB%93%E7%BB%8F%E9%AA%8C/%E9%9D%A2%E8%AF%95%E9%A2%98/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/04/05/zookeeper/Zookeeper/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/04/05/zookeeper/%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA(HA)/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/04/05/spark/spark%E7%BC%96%E7%A8%8B-RDD%E9%AB%98%E9%98%B6/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/04/05/spark/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>