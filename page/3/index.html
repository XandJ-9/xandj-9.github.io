<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/3/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-hbase/Hbase" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/hbase/Hbase/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.882Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h1><p><img src="/Hbase.assets/Hbase.png" alt="Hbase"></p>
<h1 id="HBase特点及应用"><a href="#HBase特点及应用" class="headerlink" title="HBase特点及应用"></a>HBase特点及应用</h1><img src="Hbase.assets/image-20210128005539626.png"/>

<p>Hbase数据是存储在Hdfs集群上的， 所有它能够存储海量数据。列式存储的设计能够减少空子段存储占用的空间。</p>
<p>HBase适合海量明细数据的存储，并且后期需要有很好的查询性能（单表超千万，上亿，且并发要求高）</p>
<h1 id="HBase整体架构"><a href="#HBase整体架构" class="headerlink" title="HBase整体架构"></a>HBase整体架构</h1><img src="Hbase.assets/image-20210128005310002.png"/>



<p>主要角色:</p>
<img src="Hbase.assets/image-20210128005950279.png"/>



<h1 id="数据读-写"><a href="#数据读-写" class="headerlink" title="数据读&#x2F;写"></a>数据读&#x2F;写</h1><img src="Hbase.assets/image-20210128010253601.png"/>



<p><img src="/Hbase.assets/image-20210128010616454.png" alt="image-20210128010616454"></p>
<p>注意MemStore部分的操作过程， 它存储了部分数据内容。</p>
<img src="Hbase.assets/image-20210129000732592.png"/>





<img src="Hbase.assets/image-20210129000918911.png"/>

<p>写数据时，是分别写数据到WAL和MemStore中， 当MemStore中的数据量达到设定的阈值时，会刷新到磁盘文件中，生成StoreFile文件。</p>
<p>Region合并</p>
<p><img src="/Hbase.assets/image-20210202023033453.png" alt="image-20210202023033453"></p>
<h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><p>协处理器</p>
<p><img src="/Hbase.assets/image-20210202022039428.png" alt="image-20210202022039428"></p>
<p>协处理器类似概念</p>
<p><img src="/Hbase.assets/image-20210202022114320.png" alt="image-20210202022114320"></p>
<p>两种类型的协处理器</p>
<p><img src="/Hbase.assets/image-20210202022224652.png" alt="image-20210202022224652"></p>
<p>协处理器的使用</p>
<ol>
<li>编写协处理器类</li>
<li>上传jar包到hdfs上</li>
<li>对&#x3D;修改hbase表的属性， 添加协处理器的路径及主类等信息</li>
</ol>
<p>hdfs dfs -put processor.jar  hdfs:&#x2F;&#x2F;linux111:9000&#x2F;processor</p>
<p>alter ‘tb_friends’,METHOD&#x3D;&gt;’table_att’,’Coprocessor’&#x3D;&gt;’hdfs:&#x2F;&#x2F;linux111:9000&#x2F;processor&#x2F;processor.jar|org.example.coprocessor.ObserverDemo|1001|’</p>
<p>卸载协处理器</p>
<p>disable ‘tb_friends’</p>
<p>alter ‘tb_friends’, METHOD&#x3D;&gt;’table_att_unset’,NAME&#x3D;&gt;’coprocessor$1’</p>
<p>enable ‘tb_friends’</p>
<p><strong>安装配置</strong></p>
<p>添加hadoop中的配置文件hdfs-site.xml, core-site.xml拷贝到hbase下的conf文件夹下</p>
<p>配置hbase-env.sh文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/lagou/servers/java</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定使用外部的zk集群</span></span><br><span class="line">export HBASE_MANAGES_ZK=false</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>配置hbase-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定hbase数据在集群上的位置路径 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://linux111:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 表示分布式集群 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>linux111:2181,linux112:2181,linux113:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> 修改regionservers文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">linux111</span><br><span class="line">linux112</span><br><span class="line">linux113</span><br></pre></td></tr></table></figure>

<p>创建文件backup-master  指定主节点</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linux112</span><br></pre></td></tr></table></figure>

<p>最后记得把hbase的环境变量配置好.</p>
<blockquote>
<p>export HBASE_HOME&#x3D;&#x2F;opt&#x2F;lagou&#x2F;servers&#x2F;hbase-1.3.1</p>
<p>export PATH&#x3D;${PATH}:${HBASE_HOME}&#x2F;bin</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/hbase/Hbase/" data-id="clumsd6x8000eabr7e0nlewic" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hadoop/hadoop生态之-MapReduce" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/hadoop/hadoop%E7%94%9F%E6%80%81%E4%B9%8B-MapReduce/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.875Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/hadoop/hadoop%E7%94%9F%E6%80%81%E4%B9%8B-MapReduce/" data-id="clumsd6x7000babr76q9tde92" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hadoop/hadoop生态之-HDFS" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/hadoop/hadoop%E7%94%9F%E6%80%81%E4%B9%8B-HDFS/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.874Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="HDFS概述"><a href="#HDFS概述" class="headerlink" title="HDFS概述"></a>HDFS概述</h1><h2 id="什么是HDFS？"><a href="#什么是HDFS？" class="headerlink" title="什么是HDFS？"></a>什么是HDFS？</h2><p>​		HDFS（Hadoop Distributed File System）， 它是一个分布式文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由多台服务器联合起来实现其功能，集群中的服务器有各自的角色。</p>
<p>​		<em>HDFS的使用场景：适合一次写入，多次读出的场景。一个文件经过创建，写入和关闭之后就不会改变。</em></p>
<h2 id="HDFS优缺点？"><a href="#HDFS优缺点？" class="headerlink" title="HDFS优缺点？"></a>HDFS优缺点？</h2><p><font color="red" size="4">优点</font></p>
<ul>
<li><p>高容错性</p>
<ul>
<li><p>数据自动保存多个副本，通过增加副本的形式，提高容错性</p>
</li>
<li><p>当副本丢失以后，它可以自动恢复</p>
</li>
</ul>
</li>
<li><p>适合处理大数据</p>
<p>  ​	能够处理数据规模达到GB、TB、甚至PB级别的数据；</p>
<p>  ​	能够处理百万规模以上的文件数量，数量相当之大</p>
</li>
<li><p>可构建在廉价机器上，通过多副本机制，提高可靠性</p>
</li>
</ul>
<p><font color="red" size="4">缺点</font></p>
<ul>
<li><p>不合适低延时的数据访问， 比如毫秒级的存储数据，是做不到的</p>
</li>
<li><p>无法高效对大量小文件进行存储</p>
<ul>
<li>大量小文件会占用NameNode大量的内存来存储文件目录和块信息，而NameNode的内存总是有限的。</li>
<li>小文件寻址时间会超过读取时间，它违反了HDFS的涉及目标</li>
</ul>
</li>
<li><p>不支持并发写入，文件随机修改</p>
<ul>
<li><p>一个文件只能有一个写，不允许多个线程同时写</p>
</li>
<li><p>仅支持数据append追加，不支持文件的随机修改</p>
</li>
</ul>
</li>
</ul>
<p>​		</p>
<h2 id="HDFS组成架构"><a href="#HDFS组成架构" class="headerlink" title="HDFS组成架构"></a>HDFS组成架构</h2><p>1） NameNode :  Master角色</p>
<ul>
<li>管理HDFS的名称空间，</li>
</ul>
<ul>
<li>配置副本策略(副本节点选择 )</li>
<li>管理数据块（Block）映射信息</li>
<li>处理客户端读写请求</li>
</ul>
<p>2）DataNode： Worker角色</p>
<ul>
<li>存储实际的数据块</li>
<li>执行数据块的读&#x2F;写操作</li>
</ul>
<p>3） Client 客户端</p>
<ul>
<li>文件切分，文件上传HDFS时，Client将文件切分成一个一个的Block,然后进行上传；</li>
<li>与NameNode交互，获取文件的位置信息</li>
<li>与DataNode交互，读取或者写入数据</li>
<li>Client提供命令来管理HDFS，比如NameNode格式化</li>
<li>Client可以通过一些命令访问HDFS，比如对HDFS增删查改操作</li>
</ul>
<p>4）Secondary NameNode，当NameNode节点挂掉时，它并不能马上替换NameNode并提供服务</p>
<ul>
<li>辅助NameNode,分担其工作量，比如定期合并<font color="red">FsImage</font>和<font color="red">Edits</font>，并推送给NameNode;</li>
<li>在紧急情况下，可以辅助恢复NameNode</li>
</ul>
<p><em>Secondary NameNode不是NameNode节点的热备份</em></p>
<h2 id="HDFS文件块大小"><a href="#HDFS文件块大小" class="headerlink" title="HDFS文件块大小"></a>HDFS文件块大小</h2><p><img src="/hadoop%E7%94%9F%E6%80%81%E4%B9%8B-HDFS.assets/image-20220516200530347.png" alt="image-20220516200530347"></p>
<h1 id="HDFS的文件读写"><a href="#HDFS的文件读写" class="headerlink" title="HDFS的文件读写"></a>HDFS的文件读写</h1><h2 id="文件写流程"><a href="#文件写流程" class="headerlink" title="文件写流程"></a>文件写流程</h2><p><img src="/hadoop%E7%94%9F%E6%80%81%E4%B9%8B-HDFS.assets/image-20220517003107413.png" alt="image-20220517003107413"></p>
<p>1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode收到请求后检查目标文件是否已经存在，父目录是否存在；</p>
<p>2）NameNode返回响应给客户端，通知是否可以上传文件；</p>
<p>3）客户端本地对文件切分，按照blocksize大小对文件切分成一个个block块，然后向NameNode请求上传第一个Block；</p>
<p>4）NameNode按照副本定义的个数，返回相应的DataNode节点地址列表给客户端，假设文件副本数为3， 则NameNode返回3个DataNode节点的地址dn1,dn2,dn3；</p>
<p>5）客户端通过FSDataOutputStream模块请求上传数据，dn1收到请求会继续调用dn2,然后dn2调用dn3,在这些datanode之间建立通信管道；</p>
<p>6） Datanode节点按照通信管道中的顺序逐级应答，最终应答给客户端。</p>
<p>写请求: client -&gt;dn1-dn2-&gt;dn3</p>
<p>应答请求： dn3-&gt;dn2-&gt;dn1-&gt;client</p>
<p>7)客户端开始往dn1上传第一个block（数据先从磁盘读取后放到本地内存缓存），以packet为单位发送数据给dn1,dn1收到后再发送给dn2,然后dn2发送给dn3; </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/hadoop/hadoop%E7%94%9F%E6%80%81%E4%B9%8B-HDFS/" data-id="clumsd6x70009abr73zgoe9w5" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hadoop/hadoop集群安装" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/hadoop/hadoop%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.873Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <pre><code>    独立安装hadoop集群已经不是很么困难的事情了，以往没有太过在意安装中出现的问题，每次都是参考下网络上的安装文档，将集群安装好就好了，最近趁着重新学习总结hadoop3.x版本的机会，完整按照官网上的步骤操作了一遍。现在记录下来，作为自己的资料库中一员。
</code></pre>
<h1 id="环境准备工作"><a href="#环境准备工作" class="headerlink" title="环境准备工作"></a>环境准备工作</h1><ol>
<li><p>java安装：</p>
<p>  linux下安装java这里就不再详细介绍了。</p>
</li>
<li><p>hadoop安装：</p>
<p> 上传hadoop安装包，解压到指定目录下就可以了，这里使用的hadoop版本为hadoop-3.1.3。</p>
</li>
<li><p>配置环境变量</p>
<p> 主要是把相关的程序的路径添加到环境变量中，这里我的机器配置如下:</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile.d/my_env.sh </span><br></pre></td></tr></table></figure>

<p> 相关自定义的配置放到一起如下：</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">java配置路径</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hadoop配置路径</span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop-3.1.3</span><br><span class="line">export PATH=$HADOOP_HOME/bin:$PATH                                 </span><br></pre></td></tr></table></figure>

<p> 重新加载添加环境变量</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<p> &#x2F;etc&#x2F;profile文件中会依次执行&#x2F;etc&#x2F;profile.d&#x2F;目录下的脚本。</p>
</li>
</ol>
<h1 id="独立模式-Standalone-Operation"><a href="#独立模式-Standalone-Operation" class="headerlink" title="独立模式 Standalone Operation"></a>独立模式 Standalone Operation</h1><p>hadoop支持standalone模式，该模式下直接下载安装包，解压后就可以直接使用了，它比较适合用来调式mr程序。</p>
<p>独立模式下不需要依赖hdfs文件系统，用户可以直接运行mr程序，指定本地目录即可。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mkdir</span> input</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cp</span> etc/hadoop/*.xml input</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar grep input output <span class="string">&#x27;dfs[a-z.]+&#x27;</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> output/*</span></span><br></pre></td></tr></table></figure>

<p><img src="/hadoop%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85.assets/image-20211223135342595.png" alt="image-20211223135342595"></p>
<p>如果是初次安装hadoop，还没有对$HADOOP_HOME&#x2F;etc目录下的文件，那么运行结果应该像截图上显示的。</p>
<h1 id="伪分布式集群-Pseudo-Distributed-Operation"><a href="#伪分布式集群-Pseudo-Distributed-Operation" class="headerlink" title="伪分布式集群 Pseudo-Distributed Operation"></a>伪分布式集群 Pseudo-Distributed Operation</h1><p>伪分布模式与独立模式的区别在于，伪分布模式提供了hdfs文件系统，但是它仍然还是可以在单台机器上正常运行。 </p>
<p>免密码登陆设置</p>
<p>查看是否可以免密登陆</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh hostname</span><br></pre></td></tr></table></figure>

<p><em>hostname根据实际的本机情况填写</em></p>
<p>配置免密登陆</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ssh-keygen -t rsa -P <span class="string">&#x27;&#x27;</span> -f ~/.ssh/id_rsa</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">chmod</span> 0600 ~/.ssh/authorized_keys</span></span><br></pre></td></tr></table></figure>





<ol>
<li><p>修改配置文件</p>
<p> core-site.xml</p>
</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​		hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>单台机器上只有一个副本<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>格式化文件系统</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hadoop namenode -format</span><br></pre></td></tr></table></figure>

<p>首次启动文件系统时，需要先进行格式化。</p>
<ol start="3">
<li>启动集群</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sbin/start-dfs.sh</span></span><br></pre></td></tr></table></figure>

<p>hadoop运行起来后的输出记录会写到日志文件中，该日志目录可以通过配置$HADOOP_LOG_DIR来设置，如果没有单独配置的话，会默认写到 $HADOOP_HOME&#x2F;logs目录下的日志文件中。</p>
<ol start="4">
<li>查看hdfs</li>
</ol>
<p>url : <a target="_blank" rel="noopener" href="http://hadoop101:9870/">http://hadoop101:9870/</a></p>
<p>这里服务器的hostname为hadoop101， 端口为9870，(在hadoop2.x版本中端口还是50070)</p>
<ol start="5">
<li>上传文件测试</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">bin/hdfs dfs -<span class="built_in">mkdir</span> input</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">bin/hdfs dfs -put etc/hadoop/*.xml input</span></span><br></pre></td></tr></table></figure>



<ol start="6">
<li>运行mr任务</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar grep input output <span class="string">&#x27;dfs[a-z.]+&#x27;</span></span></span><br></pre></td></tr></table></figure>



<p>另外还可以安装单点的yarn集群，步骤如下</p>
<ol>
<li><p>修改配置文件</p>
<p> yarn-site.xml</p>
</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​		mapred-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>



<ol start="2">
<li>启动yarn集群</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sbin/start-yarn.sh</span></span><br></pre></td></tr></table></figure>



<ol start="3">
<li><p>浏览yarn界面</p>
<p> url : <a target="_blank" rel="noopener" href="http://hadoop101:8088/">http://hadoop101:8088/</a></p>
</li>
</ol>
<h1 id="完全集群模式安装-Fully-Distributed-Operation"><a href="#完全集群模式安装-Fully-Distributed-Operation" class="headerlink" title="完全集群模式安装 Fully-Distributed Operation"></a>完全集群模式安装 Fully-Distributed Operation</h1><ol>
<li>集群规划</li>
</ol>
<p>注意：</p>
<p>1） NameNode与SecondNameNode不要安装在同一台机器上</p>
<p>2）ResourceManager消耗内存较多，不要和NameNode、SecondNameNode安装在一台机器上</p>
<table>
<thead>
<tr>
<th></th>
<th>hadoop101</th>
<th>hadoop102</th>
<th>hadoop103</th>
</tr>
</thead>
<tbody><tr>
<td>NameNode</td>
<td>√</td>
<td></td>
<td></td>
</tr>
<tr>
<td>SecondNameNode</td>
<td></td>
<td>√</td>
<td></td>
</tr>
<tr>
<td>DataNode</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>ResourceManager</td>
<td></td>
<td></td>
<td>√</td>
</tr>
<tr>
<td>NodeManager</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
</tbody></table>
<ol start="2">
<li>配置文件说明</li>
</ol>
<p>Hadoop配置文件分为默认和自定义的配置文件，当用户想要修改某一默认配置时，需要修改自定义配置相应的值。</p>
<p>（1） 默认配置文件位置 (这里使用的hadoop版本是3.1.3)</p>
<table>
<thead>
<tr>
<th>要获取的默认文件</th>
<th>文件存放在Hadoop的jar包中的位置</th>
</tr>
</thead>
<tbody><tr>
<td>[core-default.xml]</td>
<td>hadoop-common-3.1.3.jar&#x2F;core-default.xml</td>
</tr>
<tr>
<td>[hdfs-default.xml]</td>
<td>hadoop-hdfs-3.1.3.jar&#x2F;hdfs-default.xml</td>
</tr>
<tr>
<td>[yarn-default.xml]</td>
<td>hadoop-yarn-common-3.1.3.jar&#x2F;yarn-default.xml</td>
</tr>
<tr>
<td>[mapred-default.xml]</td>
<td>hadoop-mapreduce-client-core-3.1.3.jar&#x2F;mapred-default.xml</td>
</tr>
</tbody></table>
<p>（2）自定义配置文件</p>
<p>在$HADOOP_HOME&#x2F;etc&#x2F;hadoop&#x2F;路径下的<strong>core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xm</strong>等文件。</p>
<ol start="3">
<li>配置集群</li>
</ol>
<p>进入自定义配置目录下</p>
<p>修改core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定NameNode的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop101:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定hadoop数据的存储目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置HDFS网页登录使用的静态用户 hadoop --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>修改hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- nn web端访问地址--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 2nn web端访问地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>修改yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定MR走shuffle --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定ResourceManager的地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 环境变量的继承 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>修改mapred-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>



<ol start="4">
<li><p>配置workers文件</p>
<p> 配置记录工作节点hostname的文件记录，在老版本中文件名是slaves。</p>
<p> <img src="/hadoop%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85.assets/image-20211223161033926.png" alt="image-20211223161033926"></p>
</li>
</ol>
<p>上述配置修改好后，同步到哥哥节点机器上。</p>
<p>在hadoop101上启动hdfs服务，</p>
<p>sbin&#x2F;start-dfs.sh</p>
<p>在hadoop103上启动yarn服务</p>
<p>sbin&#x2F;start-yarn.sh</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/hadoop/hadoop%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/" data-id="clumsd6x8000cabr7cq5j7dx9" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hadoop/单机版hadoop测试安装" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/hadoop/%E5%8D%95%E6%9C%BA%E7%89%88hadoop%E6%B5%8B%E8%AF%95%E5%AE%89%E8%A3%85/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.873Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>查看ssh服务状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemsetup -getremotelogin</span><br></pre></td></tr></table></figure>

<p>开启ssh服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemsetup -setremotelogin on</span><br></pre></td></tr></table></figure>

<p>关闭ssh服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemsetup -setremotelogin on</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/hadoop/%E5%8D%95%E6%9C%BA%E7%89%88hadoop%E6%B5%8B%E8%AF%95%E5%AE%89%E8%A3%85/" data-id="clumsd6x8000dabr7cqcz6ymu" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-flume/flume使用" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/flume/flume%E4%BD%BF%E7%94%A8/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.872Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="一-Flume概述"><a href="#一-Flume概述" class="headerlink" title="一. Flume概述"></a>一. Flume概述</h1><p>​		<strong>分布式</strong>，<strong>高可靠</strong>，<strong>高可用</strong>的海量日志采集，聚合，传输系统。采集各类数据，对数据进行简单的处理, 并写到各种数据接收方(hdfs，kafka， elasticsearch等)。</p>
<p><img src="/flume%E4%BD%BF%E7%94%A8.assets/image-20210327204708727.png" alt="image-20210327204708727"></p>
<p>特点：</p>
<ul>
<li>分布式： flume分布式集群部署，扩展性好；</li>
<li>可靠性好：当节点出现故障时，日志能够被传送到其他节点上；</li>
<li>实时采集：流模式进行实时采集</li>
</ul>
<p>但是flume配置使用较繁琐，对使用者要求较高。</p>
<h2 id="1-1-Flume中主要组件："><a href="#1-1-Flume中主要组件：" class="headerlink" title="1.1 Flume中主要组件："></a>1.1 Flume中主要组件：</h2><p><font color="red" size=4>Event</font> :  Flume中数据流的最小单位；</p>
<p><font color="red" size=4>Agent</font> :  本质是一个JVM进程 ,该进程控制Event数据流从外部日志生产者传输到接收者(或者是下一个Agent), 一个完整的Agent中包含三个组件Source, Channel和Sink。</p>
<p><font color="red" size=4>Source</font> :  对接外部数据源，将接收到的数据发送到Flume Agent中；</p>
<p><font color="red" size=4>Sink</font> :  不断轮询Channel中的事件且批量移除他们，将这些事件批量写入到存储系统或索引系统中，或者被发送到另一个Flume Agent。</p>
<p>​	Sink是完全<strong>事务性</strong>的，在从Channel中批量删除数据之前，每个Sink用Channel启动一个事务，批量事务一旦成功写到后续的存储系统或下一个Agent，Sink就会利用Channel提交事务，事务一旦被提交，该Channel从自己内部缓冲区删除事件， 这样当后续存储系统崩溃时，存储到Channel中的数据也不会丢失，只有成功写落地后才会删除。</p>
<p><font color="red" size=4>Channel</font> :  Source和Sink之间的缓冲区，Channel允许Source和Sink运作在不同的速率上，Channel是线程安全的，可以同时处理多个Source的写入操作及多个Sink的读取操作。</p>
<p>​		常见的Channel包括：</p>
<p>​	1. <strong>Memory Channel</strong> 是内存中队列，在允许数据丢失的情况下适用，如果不允许数据丢失，应该避免使用Memory Channel，因为程序死亡，机器宕机或重启都可能会导致数据丢失。</p>
<pre><code>      2. **File Channel** 将所有事件Event写到磁盘，因此在程序关闭或机器宕机的情况下不会丢失数据。
</code></pre>
<h2 id="1-2-Flume常用的拓扑模式"><a href="#1-2-Flume常用的拓扑模式" class="headerlink" title="1.2 Flume常用的拓扑模式"></a>1.2 Flume常用的拓扑模式</h2><p>1.2.1 串行模式</p>
<p><img src="/flume%E4%BD%BF%E7%94%A8.assets/image-20210327222139946.png" alt="image-20210327222139946"></p>
<p>依次将数据往后传递，这种传递经过的Agent不宜太多，因为会影响数据最终到达目的地的耗时。</p>
<p>1.2.2 复制模式(单Source多Channel,Sink模式)</p>
<p><img src="/flume%E4%BD%BF%E7%94%A8.assets/image-20210327222304005.png" alt="image-20210327222304005"></p>
<p>该模式将一个来源的数据复制多份发到不同的目的地。</p>
<p>1.2.3 负载均衡模式(单Source,Channel多Sink)</p>
<p><img src="/flume%E4%BD%BF%E7%94%A8.assets/image-20210327222653175.png" alt="image-20210327222653175"></p>
<p>该模式下，使用多个Sink将数据发送到目的地，它们去向是统一个位置， 这种模式使用多个Sink来处理Channel中的数据，由于Source到Channel的速率与Channel到Sink的速率不一致， 这样能更快的消费掉堆积在Channel中的数据。</p>
<p>1.2.4 聚合模式(多Source)</p>
<p><img src="/flume%E4%BD%BF%E7%94%A8.assets/image-20210327223210125.png" alt="image-20210327223210125"></p>
<p>这种模式下将多个来源的数据聚合到同一的目的地，在实际生产中也是比较常见的，大数据场景下数据的来源就是多样的。</p>
<h2 id="1-3-Flume原理"><a href="#1-3-Flume原理" class="headerlink" title="1.3 Flume原理"></a>1.3 Flume原理</h2><p><img src="/flume%E4%BD%BF%E7%94%A8.assets/image-20210330230708158.png" alt="image-20210330230708158"></p>
<p>具体过程：</p>
<ol>
<li>Source接收事件，交给其Channel处理器处理事件；</li>
<li>处理器通过拦截器Interceptor，对事件一些处理,比如压缩解码，正则拦截，时间戳拦截，分类等；</li>
<li>经过拦截器处理过的事件再传给Channel选择器，将事件写入相应的Channel，<br>Channel Selector有两种：Replicating Channel Selector（默认）,会将source过来的Event发往所有Channel(比较常用的场景是，多个Channel实现冗余副本，保证可用性)Multiplexing Channel Selector，根据配置分发event。此selector会根据event中某个header对应的value来将event发往不同的channel</li>
<li>最后由Sink处理器处理各个Channel的事件。</li>
</ol>
<h1 id="二-Flume基础使用"><a href="#二-Flume基础使用" class="headerlink" title="二. Flume基础使用"></a>二. Flume基础使用</h1><p>​		Flume 支持的数据源种类有很多，可以来自directory、http、kafka等。Flume提供了Source组件用来采集数据源。<br>常见的 Source 有：<br>（1）avro source：监听 Avro 端口来接收外部 avro 客户端的事件流。avro-source接收到的是经过avro序列化后的数据，然后反序列化数据继续传输。如果是avro source的话，源数据必须是经过avro序列化后的数据。利用 Avro source可以实现多级流动、扇出流、扇入流等效果。接收通过flume提供的avro客户端发送的日志信息。<br>（2）exec source：可以将命令产生的输出作为source;<br>（3）netcat source: 一个NetCat Source用来监听端口，并接收监听到的数据；<br>（4）spooling directory source:  将指定文件添加到自动搜索的目录下，flume持续监听这个目录，把文件当source 来处理。<br>​			注意: 一旦文件放到目录中之后，便不能修改，如果修改，flume会报错，此外，也不能有重名文件。<br>（5）Taildir source (1.7+版本)：监听多个目录下的文件，一旦文件目录下的文件有新写入的数据，就会将其写入到指定的sink内，该类型源可靠性高，不会丢失数据，也不会对跟踪的文件做任何修改，但是不支持二进制文件，支持一行一行的读取文本文件。</p>
<p>​		采集到的日志需要进行缓存，Flume提供了Channel组件用来缓存数据。常见的<br>Channel 有：<br>（1）memory channel：缓存到内存中（最常用）<br>（2）file channel：缓存到文件中<br>（3）JDBC channel：通过JDBC缓存到关系型数据库中<br>（4）kafka channel：缓存到kafka中</p>
<p>​		缓存的数据最终需要进行保存，Flume提供了Sink组件用来保存数据。常见的 Sink有：<br>（1）logger sink：将信息显示在标准输出上，主要用于测试；<br>（2）avro sink：Flume events发送到sink，转换为Avro events，并发送到配置好的hostname&#x2F;port。从配置好的channel按照配置好的批量大小批量获取events；<br>（3）null sink：将接收到events全部丢弃；<br>（4）HDFS sink：将 events 写进HDFS。支持创建文本和序列文件，支持两种文件类型压缩。文件可以基于数据的经过时间、大小、事件的数量周期性地滚动；<br>（5）Hive sink：该sink streams 将包含分割文本或者JSON数据的events直接传送到Hive表或分区中。使用Hive 事务写events。当一系列events提交到Hive时，它们马上可以被Hive查询到；<br>（6）HBase sink：保存到HBase中；<br>（7）kafka sink：保存到kafka中；</p>
<p>使用参考 <a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html">http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html</a></p>
<p>以下样例使用netcat source, memory channel, logger sink</p>
<p>编辑配置文件, 命名为 flume-netcat-logger.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">a1是agent的名称。<span class="built_in">source</span>、channel、sink的名称分别为：r1  c1  k1</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = linux111</span><br><span class="line">a1.sources.r1.port = 8888</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">channel</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">通道类型 memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">memory通道中最大的flume事件数量</span></span><br><span class="line">a1.channels.c1.capacity = 10000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">source</span>、channel、sink之间的关系</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>启动flume agent</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$FLUME_HOME/bin/flume-ng agent --name a1 --conf-file conf/flume-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>使用telnet向本机的8888端口发送数据</p>
<p><img src="/flume%E4%BD%BF%E7%94%A8.assets/image-20210330230044443.png" alt="image-20210330230044443"></p>
<p>flume agent配置项参考:</p>
<p><a target="_blank" rel="noopener" href="http://flume.apache.org/FlumeUserGuide.html">http://flume.apache.org/FlumeUserGuide.html</a></p>
<h2 id="2-1-监控本地日志文件，收集内容到实时上传到HDFS"><a href="#2-1-监控本地日志文件，收集内容到实时上传到HDFS" class="headerlink" title="2.1 监控本地日志文件，收集内容到实时上传到HDFS"></a>2.1 监控本地日志文件，收集内容到实时上传到HDFS</h2><p>​		数据输出到HDFS上需要提供hadoop相关的jar, 在$HADOOP_HOME&#x2F;share&#x2F;hadoop&#x2F;httpfs&#x2F;tomcat&#x2F;webapps&#x2F;webhdfs&#x2F;WEB-INF&#x2F;lib系可以找到这些文件</p>
<p><img src="/flume%E4%BD%BF%E7%94%A8.assets/image-20210331003145014.png" alt="image-20210331003145014"></p>
<p>TAILDIR Source 配置</p>
<p>taildir Source的特点：</p>
<ul>
<li>使用正则表达式匹配目录中的文件名，匹配多个目录中的多个文件</li>
<li>监控的文件中，一旦有数据写入，Flume就会将信息写入到指定的Sink</li>
<li>高可靠，不会丢失数据</li>
<li>不会对跟踪文件有任何处理，不会重命名也不会删除</li>
<li>不支持Windows，不能读二进制文件。支持按行读取文本文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.positionFile =/data/lagoudw/conf/startlog_position.json</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1 = /data/lagoudw/logs/start/.*log</span><br></pre></td></tr></table></figure>

<p>HDFS SINK 配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">定义路径格式</span></span><br><span class="line">a1.sinks.k1.hdfs.path = /user/data/logs/start/%Y-%m-%d/</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = startlog.</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置文件滚动方式（文件大小32M）</span></span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 33554432</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 0</span><br><span class="line">a1.sinks.k1.hdfs.idleTimeout = 0</span><br><span class="line">a1.sinks.k1.hdfs.minBlockReplicas = 1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">向hdfs上刷新的event的个数</span></span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用本地机器时间标准</span></span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br></pre></td></tr></table></figure>

<p>HDFS Sink 都会采用滚动生成文件的方式，滚动生成文件的策略有：</p>
<ul>
<li>基于时间。hdfs.rollInterval 30秒</li>
<li>基于文件大小。hdfs.rollSize 1024字节</li>
<li>基于event数量。hdfs.rollCount 10个event</li>
<li>基于文件空闲时间。hdfs.idleTimeout 0 0，禁用</li>
<li>minBlockReplicas。默认值与 hdfs 副本数一致。设为1是为了让 Flume 感知不到hdfs的块复制，此时其他的滚动方式配置（时间间隔、文件大小、events数量）才不会受影响</li>
</ul>
<p>完整agent配置文件 flume-log2hdfs1.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">taildir <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.positionFile = </span><br><span class="line">/data/lagoudw/conf/startlog_position.json</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1 = /data/lagoudw/logs/start/.*log</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">memorychannel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 100000</span><br><span class="line">a1.channels.c1.transactionCapacity = 2000</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hdfs sink</span></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置输出到集群上的路径</span></span><br><span class="line">a1.sinks.k1.hdfs.path = /user/data/logs/start/%Y-%m-%d/</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = startlog.</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置文件滚动方式（文件大小32M）</span></span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 33554432</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 0</span><br><span class="line">a1.sinks.k1.hdfs.idleTimeout = 0</span><br><span class="line">a1.sinks.k1.hdfs.minBlockReplicas = 1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">向hdfs上刷新的event的个数</span></span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 1000</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用本地时间</span></span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$FLUME_HOME/bin/flume-ng agent --name a1 --conf-file conf/flume-log2hdfs1.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>内存溢出问题</p>
<p><img src="/flume%E4%BD%BF%E7%94%A8.assets/image-20210408225138228.png" alt="image-20210408225138228"></p>
<p>出现这种情况时，需要设置jvm内存</p>
<p>在$FLUME_HOME&#x2F;conf&#x2F;flume-env.sh中添加配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Give Flume more memory and pre-allocate, <span class="built_in">enable</span> remote monitoring via JMX</span></span><br><span class="line">export JAVA_OPTS=&quot;-Xms2000m -Xmx2000m -Dcom.sun.management.jmxremote&quot;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 可以将-Xms和-Xmx参数设置的大些</span></span></span><br></pre></td></tr></table></figure>

<p>然后再次启动，需要加载conf下的配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 使用--conf指定配置的路径</span></span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash">FLUME_HOME/bin/flume-ng agent --conf <span class="variable">$FLUME_HOME</span>/conf --name a1 --conf-file conf/flume-log2hdfs1.conf -Dflume.root.logger=INFO,console</span></span><br></pre></td></tr></table></figure>

<p>我们移动文件到指定的source路径下， flume会将文件中的内容写出到集群指定路径下。</p>
<h2 id="2-2-自定义拦截器"><a href="#2-2-自定义拦截器" class="headerlink" title="2.2 自定义拦截器"></a>2.2 自定义拦截器</h2><p>上面例子中我们不是要获取文件的所有内容，而是获取日志文件中我们需要的数据内容。</p>
<p>将日志文件中的内容打印到控制台查看</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">taildir <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.positionFile = </span><br><span class="line">/data/lagoudw/conf/startlog_position.json</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1 = /data/lagoudw/logs/start/.*log</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">memorychannel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 100000</span><br><span class="line">a1.channels.c1.transactionCapacity = 2000</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p><img src="/flume%E4%BD%BF%E7%94%A8.assets/image-20210408235836957.png" alt="image-20210408235836957"></p>
<p>可以看到Logger使用的Processor为DefaultSinkProcessor, 在1.9.0 版本的flume中不支持自定义的Processor。</p>
<p><img src="/flume%E4%BD%BF%E7%94%A8.assets/image-20210409000142397.png" alt="image-20210409000142397"></p>
<p>但是我们可以自定义拦截器来处理数据。</p>
<p>自定义拦截器的原理：</p>
<ol>
<li>自定义拦截器要继承Flume的Intercecptor</li>
<li>Event分为header和body（接收的字符串）</li>
<li>获取header和body</li>
<li>从body中获取想要的内容</li>
<li>将转换后的字符串放置在header中</li>
</ol>
<p>编写自定以拦截器</p>
<p>maven依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p>样例日志数据:</p>
<p>编写自定义拦截器</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> XandJ</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomerInterceptor</span> <span class="keyword">implements</span> <span class="title class_">Interceptor</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initialize</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// 逐条处理数据</span></span><br><span class="line">    <span class="keyword">public</span> Event <span class="title function_">intercept</span><span class="params">(Event event)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">eventBody</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>(event.getBody(), Charsets.UTF_8);</span><br><span class="line"></span><br><span class="line">        Map&lt;String, String&gt; headersMap = event.getHeaders();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 以空格分割</span></span><br><span class="line">        String[] bodyArr = eventBody.split(<span class="string">&quot;\\s+&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">jsonStr</span> <span class="operator">=</span> bodyArr[<span class="number">6</span>];</span><br><span class="line">            <span class="type">JSONObject</span> <span class="variable">jsonObject</span> <span class="operator">=</span> JSON.parseObject(jsonStr);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取数据的时间戳</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">timestampStr</span> <span class="operator">=</span> jsonObject.getJSONObject(<span class="string">&quot;app_active&quot;</span>).getString(<span class="string">&quot;time&quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 转换时间格式</span></span><br><span class="line">            <span class="type">long</span> <span class="variable">timestamp</span> <span class="operator">=</span> Long.parseLong(timestampStr);</span><br><span class="line">            <span class="type">DateTimeFormatter</span> <span class="variable">dateTimeFormatter</span> <span class="operator">=</span> DateTimeFormatter.ofPattern(<span class="string">&quot;yyyy-MM-dd&quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="type">Instant</span> <span class="variable">instant</span> <span class="operator">=</span> Instant.ofEpochMilli(timestamp);</span><br><span class="line">            <span class="type">LocalDateTime</span> <span class="variable">localDateTime</span> <span class="operator">=</span> LocalDateTime.ofInstant(instant, ZoneId.systemDefault());</span><br><span class="line">            <span class="type">String</span> <span class="variable">date</span> <span class="operator">=</span> dateTimeFormatter.format(localDateTime);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            headersMap.put(<span class="string">&quot;logtime&quot;</span>, date);</span><br><span class="line">            event.setHeaders(headersMap);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            headersMap.put(<span class="string">&quot;logtime&quot;</span>, <span class="string">&quot;Unknown&quot;</span>);</span><br><span class="line">            event.setHeaders(headersMap);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> List&lt;Event&gt; <span class="title function_">intercept</span><span class="params">(List&lt;Event&gt; list)</span> &#123;</span><br><span class="line">        List&lt;Event&gt; eventList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (Event event : list) &#123;</span><br><span class="line">            <span class="comment">// 单条转化</span></span><br><span class="line">            <span class="type">Event</span> <span class="variable">outEvent</span> <span class="operator">=</span> intercept(event);</span><br><span class="line">            <span class="keyword">if</span>(event != <span class="literal">null</span>) &#123;</span><br><span class="line">                <span class="comment">// j</span></span><br><span class="line">                eventList.add(outEvent);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 返回新的event数组</span></span><br><span class="line">        <span class="keyword">return</span> eventList;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Builder</span> <span class="keyword">implements</span> <span class="title class_">Interceptor</span>.Builder&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Interceptor <span class="title function_">build</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">CustomerInterceptor</span>();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p>测试自定义的拦截器</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">a1是agent的名称。<span class="built_in">source</span>、channel、sink的名称分别为：r1  c1  k1</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = linux111</span><br><span class="line">a1.sources.r1.port = 8888</span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = cn.lagou.dw.interceptor.CustomerInterceptor$Builder</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 10000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">source</span>、channel、sink之间的关系</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>放到服务器上测试</p>
<p><img src="/flume%E4%BD%BF%E7%94%A8.assets/image-20210410175836411.png" alt="image-20210410175836411"></p>
<p><img src="/flume%E4%BD%BF%E7%94%A8.assets/image-20210410175812989.png" alt="image-20210410175812989"></p>
<p>使用自定义拦截器来监控日志的配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">taildir <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.positionFile = </span><br><span class="line">/data/lagoudw/conf/startlog_position.json</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1 = /data/lagoudw/logs/start/.*log</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置拦截器的名称为i1</span></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = cn.lagou.dw.interceptor.CustomerInterceptor$Builder</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">memorychannel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 100000</span><br><span class="line">a1.channels.c1.transactionCapacity = 2000</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Use a channel <span class="built_in">which</span> buffers events <span class="keyword">in</span> memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/flume/flume%E4%BD%BF%E7%94%A8/" data-id="clumsd6x60008abr711ct95oz" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-flume/flume自定义拦截器" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/flume/flume%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8B%A6%E6%88%AA%E5%99%A8/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.865Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>​		在使用flume采集日志文件时，往往日志文件的内容不是我们期待的格式，可以自定义flume的拦截器将数据转换之后再传给后端。</p>
<p>例如我们要采集的日志文件格式内容如下：</p>
<p><img src="/flume%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8B%A6%E6%88%AA%E5%99%A8.assets/image-20210410165304726.png" alt="image-20210410165304726"></p>
<p>在这份数据文件中，我们只需要json格式的内容， 而日志文件前面后的内容则不是我们需要的。</p>
<p>需求: </p>
<p>将日志内容采集到HDFS集群上, 这里日志中的内容</p>
<p>自定义拦截器的原理：</p>
<ol>
<li>自定义拦截器要继承Flume的Intercecptor</li>
<li>Event分为header和body（接收的字符串）</li>
<li>获取header和body</li>
<li>从body中获取想要的内容</li>
<li>将转换后的字符串放置在header中</li>
</ol>
<h1 id="一-自定义的拦截器，解析数据"><a href="#一-自定义的拦截器，解析数据" class="headerlink" title="一. 自定义的拦截器，解析数据"></a>一. 自定义的拦截器，解析数据</h1><p>导入maven依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>CustomerInterceptor.java</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> com.google.common.base.Charsets;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.interceptor.Interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Instant;</span><br><span class="line"><span class="keyword">import</span> java.time.LocalDateTime;</span><br><span class="line"><span class="keyword">import</span> java.time.ZoneId;</span><br><span class="line"><span class="keyword">import</span> java.time.format.DateTimeFormatter;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> XandJ</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomerInterceptor</span> <span class="keyword">implements</span> <span class="title class_">Interceptor</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initialize</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// 逐条处理</span></span><br><span class="line">    <span class="keyword">public</span> Event <span class="title function_">intercept</span><span class="params">(Event event)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">eventBody</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>(event.getBody(), Charsets.UTF_8);</span><br><span class="line"></span><br><span class="line">        Map&lt;String, String&gt; headersMap = event.getHeaders();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 以空格分割</span></span><br><span class="line">        String[] bodyArr = eventBody.split(<span class="string">&quot;\\s+&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">jsonStr</span> <span class="operator">=</span> bodyArr[<span class="number">6</span>];</span><br><span class="line">            <span class="type">JSONObject</span> <span class="variable">jsonObject</span> <span class="operator">=</span> JSON.parseObject(jsonStr);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取数据的时间戳</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">timestampStr</span> <span class="operator">=</span> jsonObject.getJSONObject(<span class="string">&quot;app_active&quot;</span>).getString(<span class="string">&quot;time&quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 转换时间格式</span></span><br><span class="line">            <span class="type">long</span> <span class="variable">timestamp</span> <span class="operator">=</span> Long.parseLong(timestampStr);</span><br><span class="line">            <span class="type">DateTimeFormatter</span> <span class="variable">dateTimeFormatter</span> <span class="operator">=</span> DateTimeFormatter.ofPattern(<span class="string">&quot;yyyy-MM-dd&quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="type">Instant</span> <span class="variable">instant</span> <span class="operator">=</span> Instant.ofEpochMilli(timestamp);</span><br><span class="line">            <span class="type">LocalDateTime</span> <span class="variable">localDateTime</span> <span class="operator">=</span> LocalDateTime.ofInstant(instant, ZoneId.systemDefault());</span><br><span class="line">            <span class="type">String</span> <span class="variable">date</span> <span class="operator">=</span> dateTimeFormatter.format(localDateTime);</span><br><span class="line"></span><br><span class="line">			<span class="comment">// 将解析结果存入到header中， 在配置中可以引用该变量</span></span><br><span class="line">            headersMap.put(<span class="string">&quot;logtime&quot;</span>, date);</span><br><span class="line">            event.setHeaders(headersMap);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            headersMap.put(<span class="string">&quot;logtime&quot;</span>, <span class="string">&quot;Unknown&quot;</span>);</span><br><span class="line">            event.setHeaders(headersMap);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> List&lt;Event&gt; <span class="title function_">intercept</span><span class="params">(List&lt;Event&gt; list)</span> &#123;</span><br><span class="line">        List&lt;Event&gt; eventList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (Event event : list) &#123;</span><br><span class="line">            <span class="comment">// 单条转化</span></span><br><span class="line">            <span class="type">Event</span> <span class="variable">outEvent</span> <span class="operator">=</span> intercept(event);</span><br><span class="line">            <span class="keyword">if</span>(event != <span class="literal">null</span>) &#123;</span><br><span class="line">                <span class="comment">// j</span></span><br><span class="line">                eventList.add(outEvent);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 返回新的event数组</span></span><br><span class="line">        <span class="keyword">return</span> eventList;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Builder</span> <span class="keyword">implements</span> <span class="title class_">Interceptor</span>.Builder&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Interceptor <span class="title function_">build</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">CustomerInterceptor</span>();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>将代码归当成jar文件，上传到$FLUME_HOME&#x2F;lib目录下。</p>
<p>简单将采集内容打印在控制台上。</p>
<p>flume-netcat2log.conf </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">a1是agent的名称。<span class="built_in">source</span>、channel、sink的名称分别为：r1  c1  k1</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = linux111</span><br><span class="line">a1.sources.r1.port = 8888</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用自定义的拦截器</span></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = cn.lagou.dw.interceptor.CustomerInterceptor$Builder</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 10000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">source</span>、channel、sink之间的关系</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>启动flume开始采集日志，看到控制台打印出的结果如下:</p>
<img src="flume%E4%BD%BF%E7%94%A8.assets/image-20210410175812989.png"/>





<h1 id="二-自定义拦截器处理多种日志格式"><a href="#二-自定义拦截器处理多种日志格式" class="headerlink" title="二. 自定义拦截器处理多种日志格式"></a>二. 自定义拦截器处理多种日志格式</h1><p>要想能够处理多种格式的数据文件，则必须能够识别出多种格式文件，因此flume中可以设置header内容，再在定义的拦截器中获取这些内容，通过这种方式我们便能依据文件类型的不同采取不同的处理逻辑。</p>
<p>flume-log2hdfs3.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">a1是agent的名称。<span class="built_in">source</span>、channel、sink的名称分别为：r1  c1  k1</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">taildir <span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.positionFile = /data/lagoudw/conf/startlog_position.json</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">监控多个目录</span></span><br><span class="line">a1.sources.r1.filegroups = f1 f2</span><br><span class="line">a1.sources.r1.filegroups.f1 = /data/lagoudw/logs/start/.*log</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">自定义header内容 logtype = start</span></span><br><span class="line">a1.sources.r1.headers.f1.logtype = start</span><br><span class="line">a1.sources.r1.filegroups.f2 = /data/lagoudw/logs/event/.*log</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">自定义header内容logtype = event</span></span><br><span class="line">a1.sources.r1.headers.f2.logtype = event</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">自定义拦截器</span></span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = cn.lagou.dw.flume.interceptor.LogTypeInterceptor$Builder</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">memorychannel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 100000a1.channels.c1.transactionCapacity = 2000</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">hdfs sink</span></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">输出到hdfs上的路径</span></span><br><span class="line">a1.sinks.k1.hdfs.path = /user/data/logs/%&#123;logtype&#125;/dt=%&#123;logtime&#125;/</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = startlog</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置文件滚动方式（文件大小32M）</span></span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 33554432</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 0</span><br><span class="line">a1.sinks.k1.hdfs.idleTimeout = 0</span><br><span class="line">a1.sinks.k1.hdfs.minBlockReplicas = 1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">向hdfs上刷新的event的个数</span></span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 1000</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Bind the <span class="built_in">source</span> and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">source</span>、channel、sink之间的关系</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>



<p>上面配置中我们在header中定义了变量logtype, 在自定义的拦截器中获取这些头部的信息， 根据这个内容来采用不同的处理逻辑。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONArray;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> com.google.common.base.Charsets;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.interceptor.Interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Instant;</span><br><span class="line"><span class="keyword">import</span> java.time.LocalDateTime;</span><br><span class="line"><span class="keyword">import</span> java.time.ZoneId;</span><br><span class="line"><span class="keyword">import</span> java.time.format.DateTimeFormatter;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> XandJ</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogTypeInterceptor</span> <span class="keyword">implements</span> <span class="title class_">Interceptor</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initialize</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="comment">// 逐条处理</span></span><br><span class="line">    <span class="keyword">public</span> Event <span class="title function_">intercept</span><span class="params">(Event event)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">eventBody</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>(event.getBody(), Charsets.UTF_8);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取header内容， 可以在配置中指定header的内容</span></span><br><span class="line">        Map&lt;String, String&gt; headersMap = event.getHeaders();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 以空格分割</span></span><br><span class="line">        String[] bodyArr = eventBody.split(<span class="string">&quot;\\s+&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">jsonStr</span> <span class="operator">=</span> bodyArr[<span class="number">6</span>];</span><br><span class="line">            <span class="type">JSONObject</span> <span class="variable">jsonObject</span> <span class="operator">=</span> JSON.parseObject(jsonStr);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取数据的时间戳</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">timestampStr</span> <span class="operator">=</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 根据传递的header内容选则不同的处理逻辑</span></span><br><span class="line">            <span class="keyword">if</span>(headersMap.getOrDefault(<span class="string">&quot;logtype&quot;</span>,<span class="string">&quot;&quot;</span>).equals(<span class="string">&quot;start&quot;</span>))&#123;</span><br><span class="line">                timestampStr = jsonObject.getJSONObject(<span class="string">&quot;app_active&quot;</span>).getString(<span class="string">&quot;time&quot;</span>);</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(headersMap.getOrDefault(<span class="string">&quot;logtype&quot;</span>,<span class="string">&quot;&quot;</span>).equals(<span class="string">&quot;event&quot;</span>))&#123;</span><br><span class="line">                <span class="type">JSONArray</span> <span class="variable">jsonArray</span> <span class="operator">=</span> jsonObject.getJSONArray(<span class="string">&quot;lagou_event&quot;</span>);</span><br><span class="line">                <span class="keyword">if</span>(jsonArray.size() &gt; <span class="number">0</span>)&#123;</span><br><span class="line">                    timestampStr = jsonArray.getJSONObject(<span class="number">0</span>).getString(<span class="string">&quot;time&quot;</span>);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 转换时间格式</span></span><br><span class="line">            <span class="type">long</span> <span class="variable">timestamp</span> <span class="operator">=</span> Long.parseLong(timestampStr);</span><br><span class="line">            <span class="type">DateTimeFormatter</span> <span class="variable">dateTimeFormatter</span> <span class="operator">=</span> DateTimeFormatter.ofPattern(<span class="string">&quot;yyyy-MM-dd&quot;</span>);</span><br><span class="line"></span><br><span class="line">            <span class="type">Instant</span> <span class="variable">instant</span> <span class="operator">=</span> Instant.ofEpochMilli(timestamp);</span><br><span class="line">            <span class="type">LocalDateTime</span> <span class="variable">localDateTime</span> <span class="operator">=</span> LocalDateTime.ofInstant(instant, ZoneId.systemDefault());</span><br><span class="line">            <span class="type">String</span> <span class="variable">date</span> <span class="operator">=</span> dateTimeFormatter.format(localDateTime);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            headersMap.put(<span class="string">&quot;logtime&quot;</span>, date);</span><br><span class="line">            event.setHeaders(headersMap);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            headersMap.put(<span class="string">&quot;logtime&quot;</span>, <span class="string">&quot;Unknown&quot;</span>);</span><br><span class="line">            event.setHeaders(headersMap);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> List&lt;Event&gt; <span class="title function_">intercept</span><span class="params">(List&lt;Event&gt; list)</span> &#123;</span><br><span class="line">        List&lt;Event&gt; eventList = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (Event event : list) &#123;</span><br><span class="line">            <span class="comment">// 单条转化</span></span><br><span class="line">            <span class="type">Event</span> <span class="variable">outEvent</span> <span class="operator">=</span> intercept(event);</span><br><span class="line">            <span class="keyword">if</span>(event != <span class="literal">null</span>) &#123;</span><br><span class="line">                <span class="comment">// j</span></span><br><span class="line">                eventList.add(outEvent);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 返回新的event数组</span></span><br><span class="line">        <span class="keyword">return</span> eventList;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Builder</span> <span class="keyword">implements</span> <span class="title class_">Interceptor</span>.Builder&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Interceptor <span class="title function_">build</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">LogTypeInterceptor</span>();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>






      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/flume/flume%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8B%A6%E6%88%AA%E5%99%A8/" data-id="clumsd6x7000aabr7hiyg6t00" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-flink/Flink入门使用(scala版)" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/flink/Flink%E5%85%A5%E9%97%A8%E4%BD%BF%E7%94%A8(scala%E7%89%88)/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.864Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Flink入门使用-scala版"><a href="#Flink入门使用-scala版" class="headerlink" title="Flink入门使用(scala版)"></a>Flink入门使用(scala版)</h1><h1 id="快速入门"><a href="#快速入门" class="headerlink" title="快速入门"></a>快速入门</h1><p>版本说明：</p>
<p>Flink 1.13.0</p>
<p>scala 2.12.12</p>
<p>使用maven管理依赖配置如下</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">scala.binary.version</span>&gt;</span>2.12<span class="tag">&lt;/<span class="name">scala.binary.version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">flink.version</span>&gt;</span>1.13.0<span class="tag">&lt;/<span class="name">flink.version</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br></pre></td></tr></table></figure>



<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- flink 流处理的Scala版本 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 本地测试运行提交需要的客户端依赖，</span></span><br><span class="line"><span class="comment">早期版本的flink中将client是合并在flink-stream依赖中(1.11版本之前)，后续新版本才独立出来--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-clients_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>



<ol>
<li><p>数据批处理</p>
<p>wordcount数据批处理样例</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 隐式类型转换，需要导入 createTypeInformation 类</span></span><br><span class="line"><span class="comment">//import org.apache.flink.api.scala.&#123;DataSet, ExecutionEnvironment, createTypeInformation&#125;</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用的API返回结果类型为DataSet</span></span><br><span class="line"><span class="comment"> * DataSet 底层数据结构,</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Flink本身是流批一体的处理框架，官方推荐使用DataStreamAPI，在提交任务时再指定处理模式为BATCH</span></span><br><span class="line"><span class="comment"> * &gt; bin/flin run -Dexecution.runtime-mode=BATCH BatchWordCount.jar</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountBatchDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">ExecutionEnvironment</span> = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">// 1. 读取文件</span></span><br><span class="line">    <span class="keyword">val</span> lineData: <span class="type">DataSet</span>[<span class="type">String</span>] = env.readTextFile(<span class="string">&quot;input/word.txt&quot;</span>)</span><br><span class="line">    <span class="comment">// 2. 处理数据， 转换成单词与计数对</span></span><br><span class="line">    <span class="keyword">val</span> wordOne: <span class="type">DataSet</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = lineData.flatMap(_.split(&#x27; &#x27;)).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">// 3. 分组聚合</span></span><br><span class="line">    <span class="keyword">val</span> groupWordCount: <span class="type">GroupedDataSet</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordOne.groupBy(<span class="number">0</span>)</span><br><span class="line">    <span class="comment">// 4. 指定组内字段求和</span></span><br><span class="line">    <span class="keyword">val</span> result: <span class="type">AggregateDataSet</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = groupWordCount.sum(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 5. 输出结果</span></span><br><span class="line">    result.print()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


</li>
<li><p>流批处理</p>
<p>在Flink中，流数据处理才是整个处理逻辑的核心，流批处理统一之后的DataStream API更加强大，可以直接进行批处理和流处理。</p>
</li>
</ol>
<p>有界数据流处理</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用DataStream api来处理流数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountBoundedStreamDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">// 1. 读取文件</span></span><br><span class="line">    <span class="keyword">val</span> lineData = env.readTextFile(<span class="string">&quot;input/word.txt&quot;</span>)</span><br><span class="line">    <span class="comment">// 2. 处理数据， 转换成单词与计数对</span></span><br><span class="line">    <span class="keyword">val</span> wordOne = lineData.flatMap(_.split(&#x27; &#x27;)).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">// 3. 分组聚合</span></span><br><span class="line"><span class="comment">//    val groupWordCount = wordOne.keyBy(0)</span></span><br><span class="line"><span class="comment">//    wordOne.keyBy(data =&gt; data._1)</span></span><br><span class="line"><span class="keyword">val</span> groupWordCount = wordOne.keyBy(_._1)</span><br><span class="line">    <span class="comment">// 4. 指定组内字段求和</span></span><br><span class="line">    <span class="keyword">val</span> result = groupWordCount.sum(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 5. 输出结果到控制台</span></span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 启动流处理任务</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出结果</p>
<img src="images/image-20230606101344701.png" alt="image-20230606101344701" style="zoom:50%;margin-left:10px" />





<p>无界数据流</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountUnBoundedStreamDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> senv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="comment">// 1. 读取文件</span></span><br><span class="line">    <span class="comment">// 从socket指定的端口持续接收数据</span></span><br><span class="line">    <span class="keyword">val</span> lineData = senv.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="comment">// 2. 处理数据</span></span><br><span class="line">    <span class="keyword">val</span> wordOne = lineData.flatMap(_.split(&#x27; &#x27;)).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">// 3. 分组聚合</span></span><br><span class="line">    <span class="comment">//    val groupWordCount = wordOne.keyBy(0)</span></span><br><span class="line">    <span class="comment">//    wordOne.keyBy(data =&gt; data._1)</span></span><br><span class="line">    <span class="keyword">val</span> groupWordCount = wordOne.keyBy(_._1)</span><br><span class="line">    <span class="comment">// 4. 指定组内字段求和</span></span><br><span class="line">    <span class="keyword">val</span> result = groupWordCount.sum(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">// 5. 输出结果到控制台</span></span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 启动流处理任务</span></span><br><span class="line">    senv.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试读取socket来源的数据</p>
<p>使用nc模拟一个网络服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc -lk 9999</span><br></pre></td></tr></table></figure>

<p><img src="/images/image-20230606102523456.png" alt="image-20230606102523456"></p>
<p>启动flink程序后可以看到读取了所监听的端口数据</p>
<p><img src="/images/image-20230606102621268.png" alt="image-20230606102621268"></p>
<p>本地编写的flink代码在执行时，先在本地模拟一个flink集群，然后将作业提交到集群上，创建好要执行的任务，等待数据输入。</p>
<img src="images/image-20230606103040393.png" alt="image-20230606103040393" style="zoom:50%;margin-left:10px" />







<h1 id="Flink系统架构"><a href="#Flink系统架构" class="headerlink" title="Flink系统架构"></a>Flink系统架构</h1><h2 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h2><img src="images/image-20230606104348488.png" alt="image-20230606104348488" style="zoom:50%;" />

<p>JobManager</p>
<ul>
<li>JobMaster<br>    负责处理单独的Job, 一个JobMaster对应处理一个Job<br>    接收要执行的作业， 包括Jar文件，数据流图(Dataflow Graph)，作业图(Job Graph)<br>    将作业图转换成物理层面的执行图(ExecutionGraph), 包含了所有可并发执行的任务，JobMaster会向资源管理器申请资源来执行作业，一旦获取到足够的执行资源，就会将执行图发送到真正运行他们的TaskManager上<br>    JobMaster 会负责需要协调的操作，比如说检查点(checkpoints)的协调</li>
<li>ResourceManager<br>    资源分配与调度<br>    任务槽task slot分配</li>
<li>Dispatcher<br>    Rest接口，用来提交作业，为每一个提交的应用启动一个JobMaster<br>    启动WebUI，方便展示和监控作业的执行情况</li>
</ul>
<p>TaskManager<br>    Flink中的工作进程, taskmanager中包含一定数量的task slot,负责数据流的具体执行任务；<br>    taskmanager会向资源管理器注册它的slot， 当JobMaster要执行一个作业时，会向资源管理器申请task solt， 此时资源管理器就会将已注册好的slot分配给JobMaster， 之后JobMaster就会将任务分配到这些slot上执行；<br>    同一应用下使用的taskmanager之间可以交换数据。</p>
<h2 id="重要概念"><a href="#重要概念" class="headerlink" title="重要概念"></a>重要概念</h2><h3 id="数据流图"><a href="#数据流图" class="headerlink" title="数据流图"></a>数据流图</h3><p>​		Flink 是流式计算框架。它的程序结构，其实就是定义了一连串的处理操作，每一个数据 输入之后都会依次调用每一步计算。在 Flink代码中，我们定义的每一个处理转换操作都叫作“算子”(Operator)。</p>
<p>​		所有的 Flink 程序都可以归纳为由三部分构成:Source、Transformation 和 Sink。</p>
<ul>
<li><p>Source表示“源算子”，负责读取数据源。</p>
</li>
<li><p>Transformation表示“转换算子”，利用各种算子进行处理加工。</p>
</li>
<li><p>Sink表示“下沉算子”，负责数据的输出。<br> 在运行时，Flink 程序会被映射成所有算子按照逻辑顺序连接在一起的一张图，这被称为“逻辑数据流”(logical dataflow)，或者叫“数据流图”(dataflow graph)。在数据流图中，可以 清楚地看到 Source、Transformation、Sink 三部分。</p>
</li>
</ul>
<p><img src="/images/image-20230606161758558.png" alt="image-20230606161758558"></p>
<h3 id="并行子任务和并行度"><a href="#并行子任务和并行度" class="headerlink" title="并行子任务和并行度"></a>并行子任务和并行度</h3><p>​		把一个算子操作，“复制”多份到多个节点，数据来了之后就可以到其中任意一个执行。 这样一来，一个算子操作就被拆分成了多个并行的“子任务”(subtasks)，再将它们分发到不 同节点，就真正实现了并行计算。</p>
<p>​		在 Flink 执行过程中，每一个算子(operator)可以包含一个或多个子任务(operator subtask)， 这些子任务在<strong>不同的线程</strong>、<strong>不同的物理机</strong>或<strong>不同的容器</strong>中完全独立地执行。</p>
<p><img src="/images/image-20230606162042754.png" alt="image-20230606162042754"></p>
<p>​	一个特定算子的子任务(subtask)的个数被称之为其并行度(parallelism)。这样，包含并 行子任务的数据流，就是并行数据流，它需要多个分区(stream partition)来分配并行任务。 一般情况下，一个流程序的并行度，可以认为就是其所有算子中最大的并行度。一个程序中， 不同的算子可能具有不同的并行度。</p>
<p>​		如图所示，当前数据流中有 Source、map()、keyBy()&#x2F;window()&#x2F;apply()、Sink 四个算子， 除最后 Sink，其他算子的并行度都为 2。整个程序包含了 7 个子任务，至少需要 2 个分区来并 行执行。我们可以说，这段流处理程序的并行度就是 2。</p>
<p>​		<strong>并行度设置</strong></p>
<ol>
<li>代码中设置</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 执行环境中设置</span></span><br><span class="line">env.setParallelism(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 算子后面设置</span></span><br><span class="line">dataStream.map((_,<span class="number">1</span>)).setParallelism(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>



<ol start="2">
<li>提交作业时设置</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flink run –p 2 –c com.atguigu.wc.StreamWordCount ./FlinkTutorial-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>

<p>​		通过提交作业时指定参数-p来设置作业执行时的并行度(推荐方式)</p>
<ol start="3">
<li>配置文件中设置</li>
</ol>
<p>​		在集群配置文件flink-conf.yaml中直接更改默认的并行度</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">parallelism.default=2</span></span><br></pre></td></tr></table></figure>





<h3 id="算子链"><a href="#算子链" class="headerlink" title="算子链"></a>算子链</h3><p>​		先来了解下算子间数据传输的形式</p>
<p>​		(1) 一对一(One-to-one，forwarding) 这种模式下，数据流维护着分区以及元素的顺序。比如图中的 Source 和 map()算子，Source算子读取数据之后，可以直接发送给 map()算子做处理，它们之间不需要重新分区，也不需要 调整数据的顺序。这就意味着 map() 算子的子任务，看到的元素个数和顺序跟 Source 算子的 子任务产生的完全一样，保证着“一对一”的关系。map()、filter()、flatMap()等算子都是这种 one-to-one 的对应关系。</p>
<p>​		(2) 重分区(Redistributing) 在这种模式下，数据流的分区会发生改变。如图中的 map()和后面的keyBy()&#x2F;window()&#x2F;apply()算子之间(这里的 keyBy()是数据传输方法，后面的 window()、apply() 方法共同构成了窗口算子)，以及窗口算子和 Sink 算子之间，都是这样的关系。</p>
<p>​		每一个算子的子任务，会根据数据传输的策略，把数据发送到不同的下游目标任务。例如， keyBy()是分组操作，本质上基于键(key)的哈希值(hashCode)进行了重分区;而当并行度改变时，比如从并行度为 2 的 window 算子，要传递到并行度为 1 的 Sink 算子，这时的数据 传输方式是再平衡(rebalance)，会把数据均匀地向下游子任务分发出去。这些传输方式都会引起重分区(redistribute)的过程，这一过程类似于 Spark 中的 shuffle。</p>
<p>​		在 Flink 中，并行度相同的一对一(one to one)算子操作，可以直接链接在一起形成一个 “大”的任务(task)，这样原来的算子就成为真正任务里的一部分，如图 4-11 所示。每个 task会被一个线程执行。这样的技术被称为“算子链”(Operator Chain)。</p>
<p><img src="/images/image-20230606162708853.png" alt="image-20230606162708853"></p>
<p>​		Flink 默认会按照算子链的原则进行链接合并，如果我们想要禁止合并或者自行定义，也 可以在代码中对算子做一些特定的设置:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 禁用算子链</span></span><br><span class="line">.map((_,<span class="number">1</span>)).disableChaining() </span><br><span class="line"><span class="comment">// 从当前算子开始新链 </span></span><br><span class="line">.map((_,<span class="number">1</span>)).startNewChain()</span><br></pre></td></tr></table></figure>







<h3 id="作业图与执行图"><a href="#作业图与执行图" class="headerlink" title="作业图与执行图"></a>作业图与执行图</h3><p>​		由 Flink 程序直接映射成的数据流图(dataflow graph)，也被称为逻辑流图(logical StreamGraph)。到具体执行环节时，Flink 需要进一步将逻辑流图进行解析，转换为物理执行图。</p>
<p>​		在这个转换过程中，有几个不同的阶段，会生成不同层级的图，其中最重要的就是作业图 (JobGraph)和执行图(ExecutionGraph)。Flink 中任务调度执行的图，按照生成顺序可以分成</p>
<p>四层:<br>        逻辑流图(StreamGraph)→ 作业图(JobGraph)→ 执行图(ExecutionGraph)→ 物理图(Physical Graph)。</p>
<p><strong>逻辑流图(StreamGraph)</strong></p>
<p>​		这是根据用户通过 DataStream API 编写的代码生成的最初的 DAG 图，用来表示程序的拓 扑结构。这一步一般在客户端完成。</p>
<p><strong>作业图(JobGraph)</strong></p>
<p>​		StreamGraph 经过优化后生成的就是作业图(JobGraph)，这是提交给 JobManager 的数据结构，确定了当前作业中所有任务的划分。主要的优化为: <u><strong>将多个符合条件的节点链接在一起合并成一个任务节点，形成算子链</strong></u>，这样可以减少数据交换的消耗。JobGraph 一般也是在客 户端生成的，在作业提交时传递给 JobMaster。</p>
<p><strong>执行图(ExecutionGraph)</strong></p>
<p>​		JobMaster 收到 JobGraph 后，会根据它来生成执行图(ExecutionGraph)。ExecutionGraph是 JobGraph 的并行化版本，是调度层最核心的数据结构。</p>
<p><strong>物理图(Physical Graph)</strong></p>
<p>​		JobMaster 生成执行图后， 会将它分发给 TaskManager;各个 TaskManager 会根据执行图 部署任务，最终的物理执行过程也会形成一张“图”，一般就叫作物理图(Physical Graph)。 这只是具体执行层面的图，并不是一个具体的数据结构。</p>
<h3 id="任务-task-与任务槽-task-slot"><a href="#任务-task-与任务槽-task-slot" class="headerlink" title="任务(task)与任务槽(task slot)"></a>任务(task)与任务槽(task slot)</h3><p><img src="/images/image-20230606160142030.png" alt="image-20230606160142030"></p>
<ol>
<li><strong>任务槽(Task Slots)</strong></li>
</ol>
<p>​		Flink 中每一个 worker(也就是 TaskManager)都是一个 JVM 进程，它可以启动多个独立的 线程，来并行执行多个子任务(subtask)。为了控制并发量，我们需要在 TaskManager 上对每个任务运行所占用的资源做出明确的划 分，这就是所谓的任务槽(task slots)。每个任务槽(task slot)其实表示了TaskManager拥有计算资源的一个固定大小的子集。 这些资源就是用来独立执行一个子任务的。</p>
<p>​		</p>
<ol start="2">
<li><strong>任务槽设置</strong></li>
</ol>
<p>我们可以通过集群的配置文件来设定 TaskManager 的 slots 数量:</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">taskmanager.numberOfTaskSlots:</span> <span class="number">8</span></span><br></pre></td></tr></table></figure>

<p>通过调整 slots 的数量，我们就可以控制子任务之间的隔离级别。 需要注意的是，slots 目前仅仅用来隔离内存，不会涉及 CPU 的隔离，因此可以按照cpu核心数来配置slot1.</p>
<ol start="3">
<li><strong>任务共享slot</strong></li>
</ol>
<p>​		默认情况下，Flink 允许子任务共享 slots。如图所示，只要属于同一个作业，那么对于不同任务节点的并行子任务，就可以放到同一个 slot 上执行。如果希望某个算子对应的任务完全独占一个 slot，或者只有某一部分算子共享 slots，我们也可以通过设置“slot 共享组”(SlotSharingGroup)手动指定: </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.map((_,<span class="number">1</span>)).slotSharingGroup(“<span class="number">1</span>”);</span><br></pre></td></tr></table></figure>

<p>这样，只有属于同一个 slot 共享组的子任务，才会开启 slots 共享;不同组之间的任务是 完全隔离的，必须分配到不同的 slots 上。</p>
<h1 id="Datastream-API"><a href="#Datastream-API" class="headerlink" title="Datastream API"></a>Datastream API</h1><h2 id="Source数据源算子"><a href="#Source数据源算子" class="headerlink" title="Source数据源算子"></a>Source数据源算子</h2><h2 id="Transform转换算子"><a href="#Transform转换算子" class="headerlink" title="Transform转换算子"></a>Transform转换算子</h2><h2 id="Sink输出算子"><a href="#Sink输出算子" class="headerlink" title="Sink输出算子"></a>Sink输出算子</h2><h1 id="Table-API和SQL"><a href="#Table-API和SQL" class="headerlink" title="Table API和SQL"></a>Table API和SQL</h1><p>​		Flink 同样提供了对于“表”处理的支持，这就是更高层级的应用 API，在 Flink 中被称为 Table API 和 SQL。Table API 顾名思义，就是基于“表”(Table)的一套 API，它是内嵌在 Java、 Scala 等语言中的一种声明式领域特定语言(DSL)，也就是专门为处理表而设计的;在此基础 上，Flink 还基于 Apache Calcite 实现了对 SQL 的支持。这样一来，我们就可以在 Flink 程序中 直接写 SQL 来实现处理需求了。</p>
<p><img src="/images/image-20230606164424300.png" alt="image-20230606164424300"></p>
<h2 id="快速上手"><a href="#快速上手" class="headerlink" title="快速上手"></a>快速上手</h2><p>添加table api的依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--table api 依赖--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-api-scala-bridge_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​		这里的依赖是一个 Scala 的“桥接器”(bridge)，主要就是负责 Table API 和下层 DataStream API 的连接支持，按照不同的语言分为 Java 版和 Scala 版。</p>
<p>​		如果我们希望在本地的*集成开发环境(IDE)*里运行Table API和SQL，还需要引入以下 依赖:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-planner-blink_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>​		这里主要添加的依赖是一个“计划器”(planner)，它是 Table API 的核心组件，<strong>负责提供运行时环境</strong>，并生成程序的执行计划。这里我们用到的是新版的blink planner。<u>由于Flink安 装包的 lib 目录下会自带 planner，所以在生产集群环境中提交的作业不需要打包这个依赖。</u></p>
<p>​		另外，如果想实现自定义的数据格式来做序列化，可以引入下面的依赖:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-table-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>



<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.xujia.common.<span class="type">Event</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="comment">// table api中使用的表达式包expression</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">Expressions</span>.$</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">Table</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.bridge.scala._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SimpleTableDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> eventStream: <span class="type">DataStream</span>[<span class="type">Event</span>] = env.fromElements(</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./cart&quot;</span>, <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=1&quot;</span>, <span class="number">5</span> * <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Cary&quot;</span>, <span class="string">&quot;./home&quot;</span>, <span class="number">60</span> * <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;./prod?id=3&quot;</span>, <span class="number">90</span> * <span class="number">1000</span>L),</span><br><span class="line">      <span class="type">Event</span>(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;./prod?id=7&quot;</span>, <span class="number">105</span> * <span class="number">1000</span>L)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取表环境</span></span><br><span class="line">    <span class="keyword">val</span> tableEnv: <span class="type">StreamTableEnvironment</span> = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 数据流转换成表</span></span><br><span class="line">    <span class="keyword">val</span> table: <span class="type">Table</span> = tableEnv.fromDataStream(eventStream)</span><br><span class="line">    <span class="comment">// 用执行sql的方式提取数据</span></span><br><span class="line">    <span class="keyword">val</span> resultSqlTable: <span class="type">Table</span> = tableEnv.sqlQuery(<span class="string">&quot;select url, user from &quot;</span> + table + <span class="string">&quot; where user=&#x27;Bob&#x27;&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行table api的方式来提取数据</span></span><br><span class="line">    <span class="keyword">val</span> resultTable = table.select($(<span class="string">&quot;user&quot;</span>), $(<span class="string">&quot;url&quot;</span>)).where($(<span class="string">&quot;user&quot;</span>).isEqual(<span class="string">&quot;Alice&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将表转换成数据流</span></span><br><span class="line">    tableEnv.toDataStream(resultSqlTable).print(<span class="string">&quot;sql&quot;</span>)</span><br><span class="line">    tableEnv.toDataStream(resultTable).print(<span class="string">&quot;table api&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 执行程序</span></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>Table api</p>
<ol>
<li>程序框架</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建表环境</span></span><br><span class="line"><span class="keyword">val</span> tableEnv = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建输入表，连接外部系统读取数据</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE TEMPORARY TABLE inputTable ... WITH ( &#x27;connector&#x27; = ... )&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个表，连接到外部系统，用于输出</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE TEMPORARY TABLE outputTable ... WITH ( &#x27;connector&#x27; = ... )&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行 SQL 对表进行查询转换，得到一个新的表</span></span><br><span class="line"><span class="keyword">val</span> table1 = tableEnv.sqlQuery(<span class="string">&quot;SELECT ... FROM inputTable... &quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用 Table API 对表进行查询转换，得到一个新的表</span></span><br><span class="line"><span class="keyword">val</span> table2 = tableEnv.from(<span class="string">&quot;inputTable&quot;</span>).select(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将得到的结果写入输出表</span></span><br><span class="line"><span class="keyword">val</span> tableResult = table1.executeInsert(<span class="string">&quot;outputTable&quot;</span>)</span><br></pre></td></tr></table></figure>





<ol start="2">
<li>创建表环境</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传统获取table environment方式</span></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="keyword">val</span> tableEnv1 = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建 EnvironmentSettings对象的生成器builder</span></span><br><span class="line"><span class="keyword">val</span> builder = <span class="type">EnvironmentSettings</span>.newInstance()</span><br><span class="line"><span class="comment">// 配置 EnvironmentSettings 对象</span></span><br><span class="line">builder.inStreamingMode()</span><br><span class="line">  .useBlinkPlanner()</span><br><span class="line"><span class="comment">// 构造器方法创建EnvironmentSettings对象</span></span><br><span class="line"><span class="keyword">val</span> settings = builder.build()</span><br><span class="line"><span class="comment">// 传入 EnvironmentSettings对象</span></span><br><span class="line"><span class="keyword">val</span> tableEnv2 = <span class="type">TableEnvironment</span>.create(settings)</span><br></pre></td></tr></table></figure>



<ol start="3">
<li>创建表</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 通过DDL语句方式就可以创建一张表</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE TABLE eventTable (&quot;</span> +</span><br><span class="line">  <span class="string">&quot;uid STRING,&quot;</span> +</span><br><span class="line">  <span class="string">&quot;url STRING,&quot;</span> +</span><br><span class="line">  <span class="string">&quot;ts BIGINT&quot;</span> +</span><br><span class="line">  <span class="string">&quot;) WITH (&quot;</span> +</span><br><span class="line">  <span class="string">&quot; &#x27;connector&#x27; = &#x27;filesystem&#x27;,&quot;</span> +</span><br><span class="line">  <span class="string">&quot; &#x27;path&#x27; = &#x27;input/clicks.txt&#x27;,&quot;</span> +</span><br><span class="line">  <span class="string">&quot; &#x27;format&#x27; = &#x27;csv&#x27; &quot;</span> +</span><br><span class="line">  <span class="string">&quot;)&quot;</span>)</span><br></pre></td></tr></table></figure>



<ol start="4">
<li>查询数据转换</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 表查询转换</span></span><br><span class="line"><span class="comment">// sql方式</span></span><br><span class="line"><span class="keyword">val</span> resultSql = tableEnv.sqlQuery(<span class="string">&quot;select uid, url, ts from eventTable where uid = &#x27;Alice&#x27;&quot;</span>)</span><br><span class="line"><span class="comment">// 注册查询结果为临时表，方便后续调用</span></span><br><span class="line">tableEnv.createTemporaryView(<span class="string">&quot;tempTable&quot;</span>, resultSql)</span><br><span class="line"><span class="keyword">val</span> urlCountSql = tableEnv.sqlQuery(<span class="string">&quot;select uid, count(url) from eventTable group by uid&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// table 方式</span></span><br><span class="line"><span class="keyword">val</span> resultTable = tableEnv.from(<span class="string">&quot;eventTable&quot;</span>).select($(<span class="string">&quot;uid&quot;</span>), $(<span class="string">&quot;url&quot;</span>), $(<span class="string">&quot;ts&quot;</span>)).where($(<span class="string">&quot;uid&quot;</span>).isEqual(<span class="string">&quot;Bob&quot;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<ol start="5">
<li>输出结果表</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输出数据的结果表</span></span><br><span class="line">tableEnv.executeSql(<span class="string">&quot;CREATE TABLE outputTable (&quot;</span> +</span><br><span class="line">  <span class="string">&quot;user_name STRING,&quot;</span> +</span><br><span class="line">  <span class="string">&quot;url STRING,&quot;</span> +</span><br><span class="line">  <span class="string">&quot;timestamps BIGINT&quot;</span> +</span><br><span class="line">  <span class="string">&quot;) WITH (&quot;</span> +</span><br><span class="line">  <span class="string">&quot; &#x27;connector&#x27; = &#x27;filesystem&#x27;,&quot;</span> +</span><br><span class="line">  <span class="string">&quot; &#x27;path&#x27; = &#x27;output&#x27;,&quot;</span> +</span><br><span class="line">  <span class="string">&quot; &#x27;format&#x27; = &#x27;csv&#x27; &quot;</span> +</span><br><span class="line">  <span class="string">&quot;)&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行任务</span></span><br><span class="line">resultSql.executeInsert(<span class="string">&quot;outputTable&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>​		在底层，表的输出是通过将数据写入到 TableSink 来实现的。TableSink 是 Table API 中提 供的一个向外部系统写入数据的通用接口，可以支持不同的文件格式(比如 CSV、Parquet)、 存储数据库(比如 JDBC、HBase、Elasticsearch)和消息队列(比如 Kafka)。它有些类似于 DataStream API 中调用 addSink()方法时传入的 SinkFunction，有不同的连接器对它进行了实现</p>
<ol start="6">
<li>表和流数据转换</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 表转换成一般数据流</span></span><br><span class="line">tableEnv.toDataStream(resultSql).print(<span class="string">&quot;append&quot;</span>)</span><br><span class="line"><span class="comment">// 转换成更改日志流</span></span><br><span class="line">tableEnv.toChangelogStream(urlCountSql).print(<span class="string">&quot;change stream&quot;</span>)</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/flink/Flink%E5%85%A5%E9%97%A8%E4%BD%BF%E7%94%A8(scala%E7%89%88)/" data-id="clumsd6x50006abr76ry592q2" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-flink/flink笔记" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/flink/flink%E7%AC%94%E8%AE%B0/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.855Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="角色架构"><a href="#角色架构" class="headerlink" title="角色架构"></a>角色架构</h1><p>Jobmanager</p>
<p>Resourcemanager</p>
<p>TaskManager</p>
<p>Dispatcher</p>
<p><img src="/flink%E7%AC%94%E8%AE%B0.assets/image-20211130160920967.png" alt="image-20211130160920967"></p>
<h1 id="流处理API"><a href="#流处理API" class="headerlink" title="流处理API"></a>流处理API</h1><p><img src="/flink%E7%AC%94%E8%AE%B0.assets/image-20211106234954325.png" alt="image-20211106234954325"></p>
<h2 id="1-Environment"><a href="#1-Environment" class="headerlink" title="1. Environment"></a>1. Environment</h2><h3 id="1-1-getExecutionEnvirionment"><a href="#1-1-getExecutionEnvirionment" class="headerlink" title="1.1 getExecutionEnvirionment"></a>1.1 getExecutionEnvirionment</h3><p>​		创建执行环境，表示当前执行成勋的上下文。如果程序独立调用，则此方法返回本地执行环境；如果从命令行客户端调用程序以提交到集群，则此方法返回集群的执行环境，也就是说getExecutionEnvironment方法会根据查询运行的方式决定返回什么样的执行环境，这是最常用的创建执行环境的方式。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">streamEnv</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br></pre></td></tr></table></figure>



<h3 id="1-2-createLocalEnvironment"><a href="#1-2-createLocalEnvironment" class="headerlink" title="1.2 createLocalEnvironment"></a>1.2 createLocalEnvironment</h3><p>​		返回本地的执行环境，可以在调用是指定默认的并行度。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">LocalStreamEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.createLocalEnvironment(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>



<h3 id="1-2-createRemoteEnvironment"><a href="#1-2-createRemoteEnvironment" class="headerlink" title="1.2 createRemoteEnvironment"></a>1.2 createRemoteEnvironment</h3><p>​		返回集群执行环境，将jar提交到远程服务器，需要在调用时指定JobManager的IP和端口，并指定要在集群中运行的jar文件。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.createRemoteEnvironment(<span class="string">&quot;linux111&quot;</span>,<span class="number">6123</span>,<span class="string">&quot;jarfile.path&quot;</span>);</span><br></pre></td></tr></table></figure>



<h2 id="2-Source"><a href="#2-Source" class="headerlink" title="2. Source"></a>2. Source</h2><h3 id="2-1-从集合中读取数据"><a href="#2-1-从集合中读取数据" class="headerlink" title="2.1 从集合中读取数据"></a>2.1 从集合中读取数据</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SourceTest1_Collection</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 创建执行环境</span></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">// 设置并行度(可选)</span></span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从集合中获取数据</span></span><br><span class="line">        DataStreamSource&lt;SensorReading&gt; sensorDataStream = env.fromCollection(Arrays.asList(</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">SensorReading</span>(<span class="string">&quot;sensor_1&quot;</span>, <span class="keyword">new</span> <span class="title class_">Date</span>().getTime(), <span class="number">35.8</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">SensorReading</span>(<span class="string">&quot;sensor_4&quot;</span>, <span class="keyword">new</span> <span class="title class_">Date</span>().getTime(), <span class="number">34.2</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">SensorReading</span>(<span class="string">&quot;sensor_3&quot;</span>, <span class="keyword">new</span> <span class="title class_">Date</span>().getTime(), <span class="number">37.4</span>),</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">SensorReading</span>(<span class="string">&quot;sensor_8&quot;</span>, <span class="keyword">new</span> <span class="title class_">Date</span>().getTime(), <span class="number">30.9</span>)</span><br><span class="line">        ));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 元素集合中获取元素</span></span><br><span class="line">        DataStreamSource&lt;Integer&gt; integerDataStream = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">67</span>, <span class="number">198</span>);</span><br><span class="line">        <span class="comment">// 打印到控制台</span></span><br><span class="line">        sensorDataStream.print(<span class="string">&quot;data&quot;</span>);</span><br><span class="line">        integerDataStream.print(<span class="string">&quot;int&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 执行</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h3 id="2-2-从文件中读取数据"><a href="#2-2-从文件中读取数据" class="headerlink" title="2.2 从文件中读取数据"></a>2.2 从文件中读取数据</h3><p>​		使用readTextFile方法提供文件路径。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SourceTest2_File</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 创建执行环境</span></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 从文本文件中读取数据</span></span><br><span class="line">        DataStreamSource&lt;String&gt; dataStream = env.readTextFile(<span class="string">&quot;src/main/resources/sensor.txt&quot;</span>);</span><br><span class="line">        <span class="comment">// 打印到控制台</span></span><br><span class="line">        dataStream.print(<span class="string">&quot;data&quot;</span>);</span><br><span class="line">        <span class="comment">// 程序执行</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="2-3-从socket接口获取数据"><a href="#2-3-从socket接口获取数据" class="headerlink" title="2.3 从socket接口获取数据"></a>2.3 从socket接口获取数据</h3><p>​		socketTextStream方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;String&gt; dataStream = env.socketTextStream(<span class="string">&quot;linux111&quot;</span>, <span class="number">7777</span>);</span><br></pre></td></tr></table></figure>



<h3 id="2-4-从kafka中读取数据，作为来源"><a href="#2-4-从kafka中读取数据，作为来源" class="headerlink" title="2.4 从kafka中读取数据，作为来源"></a>2.4 从kafka中读取数据，作为来源</h3><p>​		添加kafka连接器的依赖</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- flink-connector-kafka-0.11_$&#123;scala版本&#125; --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.11.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>





<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SourceTest3_Kafka</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// kafka消费配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        properties.put(<span class="string">&quot;bootstrap.servers&quot;</span>,<span class="string">&quot;linux111:9092&quot;</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;group.id&quot;</span>,<span class="string">&quot;test&quot;</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;key.deserializer&quot;</span>,<span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;value.deserializer&quot;</span>,<span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置kafka connector，消费数据</span></span><br><span class="line">        DataStreamSource&lt;String&gt; dataStream = env.addSource(<span class="keyword">new</span> <span class="title class_">FlinkKafkaConsumer</span>&lt;String&gt;(<span class="string">&quot;test&quot;</span>, <span class="keyword">new</span> <span class="title class_">SimpleStringSchema</span>(), properties));</span><br><span class="line"></span><br><span class="line">        dataStream.print();</span><br><span class="line">        env.execute();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h3 id="2-5-自定义Source"><a href="#2-5-自定义Source" class="headerlink" title="2.5 自定义Source"></a>2.5 自定义Source</h3><p>​		自定义Source有三种可以选择的SourceFunction：</p>
<ul>
<li>(implements) SourceFunction</li>
<li>(implements) ParallelSourceFunction  (并发)</li>
<li>(extends) RichParallelSourceFunction  (并发)</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SourceTest4_UDF</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 创建执行环境</span></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(<span class="number">2</span>);</span><br><span class="line">        <span class="comment">// 添加自定的Source</span></span><br><span class="line"><span class="comment">//        DataStreamSource&lt;SensorReading&gt; dataStream = env.addSource(new MySensorSource());</span></span><br><span class="line">        DataStreamSource&lt;SensorReading&gt; dataStream = env.addSource(<span class="keyword">new</span> <span class="title class_">MySensorParallelSource</span>());</span><br><span class="line"><span class="comment">//        DataStreamSource&lt;SensorReading&gt; dataStream = env.addSource(new MySensorRichSource());</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 打印到控制台</span></span><br><span class="line">        dataStream.print(<span class="string">&quot;data&quot;</span>);</span><br><span class="line">        <span class="comment">// 程序执行</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 实现SourceFunction接口来自定义Source</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MySensorSource</span> <span class="keyword">implements</span> <span class="title class_">SourceFunction</span>&lt;SensorReading&gt; &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置标志，来控制数据的产生</span></span><br><span class="line">        <span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">isRunning</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(SourceContext&lt;SensorReading&gt; ctx)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 随机生成数据</span></span><br><span class="line">            <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">            HashMap&lt;String, Double&gt; sensorTempMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">                sensorTempMap.put(<span class="string">&quot;sensor_&quot;</span> + i, random.nextGaussian());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 数据传递到流式集合中</span></span><br><span class="line">            <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">                <span class="keyword">for</span> (String sensorId : sensorTempMap.keySet()) &#123;</span><br><span class="line">                    <span class="comment">// 制造随机波动</span></span><br><span class="line">                    <span class="type">Double</span> <span class="variable">newTemp</span> <span class="operator">=</span> sensorTempMap.get(sensorId) + random.nextGaussian();</span><br><span class="line">                    sensorTempMap.put(sensorId, newTemp);</span><br><span class="line">                    <span class="comment">// 生成数据,放入到上下文中</span></span><br><span class="line">                    ctx.collect(<span class="keyword">new</span> <span class="title class_">SensorReading</span>(sensorId, System.currentTimeMillis(), newTemp));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">cancel</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (isRunning) &#123;</span><br><span class="line">                isRunning = <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 实现ParallelSourceFunction接口来自定义Source</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MySensorParallelSource</span> <span class="keyword">implements</span> <span class="title class_">ParallelSourceFunction</span>&lt;SensorReading&gt;&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置标志，来控制数据的产生</span></span><br><span class="line">        <span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">isRunning</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(SourceContext&lt;SensorReading&gt; ctx)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">            <span class="comment">// 随机生成数据</span></span><br><span class="line">            <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">            HashMap&lt;String, Double&gt; sensorTempMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">                sensorTempMap.put(<span class="string">&quot;sensor_&quot;</span> + i, random.nextGaussian());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 数据传递到流式集合中</span></span><br><span class="line">            <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">                <span class="keyword">for</span> (String sensorId : sensorTempMap.keySet()) &#123;</span><br><span class="line">                    <span class="comment">// 制造随机波动</span></span><br><span class="line">                    <span class="type">Double</span> <span class="variable">newTemp</span> <span class="operator">=</span> sensorTempMap.get(sensorId) + random.nextGaussian();</span><br><span class="line">                    sensorTempMap.put(sensorId, newTemp);</span><br><span class="line">                    <span class="comment">// 生成数据,放入到上下文中</span></span><br><span class="line">                    ctx.collect(<span class="keyword">new</span> <span class="title class_">SensorReading</span>(sensorId, System.currentTimeMillis(), newTemp));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">cancel</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (isRunning) &#123;</span><br><span class="line">                isRunning = <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 继承RichParallelSourceFunction类实现自定义的Source</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MySensorRichSource</span> <span class="keyword">extends</span> <span class="title class_">RichParallelSourceFunction</span>&lt;SensorReading&gt;&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置标志，来控制数据的产生</span></span><br><span class="line">        <span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">isRunning</span> <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">(SourceContext&lt;SensorReading&gt; ctx)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">            <span class="comment">// 随机生成数据</span></span><br><span class="line">            <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">            HashMap&lt;String, Double&gt; sensorTempMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">                sensorTempMap.put(<span class="string">&quot;sensor_&quot;</span> + i, random.nextGaussian());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 数据传递到流式集合中</span></span><br><span class="line">            <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">                <span class="keyword">for</span> (String sensorId : sensorTempMap.keySet()) &#123;</span><br><span class="line">                    <span class="comment">// 制造随机波动</span></span><br><span class="line">                    <span class="type">Double</span> <span class="variable">newTemp</span> <span class="operator">=</span> sensorTempMap.get(sensorId) + random.nextGaussian();</span><br><span class="line">                    sensorTempMap.put(sensorId, newTemp);</span><br><span class="line">                    <span class="comment">// 生成数据,放入到上下文中</span></span><br><span class="line">                    ctx.collect(<span class="keyword">new</span> <span class="title class_">SensorReading</span>(sensorId, System.currentTimeMillis(), newTemp));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">cancel</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (isRunning) &#123;</span><br><span class="line">                isRunning = <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="3-Transform"><a href="#3-Transform" class="headerlink" title="3. Transform"></a>3. Transform</h2><p>转换算子</p>
<h3 id="3-1-map"><a href="#3-1-map" class="headerlink" title="3.1 map"></a>3.1 map</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. map</span></span><br><span class="line">SingleOutputStreamOperator&lt;Integer&gt; mapOutputStream = inputStream.map(<span class="keyword">new</span> <span class="title class_">MapFunction</span>&lt;String, Integer&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Integer <span class="title function_">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 返回字符串的长度</span></span><br><span class="line">        <span class="keyword">return</span> value.length();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>



<h3 id="3-2-flatMap"><a href="#3-2-flatMap" class="headerlink" title="3.2 flatMap"></a>3.2 flatMap</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2.flatmap</span></span><br><span class="line">SingleOutputStreamOperator&lt;String&gt; flatMapOutputStream = inputStream.flatMap(<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;String, String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">flatMap</span><span class="params">(String value, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        String[] fields = value.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (String field : fields) &#123;</span><br><span class="line">            out.collect(field);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>



<h3 id="3-3-filter"><a href="#3-3-filter" class="headerlink" title="3.3 filter"></a>3.3 filter</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 3. filter 过滤</span></span><br><span class="line">SingleOutputStreamOperator&lt;String&gt; filterOutputStream = inputStream.filter(<span class="keyword">new</span> <span class="title class_">FilterFunction</span>&lt;String&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">filter</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">return</span> value.startsWith(<span class="string">&quot;1,&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>





<h3 id="3-4-keyBy"><a href="#3-4-keyBy" class="headerlink" title="3.4 keyBy"></a>3.4 keyBy</h3><h4 id="DataStream-→-KeyedStream"><a href="#DataStream-→-KeyedStream" class="headerlink" title="DataStream → KeyedStream #"></a>DataStream → KeyedStream <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/operators/overview/#datastream-rarr-keyedstream">#</a></h4><p>​		Logically partitions a stream into disjoint partitions. All records with the same key are assigned to the same partition. Internally, <em>keyBy()</em> is implemented with hash partitioning</p>
<p>​		将数据流逻辑上划分到不同的分区中，具有相同key的记录会被分配到同一个分区中，使用的是hash分区算法对记录分区，因此相同hash值的记录会被分配到同一个分区(记录本身可能不同)。</p>
<p>​		<img src="/flink%E7%AC%94%E8%AE%B0.assets/image-20211107182703069.png" alt="image-20211107182703069"></p>
<h3 id="3-5-max与maxBy"><a href="#3-5-max与maxBy" class="headerlink" title="3.5 max与maxBy"></a>3.5 max与maxBy</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TransformTest2_RollingAggregation</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 创建执行环境</span></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从文本文件中读取数据</span></span><br><span class="line">        DataStreamSource&lt;String&gt; dataStream = env.readTextFile(<span class="string">&quot;src/main/resources/sensor.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        SingleOutputStreamOperator&lt;SensorReading&gt; mapedStream = dataStream.map(new MapFunction&lt;String, SensorReading&gt;() &#123;</span></span><br><span class="line"><span class="comment">//            @Override</span></span><br><span class="line"><span class="comment">//            public SensorReading map(String value) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//                String[] fields = value.split(&quot;,&quot;);</span></span><br><span class="line"><span class="comment">//                return new SensorReading(fields[0], Long.valueOf(fields[1]), Double.valueOf(fields[2]));</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//        &#125;);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用lambda表达式简化</span></span><br><span class="line">        SingleOutputStreamOperator&lt;SensorReading&gt; mapedStream = dataStream.map(v -&gt; &#123;</span><br><span class="line">            String[] fields = v.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">SensorReading</span>(fields[<span class="number">0</span>], Long.valueOf(fields[<span class="number">1</span>]), Double.valueOf(fields[<span class="number">2</span>]));</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据stream中sensorReading对象的id字段来分区</span></span><br><span class="line">        KeyedStream&lt;SensorReading, Tuple&gt; keyedStream = mapedStream.keyBy(<span class="string">&quot;id&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// keyBy的另一种方式，</span></span><br><span class="line">        <span class="comment">// SensorReading::getId是jdk8中的方法引用的使用，表示获取对象类型中的方法</span></span><br><span class="line">        <span class="comment">// KeyedStream&lt;SensorReading, String&gt; keyedStream1 = mapedStream.keyBy(SensorReading::getId);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 滚动聚合</span></span><br><span class="line">        <span class="comment">// 仅仅指定key的最大值覆盖对象中原值，结果不是当前的对象</span></span><br><span class="line">        SingleOutputStreamOperator&lt;SensorReading&gt; singleOutputStream = keyedStream.max(<span class="string">&quot;temperature&quot;</span>);</span><br><span class="line">        <span class="comment">// 获取指定key值最大的对象信息</span></span><br><span class="line">        <span class="comment">// SingleOutputStreamOperator&lt;SensorReading&gt; singleOutputStream = keyedStream.maxBy(&quot;temperature&quot;);</span></span><br><span class="line">        <span class="comment">// 多实验体会max与maxBy的区别</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 打印到控制台</span></span><br><span class="line">        singleOutputStream.print(<span class="string">&quot;max&quot;</span>);</span><br><span class="line">        <span class="comment">// 程序执行</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="3-6-reduce"><a href="#3-6-reduce" class="headerlink" title="3.6 reduce"></a>3.6 reduce</h3><h4 id="KeyedStream-→-DataStream"><a href="#KeyedStream-→-DataStream" class="headerlink" title="KeyedStream → DataStream #"></a>KeyedStream → DataStream <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/operators/overview/#keyedstream-rarr-datastream">#</a></h4><p>​			A “rolling” reduce on a keyed data stream. Combines the current element with the last reduced value and emits the new value.</p>
<p>​			将当前记录数据与上一次规约的结果再次规约后输出。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TransformTest3_Reduce</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 创建执行环境</span></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从文本文件中读取数据</span></span><br><span class="line">        DataStreamSource&lt;String&gt; dataStream = env.readTextFile(<span class="string">&quot;src/main/resources/sensor.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用lambda表达式简化</span></span><br><span class="line">        SingleOutputStreamOperator&lt;SensorReading&gt; mapedStream = dataStream.map(v -&gt; &#123;</span><br><span class="line">            String[] fields = v.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">SensorReading</span>(fields[<span class="number">0</span>], Long.valueOf(fields[<span class="number">1</span>]), Double.valueOf(fields[<span class="number">2</span>]));</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据stream中sensorReading对象的id字段来分区</span></span><br><span class="line">        KeyedStream&lt;SensorReading, Tuple&gt; keyedStream = mapedStream.keyBy(<span class="string">&quot;id&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// reduce操作，获取temperature的（历史）最大值，以及当前的时间戳</span></span><br><span class="line"><span class="comment">//        SingleOutputStreamOperator&lt;SensorReading&gt; reduceResult = </span></span><br><span class="line"><span class="comment">//        keyedStream.reduce(new ReduceFunction&lt;SensorReading&gt;() &#123;</span></span><br><span class="line"><span class="comment">//            @Override</span></span><br><span class="line"><span class="comment">//            public SensorReading reduce(SensorReading value1, SensorReading value2) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//                return new SensorReading(</span></span><br><span class="line"><span class="comment">//                        value1.getId(),</span></span><br><span class="line"><span class="comment">//                        value2.getTimestamp(),</span></span><br><span class="line"><span class="comment">//                        Math.max(value1.getTemperature(), value2.getTemperature())</span></span><br><span class="line"><span class="comment">//                );</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//        &#125;);</span></span><br><span class="line">        <span class="comment">// lambda表达式写法</span></span><br><span class="line">        SingleOutputStreamOperator&lt;SensorReading&gt; reduceResult = </span><br><span class="line">            keyedStream.reduce((curState, newData) -&gt;</span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">SensorReading</span>(</span><br><span class="line">                        curState.getId(),</span><br><span class="line">                        newData.getTimestamp(),</span><br><span class="line">                        Math.max(curState.getTemperature(), newData.getTemperature())));</span><br><span class="line">        reduceResult.print();</span><br><span class="line">        <span class="comment">// 程序执行</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="3-7-Split与Select"><a href="#3-7-Split与Select" class="headerlink" title="3.7 Split与Select"></a>3.7 Split与Select</h3><p><img src="/flink%E7%AC%94%E8%AE%B0.assets/image-20211107181237261.png" alt="image-20211107181237261"></p>
<p>**DataStream—-&gt;SplitStream **  根据某些特征将一个DataStream切分成两个或多个DataStream.</p>
<p>​		</p>
<p><img src="/flink%E7%AC%94%E8%AE%B0.assets/image-20211107183043268.png" alt="image-20211107183043268"></p>
<p><strong>SplitStream—-&gt;DataStream</strong> 从一个SplitStream中获取一个或多个DataStream。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TransformTest4_MultipleStream</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 创建执行环境</span></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment().setParallelism(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 从文本文件中读取数据</span></span><br><span class="line">        DataStreamSource&lt;String&gt; dataStream = env.readTextFile(<span class="string">&quot;src/main/resources/sensor.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;SensorReading&gt; sensorReadingStream = dataStream.map(v -&gt; &#123;</span><br><span class="line">            String[] fields = v.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">SensorReading</span>(fields[<span class="number">0</span>], Long.valueOf(fields[<span class="number">1</span>]), Double.valueOf(fields[<span class="number">2</span>]));</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用split标记切分流数据</span></span><br><span class="line">        SplitStream&lt;SensorReading&gt; splitStream = sensorReadingStream.split(<span class="keyword">new</span> <span class="title class_">OutputSelector</span>&lt;SensorReading&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Iterable&lt;String&gt; <span class="title function_">select</span><span class="params">(SensorReading value)</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> value.getTemperature() &gt; <span class="number">35</span> ? Collections.singleton(<span class="string">&quot;high&quot;</span>):Collections.singleton(<span class="string">&quot;low&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用select筛选</span></span><br><span class="line">        DataStream&lt;SensorReading&gt; highStream = splitStream.select(<span class="string">&quot;high&quot;</span>);</span><br><span class="line">        DataStream&lt;SensorReading&gt; lowStream = splitStream.select(<span class="string">&quot;low&quot;</span>);</span><br><span class="line">        DataStream&lt;SensorReading&gt; allStream = splitStream.select(<span class="string">&quot;high&quot;</span>, <span class="string">&quot;low&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 打印到控制台</span></span><br><span class="line">        highStream.print(<span class="string">&quot;high&quot;</span>); <span class="comment">// 打印标记为“high”的数据</span></span><br><span class="line">        lowStream.print(<span class="string">&quot;low&quot;</span>); <span class="comment">// 打印标记为&quot;low&quot;的数据</span></span><br><span class="line">        allStream.print(<span class="string">&quot;all&quot;</span>); <span class="comment">// 打印标记为&quot;high&quot;和&quot;low&quot;的数据</span></span><br><span class="line">        <span class="comment">// 程序执行</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="3-8-Connect和CoMap"><a href="#3-8-Connect和CoMap" class="headerlink" title="3.8 Connect和CoMap"></a>3.8 Connect和CoMap</h3><h4 id="DataStream-DataStream-→-ConnectedStream"><a href="#DataStream-DataStream-→-ConnectedStream" class="headerlink" title="DataStream,DataStream → ConnectedStream #"></a>DataStream,DataStream → ConnectedStream <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/operators/overview/#datastreamdatastream-rarr-connectedstream">#</a></h4><p>“Connects” two data streams retaining their types. Connect allowing for shared state between the two streams.</p>
<p><img src="/flink%E7%AC%94%E8%AE%B0.assets/image-20211107230133974.png" alt="image-20211107230133974"></p>
<p><strong>DataStream,DataStream—-&gt;ConnectedStream</strong> 两个stream被connect，只是把它们放到同一个篮子中，内部依然保持各自的数据和形式上不发生变化，两个流相互独立。connect操作中的两条流的状态可以共享。</p>
<h4 id="ConnectedStream-→-DataStream"><a href="#ConnectedStream-→-DataStream" class="headerlink" title="ConnectedStream → DataStream #"></a>ConnectedStream → DataStream <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/operators/overview/#connectedstream-rarr-datastream">#</a></h4><p>Similar to map and flatMap on a connected data stream</p>
<p><img src="/flink%E7%AC%94%E8%AE%B0.assets/image-20211107232313476.png" alt="image-20211107232313476"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 合流 connect</span></span><br><span class="line"><span class="comment">// 测试两种不同类型的数据流合并</span></span><br><span class="line"><span class="comment">// 1. 将high流转换成二元组类型，与low流连接合并，输出信息</span></span><br><span class="line">SingleOutputStreamOperator&lt;Tuple2&lt;String, Double&gt;&gt; highTempStream = highStream.map(<span class="keyword">new</span> <span class="title class_">MapFunction</span>&lt;SensorReading, Tuple2&lt;String, Double&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Tuple2&lt;String, Double&gt; <span class="title function_">map</span><span class="params">(SensorReading value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(value.getId(), value.getTemperature());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 2. 使用connect算子合并</span></span><br><span class="line">ConnectedStreams&lt;Tuple2&lt;String, Double&gt;, SensorReading&gt; connectedStreams = highTempStream.connect(lowStream);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 合并两条流， connectedStream的map是分别对其中的stream执行map方法</span></span><br><span class="line">SingleOutputStreamOperator&lt;Object&gt; resultStream = connectedStreams.map(<span class="keyword">new</span> <span class="title class_">CoMapFunction</span>&lt;Tuple2&lt;String, Double&gt;, SensorReading, Object&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Object <span class="title function_">map1</span><span class="params">(Tuple2&lt;String, Double&gt; value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple3</span>&lt;String, Double, String&gt;(value.f0, value.f1, <span class="string">&quot;high temp warning&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Object <span class="title function_">map2</span><span class="params">(SensorReading value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(value.getId(), <span class="string">&quot;normal&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">// 打印</span></span><br><span class="line">resultStream.print();</span><br></pre></td></tr></table></figure>



<h3 id="3-9-Union"><a href="#3-9-Union" class="headerlink" title="3.9 Union"></a>3.9 Union</h3><h4 id="DataStream-→-DataStream"><a href="#DataStream-→-DataStream" class="headerlink" title="DataStream* → DataStream #"></a>DataStream* → DataStream <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/operators/overview/#datastream-rarr-datastream-3">#</a></h4><p>Union of two or more data streams creating a new stream containing all the elements from all the streams. Note: If you union a data stream with itself you will get each element twice in the resulting stream.</p>
<p><img src="/flink%E7%AC%94%E8%AE%B0.assets/image-20211108005213438.png" alt="image-20211108005213438"></p>
<p>Union操作的多个流必须是相同的数据类型。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// union 合并相同类型的多条流</span></span><br><span class="line">DataStream&lt;SensorReading&gt; unionStream = lowStream.union(allStream);</span><br><span class="line">unionStream.print(<span class="string">&quot;union&quot;</span>);</span><br></pre></td></tr></table></figure>

<h3 id="3-10-实现UDF函数–更细粒度的控制流"><a href="#3-10-实现UDF函数–更细粒度的控制流" class="headerlink" title="3.10 实现UDF函数–更细粒度的控制流"></a>3.10 实现UDF函数–更细粒度的控制流</h3><h4 id="1-函数类（Function-Classes）"><a href="#1-函数类（Function-Classes）" class="headerlink" title="1. 函数类（Function Classes）"></a>1. 函数类（Function Classes）</h4><p>​		Flink中暴露了所有的udf函数的接口（实现方式为接口或抽象类）。例如MapFunction,FilterFunction,ProcessFunction等等。</p>
<p>​		例如:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyMapFunction</span> <span class="keyword">implements</span> <span class="title class_">MapFunction</span>&lt;String, Integer&gt; &#123;</span><br><span class="line">  <span class="keyword">public</span> Integer <span class="title function_">map</span><span class="params">(String value)</span> &#123; <span class="keyword">return</span> Integer.parseInt(value); &#125;</span><br><span class="line">&#125;;</span><br><span class="line">data.map(<span class="keyword">new</span> <span class="title class_">MyMapFunction</span>());</span><br></pre></td></tr></table></figure>

<h4 id="2-匿名函数（Lambda-Functions）"><a href="#2-匿名函数（Lambda-Functions）" class="headerlink" title="2.匿名函数（Lambda Functions）"></a>2.匿名函数（Lambda Functions）</h4><p>​		例如:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data.map(<span class="keyword">new</span> <span class="title class_">MapFunction</span>&lt;String, Integer&gt; () &#123;</span><br><span class="line">  <span class="keyword">public</span> Integer <span class="title function_">map</span><span class="params">(String value)</span> &#123; <span class="keyword">return</span> Integer.parseInt(value); &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;SensorReading&gt; reduceResult = keyedStream.reduce((curState, newData) -&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">SensorReading</span>(</span><br><span class="line">                curState.getId(),</span><br><span class="line">                newData.getTimestamp(),</span><br><span class="line">                Math.max(curState.getTemperature(), newData.getTemperature())));</span><br></pre></td></tr></table></figure>

<h4 id="3-富函数（Rich-Functions）"><a href="#3-富函数（Rich-Functions）" class="headerlink" title="3.富函数（Rich Functions）"></a>3.富函数（Rich Functions）</h4><p>​		Rich functions provide, in addition to the user-defined function (map, reduce, etc), four methods: <code>open</code>, <code>close</code>, <code>getRuntimeContext</code>, and <code>setRuntimeContext</code>. These are useful for parameterizing the function (see <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/dataset/overview/#passing-parameters-to-functions">Passing Parameters to Functions</a>), creating and finalizing local state, accessing broadcast variables (see <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/dataset/overview/#broadcast-variables">Broadcast Variables</a>), and for accessing runtime information such as accumulators and counters (see <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/datastream/user_defined_functions/#accumulators--counters">Accumulators and Counters</a>), and information on iterations (see <a target="_blank" rel="noopener" href="https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/dataset/iterations/">Iterations</a>).</p>
<p>​		所有的Flink函数类都有Rich版本，它与常规函数的不同点在于，可以获取运行环境的上下文，并拥有一些生命周期的方法，所以可以实现一些复杂的功能。</p>
<p>典型的生命周期方法:</p>
<ul>
<li>open() 方法是RichFunction的初始方法，在一个的算子例如map或filter被调用之前open()方法就会被调用。</li>
<li>close() 方法是生命周期中最后被调用的方法，做一些清理工作。</li>
<li>getRuntimeContext()方法提供了RuntimeContext的一些信息，例如函数执行的并行度，任务名字，以及state状态</li>
<li>。。。</li>
</ul>
<p>向转换算子中传递参数</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">DataSet&lt;Integer&gt; toFilter = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="type">Configuration</span> <span class="variable">config</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">config.setInteger(<span class="string">&quot;limit&quot;</span>, <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">toFilter.filter(<span class="keyword">new</span> <span class="title class_">RichFilterFunction</span>&lt;Integer&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> limit;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">      limit = parameters.getInteger(<span class="string">&quot;limit&quot;</span>, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">filter</span><span class="params">(Integer value)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">      <span class="keyword">return</span> value &gt; limit;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;).withParameters(config);</span><br></pre></td></tr></table></figure>



<h2 id="4-Sink"><a href="#4-Sink" class="headerlink" title="4. Sink"></a>4. Sink</h2><h3 id="4-1-sink-kafka"><a href="#4-1-sink-kafka" class="headerlink" title="4.1 sink kafka"></a>4.1 sink kafka</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SinkTest1_Kafka</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 创建执行环境</span></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">// 从文本文件中读取数据</span></span><br><span class="line"><span class="comment">//        DataStreamSource&lt;String&gt; inputStream = env.readTextFile(&quot;src/main/resources/sensor.txt&quot;);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// kafka消费配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        properties.put(<span class="string">&quot;bootstrap.servers&quot;</span>,<span class="string">&quot;linux111:9092&quot;</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;group.id&quot;</span>,<span class="string">&quot;test&quot;</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;key.deserializer&quot;</span>,<span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;value.deserializer&quot;</span>,<span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        properties.put(<span class="string">&quot;auto.offset.reset&quot;</span>,<span class="string">&quot;latest&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置kafka connector，消费数据</span></span><br><span class="line">        DataStreamSource&lt;String&gt; inputStream = env.addSource(<span class="keyword">new</span> <span class="title class_">FlinkKafkaConsumer</span>&lt;String&gt;(<span class="string">&quot;test&quot;</span>, <span class="keyword">new</span> <span class="title class_">SimpleStringSchema</span>(), properties));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; dataStream = inputStream.map(v -&gt; &#123;</span><br><span class="line">            String[] fields = v.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">SensorReading</span>(fields[<span class="number">0</span>], Long.valueOf(fields[<span class="number">1</span>]), Double.valueOf(fields[<span class="number">2</span>])).toString();</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出到kafka</span></span><br><span class="line">        dataStream.addSink(<span class="keyword">new</span> <span class="title class_">FlinkKafkaProducer</span>&lt;String&gt;(<span class="string">&quot;linux111:9092,linux112:9092&quot;</span>, <span class="string">&quot;flinktest&quot;</span>, <span class="keyword">new</span> <span class="title class_">SimpleStringSchema</span>()));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 启动执行</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="4-2-sink-Redis"><a href="#4-2-sink-Redis" class="headerlink" title="4.2 sink Redis"></a>4.2 sink Redis</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SinkTest2_Redis</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 创建执行环境</span></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        <span class="comment">// 从文本文件中读取数据</span></span><br><span class="line">        DataStreamSource&lt;String&gt; inputStream = env.readTextFile(<span class="string">&quot;src/main/resources/sensor.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;SensorReading&gt; dataStream = inputStream.map(v -&gt; &#123;</span><br><span class="line">            String[] fields = v.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">SensorReading</span>(fields[<span class="number">0</span>], Long.valueOf(fields[<span class="number">1</span>]), Double.valueOf(fields[<span class="number">2</span>]));</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置redis的连接配置</span></span><br><span class="line">        <span class="type">FlinkJedisPoolConfig</span> <span class="variable">config</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlinkJedisPoolConfig</span>.Builder()</span><br><span class="line">                .setHost(<span class="string">&quot;linux111&quot;</span>)</span><br><span class="line">                .setPort(<span class="number">6379</span>)</span><br><span class="line">                .build();</span><br><span class="line">        <span class="comment">// 将RedisSink添加进流</span></span><br><span class="line">        dataStream.addSink(<span class="keyword">new</span> <span class="title class_">RedisSink</span>&lt;&gt;(config, <span class="keyword">new</span> <span class="title class_">MyRedisMapper</span>()));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 启动执行</span></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MyRedisMapper</span> <span class="keyword">implements</span> <span class="title class_">RedisMapper</span>&lt;SensorReading&gt;&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> RedisCommandDescription <span class="title function_">getCommandDescription</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="comment">// redis的命令操作， 使用hset 将数据存入&quot;sensor_temp&quot;表中，key为id, value为temperature</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">RedisCommandDescription</span>(RedisCommand.HSET, <span class="string">&quot;sensor_temp&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> String <span class="title function_">getKeyFromData</span><span class="params">(SensorReading sensorReading)</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> sensorReading.getId();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> String <span class="title function_">getValueFromData</span><span class="params">(SensorReading sensorReading)</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> sensorReading.getTemperature().toString();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h1 id="窗口–Window-API"><a href="#窗口–Window-API" class="headerlink" title="窗口–Window API"></a>窗口–Window API</h1><p>Aggregating events (e.g., counts, sums) works differently on streams than in batch processing. For example, it is impossible to count all elements in a stream, because streams are in general infinite (unbounded). Instead, aggregates on streams (counts, sums, etc), are scoped by <strong>windows</strong>, such as <em>“count over the last 5 minutes”</em>, or <em>“sum of the last 100 elements”</em>.</p>
<p>Windows can be <em>time driven</em> (example: every 30 seconds) or <em>data driven</em> (example: every 100 elements). One typically distinguishes different types of windows, such as <em>tumbling windows</em> (no overlap), <em>sliding windows</em> (with overlap), and <em>session windows</em> (punctuated by a gap of inactivity).</p>
<p><img src="/flink%E7%AC%94%E8%AE%B0.assets/image-20211108170754108.png" alt="image-20211108170754108"></p>
<p>Please check out this <a target="_blank" rel="noopener" href="https://flink.apache.org/news/2015/12/04/Introducing-windows.html">blog post</a> for additional examples of windows or take a look a <a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/operators/windows/">window documentation</a> of the DataStream API.</p>
<p>​		在无限的数据流上做聚合操作时，引入窗口的概念。可以设定一段时间对接收到的数据做聚合操作，也可以通过判断接受的数据量来决定是否开始聚合操作，在flink中这两种对数据的聚合操作分别称为: <strong>时间驱动</strong>和<strong>数据驱动</strong></p>
<p>参考链接: <a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/operators/windows/">https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/operators/windows/</a></p>
<h2 id="窗口类型"><a href="#窗口类型" class="headerlink" title="窗口类型"></a>窗口类型</h2><ul>
<li><p>时间窗口（Time Window）</p>
<ul>
<li>滚动时间窗口 （Tumbling Windows）</li>
<li>滑动时间窗口（Sliding Windows）</li>
<li>会话窗口（Session Windows）</li>
</ul>
</li>
<li><p>计数窗口（Count Window）</p>
<ul>
<li>滚动计数窗口</li>
<li>滑动计数窗口</li>
</ul>
</li>
</ul>
<h2 id="窗口的生命周期"><a href="#窗口的生命周期" class="headerlink" title="窗口的生命周期"></a>窗口的生命周期</h2><pre><code>      In a nutshell, a window is **created** as soon as the first element that should belong to this window arrives, and the window is **completely removed** when the time (event or processing time) passes its end timestamp plus the user-specified `allowed lateness` (see [Allowed Lateness](https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/operators/windows/#allowed-lateness)). Flink guarantees removal only for time-based windows and not for other types, *e.g.* global windows (see [Window Assigners](https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/operators/windows/#window-assigners)). For example, with an event-time-based windowing strategy that creates non-overlapping (or tumbling) windows every 5 minutes and has an allowed lateness of 1 min, Flink will create a new window for the interval between `12:00` and `12:05` when the first element with a timestamp that falls into this interval arrives, and it will remove it when the watermark passes the `12:06` timestamp.
</code></pre>
<p>​		当第一个元素传递过来时，窗口就被创建了，flink会在指定的时间窗口内对窗口内的数据进行计算，当到达窗口结束时间时，窗口就会被移除。</p>
<p>In addition, each window will have a <code>Trigger</code> (see <a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/operators/windows/#triggers">Triggers</a>) and a function (<code>ProcessWindowFunction</code>, <code>ReduceFunction</code>, or <code>AggregateFunction</code>) (see <a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/operators/windows/#window-functions">Window Functions</a>) attached to it. The function will contain the computation to be applied to the contents of the window, while the <code>Trigger</code> specifies the conditions under which the window is considered ready for the function to be applied. A triggering policy might be something like “when the number of elements in the window is more than 4”, or “when the watermark passes the end of the window”. A trigger can also decide to purge a window’s contents any time between its creation and removal. Purging in this case only refers to the elements in the window, and <em>not</em> the window metadata. This means that new data can still be added to that window.</p>
<p>​		每个窗口都会有一个触发器(Trigger)和计算函数(ProcessWindowFunction,ReduceFunction，AggregateFunction).</p>
<p>Apart from the above, you can specify an <code>Evictor</code> (see <a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/operators/windows/#evictors">Evictors</a>) which will be able to remove elements from the window after the trigger fires and before and&#x2F;or after the function is applied.</p>
<p>​		除了上面的触发器和函数，也可以使用Evictor方法来移除窗口中的元素, 例如带两个参数的countWindow方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Windows this &#123;<span class="doctag">@code</span> KeyedStream&#125; into sliding count windows.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> size The size of the windows in number of elements.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> slide The slide interval in number of elements.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> WindowedStream&lt;T, KEY, GlobalWindow&gt; <span class="title function_">countWindow</span><span class="params">(<span class="type">long</span> size, <span class="type">long</span> slide)</span> &#123;</span><br><span class="line">	<span class="keyword">return</span> window(GlobalWindows.create())</span><br><span class="line">			.evictor(CountEvictor.of(size))</span><br><span class="line">			.trigger(CountTrigger.of(slide));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>In the following we go into more detail for each of the components above. We start with the required parts in the above snippet (see <a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/operators/windows/#keyed-vs-non-keyed-windows">Keyed vs Non-Keyed Windows</a>, <a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/operators/windows/#window-assigner">Window Assigner</a>, and <a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/operators/windows/#window-function">Window Function</a>) before moving to the optional ones.</p>
<h2 id="窗口分配器-Window-Assigner"><a href="#窗口分配器-Window-Assigner" class="headerlink" title="窗口分配器  Window Assigner"></a>窗口分配器  Window Assigner</h2><p>​		使用window()方法来定义一个窗口，然后基于这个窗口做一些聚合或其他操作。window()方法必须在keyBy之后才可以使用。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Windows this data stream to a &#123;<span class="doctag">@code</span> WindowedStream&#125;, which evaluates windows</span></span><br><span class="line"><span class="comment"> * over a key grouped stream. Elements are put into windows by a &#123;<span class="doctag">@link</span> WindowAssigner&#125;. The</span></span><br><span class="line"><span class="comment"> * grouping of elements is done both by key and by window.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;A &#123;<span class="doctag">@link</span> org.apache.flink.streaming.api.windowing.triggers.Trigger&#125; can be defined to</span></span><br><span class="line"><span class="comment"> * specify when windows are evaluated. However, &#123;<span class="doctag">@code</span> WindowAssigners&#125; have a default</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@code</span> Trigger&#125; that is used if a &#123;<span class="doctag">@code</span> Trigger&#125; is not specified.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> assigner The &#123;<span class="doctag">@code</span> WindowAssigner&#125; that assigns elements to windows.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> The trigger windows data stream.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@PublicEvolving</span></span><br><span class="line"><span class="keyword">public</span> &lt;W <span class="keyword">extends</span> <span class="title class_">Window</span>&gt; WindowedStream&lt;T, KEY, W&gt; <span class="title function_">window</span><span class="params">(WindowAssigner&lt;? <span class="built_in">super</span> T, W&gt; assigner)</span> &#123;</span><br><span class="line">   <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">WindowedStream</span>&lt;&gt;(<span class="built_in">this</span>, assigner);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>窗口分配器WindowAssigner类负责将经过keyBy的数据分配到每个窗口中，分配器提供了触发器（Trigger）给WindowedStream，当满足窗口触发的条件时，就会触发定义在窗口上的操作开始执行。</p>
<p>例如滚动窗口分配器</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TumblingEventTimeWindows</span> <span class="keyword">extends</span> <span class="title class_">WindowAssigner</span>&lt;Object, TimeWindow&gt;&#123;</span><br><span class="line">...</span><br><span class="line">    </span><br><span class="line">    	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">public</span> Collection&lt;TimeWindow&gt; <span class="title function_">assignWindows</span><span class="params">(Object element, <span class="type">long</span> timestamp, WindowAssignerContext context)</span> &#123;</span><br><span class="line">		<span class="keyword">if</span> (timestamp &gt; Long.MIN_VALUE) &#123;</span><br><span class="line">			<span class="comment">// Long.MIN_VALUE is currently assigned when no timestamp is present</span></span><br><span class="line">			<span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span> TimeWindow.getWindowStartWithOffset(timestamp, offset, size);</span><br><span class="line">			<span class="keyword">return</span> Collections.singletonList(<span class="keyword">new</span> <span class="title class_">TimeWindow</span>(start, start + size));</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(<span class="string">&quot;Record has Long.MIN_VALUE timestamp (= no timestamp marker). &quot;</span> +</span><br><span class="line">					<span class="string">&quot;Is the time characteristic set to &#x27;ProcessingTime&#x27;, or did you forget to call &quot;</span> +</span><br><span class="line">					<span class="string">&quot;&#x27;DataStream.assignTimestampsAndWatermarks(...)&#x27;?&quot;</span>);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>方法assignWindows()负责划分窗口的起始和结束时间点，</p>
<p>在TimeWindow类中</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TimeWindow</span> <span class="keyword">extends</span> <span class="title class_">Window</span> &#123;</span><br><span class="line">...</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="type">long</span> <span class="title function_">getWindowStartWithOffset</span><span class="params">(<span class="type">long</span> timestamp, <span class="type">long</span> offset, <span class="type">long</span> windowSize)</span> &#123;</span><br><span class="line">   		<span class="keyword">return</span> timestamp - (timestamp - offset + windowSize) % windowSize;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到具体实现分窗的细节，上述代码中getWindowStartWithOffset方法返回了窗口的开始位置，即窗口大小的整数倍。</p>
<p>在window()方法中可以指定使用的窗口分配器，在flink中已定义好的窗口分配器包括: </p>
<p>滚动窗口(<em>tumbling windows</em>)</p>
<p>滑动窗口( <em>sliding windows</em>),</p>
<p>会话窗口( <em>session windows</em>) </p>
<p>全局窗口( <em>global windows</em>)</p>
<p><strong>滚动窗口 Tumbling Windows</strong></p>
<p>A <em>tumbling windows</em> assigner assigns each element to a window of a specified <em>window size</em>. Tumbling windows have a fixed size and do not overlap. For example, if you specify a tumbling window with a size of 5 minutes, the current window will be evaluated and a new window will be started every five minutes as illustrated by the following figure.</p>
<p><img src="/flink%E7%AC%94%E8%AE%B0.assets/tumbling-windows.svg" alt="Tumbling Windows"></p>
<p>数据按照固定的窗口长度对数据切分</p>
<p>时间对齐，窗口长度固定，没有重叠</p>
<p>使用方式</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;T&gt; input = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">// tumbling event-time windows</span></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// tumbling processing-time windows</span></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">5</span>)))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// daily tumbling event-time windows offset by -8 hours.</span></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.days(<span class="number">1</span>), Time.hours(-<span class="number">8</span>)))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br></pre></td></tr></table></figure>





<p><strong>滑动窗口 Sliding Windows</strong></p>
<p>The <em>sliding windows</em> assigner assigns elements to windows of fixed length. Similar to a tumbling windows assigner, the size of the windows is configured by the <em>window size</em> parameter. An additional <em>window slide</em> parameter controls how frequently a sliding window is started. Hence, sliding windows can be overlapping if the slide is smaller than the window size. In this case elements are assigned to multiple windows.</p>
<p>For example, you could have windows of size 10 minutes that slides by 5 minutes. With this you get every 5 minutes a window that contains the events that arrived during the last 10 minutes as depicted by the following figure.</p>
<p><img src="/flink%E7%AC%94%E8%AE%B0.assets/sliding-windows.svg" alt="sliding windows"></p>
<p>滑动窗口是固定窗口更广泛的一种形式，滑动窗口有固定窗口和滑动间隔组成</p>
<p>窗口的长度固定，数据可以有重叠</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;T&gt; input = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">// sliding event-time windows</span></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(SlidingEventTimeWindows.of(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">5</span>)))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// sliding processing-time windows</span></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(SlidingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">5</span>)))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// sliding processing-time windows offset by -8 hours</span></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(SlidingProcessingTimeWindows.of(Time.hours(<span class="number">12</span>), Time.hours(<span class="number">1</span>), Time.hours(-<span class="number">8</span>)))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br></pre></td></tr></table></figure>



<p><strong>会话窗口 Session Windows</strong></p>
<p>The <em>session windows</em> assigner groups elements by sessions of activity. Session windows do not overlap and do not have a fixed start and end time, in contrast to <em>tumbling windows</em> and <em>sliding windows</em>. Instead a session window closes when it does not receive elements for a certain period of time, <em>i.e.</em>, when a gap of inactivity occurred. A session window assigner can be configured with either a static <em>session gap</em> or with a <em>session gap extractor</em> function which defines how long the period of inactivity is. When this period expires, the current session closes and subsequent elements are assigned to a new session window.</p>
<p><img src="/flink%E7%AC%94%E8%AE%B0.assets/session-windows.svg" alt="session windows"></p>
<p>当超时仍未接收到消息时，会话窗口关闭。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;T&gt; input = ...;</span><br><span class="line"></span><br><span class="line"><span class="comment">// event-time session windows with static gap</span></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(EventTimeSessionWindows.withGap(Time.minutes(<span class="number">10</span>)))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line">    </span><br><span class="line"><span class="comment">// event-time session windows with dynamic gap</span></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(EventTimeSessionWindows.withDynamicGap((element) -&gt; &#123;</span><br><span class="line">        <span class="comment">// determine and return session gap</span></span><br><span class="line">    &#125;))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// processing-time session windows with static gap</span></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(ProcessingTimeSessionWindows.withGap(Time.minutes(<span class="number">10</span>)))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br><span class="line">    </span><br><span class="line"><span class="comment">// processing-time session windows with dynamic gap</span></span><br><span class="line">input</span><br><span class="line">    .keyBy(&lt;key selector&gt;)</span><br><span class="line">    .window(ProcessingTimeSessionWindows.withDynamicGap((element) -&gt; &#123;</span><br><span class="line">        <span class="comment">// determine and return session gap</span></span><br><span class="line">    &#125;))</span><br><span class="line">    .&lt;windowed transformation&gt;(&lt;window function&gt;);</span><br></pre></td></tr></table></figure>



<h3 id="全局窗口-Global-Windows"><a href="#全局窗口-Global-Windows" class="headerlink" title="全局窗口 Global Windows #"></a>全局窗口 Global Windows <a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/operators/windows/#global-windows">#</a></h3><p>A <em>global windows</em> assigner assigns all elements with the same key to the same single <em>global window</em>. This windowing scheme is only useful if you also specify a custom <a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/operators/windows/#triggers">trigger</a>. Otherwise, no computation will be performed, as the global window does not have a natural end at which we could process the aggregated elements.</p>
<p>​		全局窗口就是会收集所有的数据，使用全局窗口时，需要有用户定义好计算执行的触发器，否则全局窗口是不会做任何操作。</p>
<p><img src="/flink%E7%AC%94%E8%AE%B0.assets/non-windowed.svg" alt="global windows"></p>
<h2 id="窗口函数-Window-Function"><a href="#窗口函数-Window-Function" class="headerlink" title="窗口函数 Window Function"></a>窗口函数 Window Function</h2><ol>
<li><p>增来窗口聚合函数</p>
<p> ​	每来一条数据就进行处理，保持一个简单的状态</p>
<p> 例如: ReduceFunction，AggregatationFunction</p>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">        <span class="comment">// 增量聚合</span></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; resultStream = dataStream.keyBy(<span class="string">&quot;id&quot;</span>)</span><br><span class="line"><span class="comment">//                .countWindow(1000);  // 计数窗口</span></span><br><span class="line"><span class="comment">//                .timeWindow(Time.seconds(10))  // 时间窗口</span></span><br><span class="line">                .timeWindow(Time.seconds(<span class="number">10</span>))</span><br><span class="line">                .aggregate(<span class="keyword">new</span> <span class="title class_">AggregateFunction</span>&lt;SensorReading, Integer, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">                               <span class="comment">// 统计时间窗口中的记录个数，每来一条数据就做一次累加</span></span><br><span class="line"></span><br><span class="line">                               <span class="comment">// 简单记录key的id， 这里由于分区的字段只有一个，因此可以单独用变量记录，如果是按照多个字段分组的话，就会有多个记录的key</span></span><br><span class="line">                               <span class="keyword">private</span> String keyId;</span><br><span class="line"></span><br><span class="line">                               <span class="meta">@Override</span></span><br><span class="line">                               <span class="keyword">public</span> Integer <span class="title function_">createAccumulator</span><span class="params">()</span> &#123;</span><br><span class="line">                                   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                               &#125;</span><br><span class="line"></span><br><span class="line">                               <span class="meta">@Override</span></span><br><span class="line">                               <span class="keyword">public</span> Integer <span class="title function_">add</span><span class="params">(SensorReading value, Integer accumulator)</span> &#123;</span><br><span class="line">                                   keyId = value.getId();</span><br><span class="line">                                   <span class="keyword">return</span> accumulator + <span class="number">1</span>;</span><br><span class="line">                               &#125;</span><br><span class="line"></span><br><span class="line">                               <span class="meta">@Override</span></span><br><span class="line">                               <span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title function_">getResult</span><span class="params">(Integer accumulator)</span> &#123;</span><br><span class="line">                                   <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(keyId, accumulator);</span><br><span class="line">                               &#125;</span><br><span class="line"></span><br><span class="line">                               <span class="meta">@Override</span></span><br><span class="line">                               <span class="keyword">public</span> Integer <span class="title function_">merge</span><span class="params">(Integer a, Integer b)</span> &#123;</span><br><span class="line">                                   <span class="keyword">return</span> a + b;</span><br><span class="line">                               &#125;</span><br><span class="line">                           &#125;</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        resultStream.print(<span class="string">&quot;result&quot;</span>);</span><br></pre></td></tr></table></figure>



<p>​	2.全量窗口聚合函数</p>
<p>​			先把窗口所有数据收集起来，等到计算的时候再遍历处理所有数据</p>
<p>​		例如: ProcessWindowFunction， WindowFunction</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">        SingleOutputStreamOperator&lt;Tuple3&lt;String, Long, Integer&gt;&gt; resultStream2 = dataStream.keyBy(<span class="string">&quot;id&quot;</span>)</span><br><span class="line">                .timeWindow(Time.seconds(<span class="number">10</span>))</span><br><span class="line"><span class="comment">//                .process(new ProcessWindowFunction&lt;SensorReading, Object, Tuple, TimeWindow&gt;() &#123;</span></span><br><span class="line"><span class="comment">//                    @Override</span></span><br><span class="line"><span class="comment">//                    public void process(Tuple tuple, Context context, Iterable&lt;SensorReading&gt; elements, Collector&lt;Object&gt; out) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//                    &#125;</span></span><br><span class="line"><span class="comment">//                &#125;)</span></span><br><span class="line">                <span class="comment">// apply方法处理窗口里的所有数据，需要等窗口中的数据都到达或窗口结束时统一处理</span></span><br><span class="line">                <span class="comment">// 使用这种全局函数，能够获取的信息更多，使用上更加灵活些</span></span><br><span class="line">                .apply(<span class="keyword">new</span> <span class="title class_">WindowFunction</span>&lt;SensorReading, Tuple3&lt;String, Long, Integer&gt;, Tuple, TimeWindow&gt;() &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">apply</span><span class="params">(Tuple tuple, TimeWindow window, Iterable&lt;SensorReading&gt; input, Collector&lt;Tuple3&lt;String, Long, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="comment">// 计算每个key对应的数据在当前窗口中的个数</span></span><br><span class="line">                        <span class="type">String</span> <span class="variable">id</span> <span class="operator">=</span> tuple.getField(<span class="number">0</span>);</span><br><span class="line">                        <span class="type">long</span> <span class="variable">windowEnd</span> <span class="operator">=</span> window.getEnd();</span><br><span class="line">                        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> IteratorUtils.toList(input.iterator()).size();</span><br><span class="line">                        out.collect(<span class="keyword">new</span> <span class="title class_">Tuple3</span>&lt;&gt;(id, windowEnd, count));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        resultStream2.print(<span class="string">&quot;result2&quot;</span>);</span><br></pre></td></tr></table></figure>





<h1 id="时间语义"><a href="#时间语义" class="headerlink" title="时间语义"></a>时间语义</h1><ul>
<li>Event Time 事件创发生的时间</li>
<li>Ingestion Time 数据进入Flink的时间</li>
<li>Process Time 数据被Flink处理的时间，执行操作算子的时间，与机器系统的时间有关</li>
</ul>
<p>下面两段官方文档对处理时间(ProcessTime)和事件时间(EventTime)的说明</p>
<ul>
<li><p><strong>Processing time:</strong> Processing time refers to the system time of the machine that is executing the respective operation.</p>
<p>  程序各自执行处理操作时，本地机器的时间。</p>
<p>  When a streaming program runs on processing time, all time-based operations (like time windows) will use the system clock of the machines that run the respective operator. An hourly processing time window will include all records that arrived at a specific operator between the times when the system clock indicated the full hour. For example, if an application begins running at 9:15am, the first hourly processing time window will include events processed between 9:15am and 10:00am, the next window will include events processed between 10:00am and 11:00am, and so on.</p>
<p>  ProcessTime模式下，流处理程序所有基于时间的操作都是基于机器的时间系统来各自运行的，例如当设定时每小时的时间窗口，那么执行的时间会在每个整点时刻执行，当前一个小时内的所有数据，以此类推。</p>
<p>  Processing time is the simplest notion of time and requires no coordination between streams and machines. It provides the best performance and the lowest latency. However, in distributed and asynchronous environments processing time does not provide determinism, because it is susceptible to the speed at which records arrive in the system (for example from the message queue), to the speed at which the records flow between operators inside the system, and to outages (scheduled, or otherwise).</p>
<p>  在单机环境下使用ProcessTime模式是最简单的，但在分布式异步的处理环境中，由于数据到达每台机器的速率不同等原因(尤其是当数据是经过了消息队列收集)，是的数据处理的结果不准确。</p>
</li>
<li><p><strong>Event time:</strong> Event time is the time that each individual event occurred on its producing device. </p>
<p>  This time is typically embedded within the records before they enter Flink, and that <em>event timestamp</em> can be extracted from each record. In event time, the progress of time depends on the data, not on any wall clocks. Event time programs must specify how to generate <em>Event Time Watermarks</em>, which is the mechanism that signals progress in event time. This watermarking mechanism is described in a later section, <a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/concepts/time/#event-time-and-watermarks">below</a>.</p>
<p>  依据数据中事件发生的时间来划分流处理的时间窗口，这样就不会因为程序所在的机器时间不同步等原因导致处理结果不准确。</p>
<p>  In a perfect world, event time processing would yield completely consistent and deterministic results, regardless of when events arrive, or their ordering. However, unless the events are known to arrive in-order (by timestamp), event time processing incurs some latency while waiting for out-of-order events. As it is only possible to wait for a finite period of time, this places a limit on how deterministic event time applications can be.</p>
</li>
</ul>
<h2 id="事件时间模式"><a href="#事件时间模式" class="headerlink" title="事件时间模式"></a>事件时间模式</h2><p>​		要使用事件时间的模式，需要先指定时间特性为事件时间，可以如下设置：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// 指定使用EventTime模式, 当前使用的版本(1.11)中默认为ProcessTime模式</span></span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); </span><br></pre></td></tr></table></figure>

<p>​		要使用EventTime模式，就需要知道流中的时间是基于那些信息的来，不同于ProcessTime的时间来自于程序执行时的系统时间，EventTime使用的时间需要有用户自己指定，在Flink中提供了<strong>TimeAssigner</strong>接口及其实现类从接收到的数据中获得EventTime。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">        DataStreamSource&lt;String&gt; inputStream = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">7777</span>);</span><br><span class="line">      </span><br><span class="line">		SingleOutputStreamOperator&lt;SensorReading&gt; dataStream = inputStream.map(v -&gt; &#123;</span><br><span class="line">            String[] fields = v.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">SensorReading</span>(fields[<span class="number">0</span>], Long.valueOf(fields[<span class="number">1</span>]), Double.valueOf(fields[<span class="number">2</span>]));</span><br><span class="line">        &#125;)</span><br><span class="line">                <span class="comment">// 升序数据设置时间戳与水印</span></span><br><span class="line"><span class="comment">//                .assignTimestampsAndWatermarks(new AscendingTimestampExtractor&lt;SensorReading&gt;() &#123;</span></span><br><span class="line"><span class="comment">//                    @Override</span></span><br><span class="line"><span class="comment">//                    public long extractAscendingTimestamp(SensorReading element) &#123;</span></span><br><span class="line"><span class="comment">//                        return element.getTimestamp()*1000L;</span></span><br><span class="line"><span class="comment">//                    &#125;</span></span><br><span class="line"><span class="comment">//                &#125;)</span></span><br><span class="line">                <span class="comment">// 乱序数据设置时间戳与水印</span></span><br><span class="line">                .assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="title class_">BoundedOutOfOrdernessTimestampExtractor</span>&lt;SensorReading&gt;(Time.seconds(<span class="number">3</span>)) &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">extractTimestamp</span><span class="params">(SensorReading element)</span> &#123;</span><br><span class="line">                        <span class="keyword">return</span> element.getTimestamp() * <span class="number">1000L</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br></pre></td></tr></table></figure>

<p>BoundedOutOfOrdernessTimestampExtractor抽象类就是TimeAssigner的子类，上述代码中以匿名类的方式实现该抽象类，其中的extractTimestamp()方法指定了Event-Time的获取方式。</p>
<h2 id="水印，数据延迟"><a href="#水印，数据延迟" class="headerlink" title="水印，数据延迟"></a>水印，数据延迟</h2><h3 id="水印（-WaterMark）"><a href="#水印（-WaterMark）" class="headerlink" title="水印（ WaterMark）"></a>水印（ WaterMark）</h3><p><img src="/flink%E7%AC%94%E8%AE%B0.assets/image-20211118002519387.png" alt="image-20211118002519387"></p>
<p>​		从这段话中可以看到，所谓水印就是针对流中 延迟达到的数据，</p>
<p><img src="/flink%E7%AC%94%E8%AE%B0.assets/stream_watermark_in_order.svg" alt="A data stream with events (in order) and watermarks"></p>
<p><img src="/flink%E7%AC%94%E8%AE%B0.assets/stream_watermark_out_of_order.svg" alt="A data stream with events (out of order) and watermarks"></p>
<p>在乱序的数据流中注入水印数据，我们认为当读取到的水印数据时间为一个时间窗口的最后时刻时，则认为小于这个水印时间的数据都已经接收到窗口中，这时就会触发窗口计算执行。</p>
<p>​		上述代码中使用assignTimestampsAndWatermarks这个算子除了指定EventTime的取值，同时也指定了水印数据的生成。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="title class_">BoundedOutOfOrdernessTimestampExtractor</span>&lt;SensorReading&gt;(Time.seconds(<span class="number">3</span>)) &#123;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">extractTimestamp</span><span class="params">(SensorReading element)</span> &#123;</span><br><span class="line">                        <span class="keyword">return</span> element.getTimestamp() * <span class="number">1000L</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The (fixed) interval between the maximum seen timestamp seen in the records</span></span><br><span class="line"><span class="comment"> * and that of the watermark to be emitted.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> maxOutOfOrderness;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="title function_">BoundedOutOfOrdernessTimestampExtractor</span><span class="params">(Time maxOutOfOrderness)</span> &#123;</span><br><span class="line">   <span class="keyword">if</span> (maxOutOfOrderness.toMilliseconds() &lt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(<span class="string">&quot;Tried to set the maximum allowed &quot;</span> +</span><br><span class="line">         <span class="string">&quot;lateness to &quot;</span> + maxOutOfOrderness + <span class="string">&quot;. This parameter cannot be negative.&quot;</span>);</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="built_in">this</span>.maxOutOfOrderness = maxOutOfOrderness.toMilliseconds();</span><br><span class="line">   <span class="built_in">this</span>.currentMaxTimestamp = Long.MIN_VALUE + <span class="built_in">this</span>.maxOutOfOrderness;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">    	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">final</span> Watermark <span class="title function_">getCurrentWatermark</span><span class="params">()</span> &#123;</span><br><span class="line">		<span class="comment">// this guarantees that the watermark never goes backwards.</span></span><br><span class="line">		<span class="type">long</span> <span class="variable">potentialWM</span> <span class="operator">=</span> currentMaxTimestamp - maxOutOfOrderness;</span><br><span class="line">		<span class="keyword">if</span> (potentialWM &gt;= lastEmittedWatermark) &#123;</span><br><span class="line">			lastEmittedWatermark = potentialWM;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Watermark</span>(lastEmittedWatermark);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<p>这个构造方法中的参数就是当前记录的时间戳与水印对象的时间间隔。</p>
<p>由该类中的getCurrentWaterMark方法可以知道事件时间与水印时间之间的关系</p>
<p>水印时间&#x3D;事件时间-允许的延迟时间</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">    env.setParallelism(<span class="number">1</span>);</span><br><span class="line">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);  <span class="comment">// 使用EventTime模式, 默认为ProcessTime模式</span></span><br><span class="line">    DataStreamSource&lt;String&gt; inputStream = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">7777</span>);</span><br><span class="line">    SingleOutputStreamOperator&lt;SensorReading&gt; dataStream = inputStream.map(v -&gt; &#123;</span><br><span class="line">        String[] fields = v.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">SensorReading</span>(fields[<span class="number">0</span>], Long.valueOf(fields[<span class="number">1</span>]), Double.valueOf(fields[<span class="number">2</span>]));</span><br><span class="line">    &#125;)</span><br><span class="line">            <span class="comment">// 乱序数据设置时间戳与水印</span></span><br><span class="line">            .assignTimestampsAndWatermarks(<span class="keyword">new</span> <span class="title class_">BoundedOutOfOrdernessTimestampExtractor</span>&lt;SensorReading&gt;(Time.seconds(<span class="number">3</span>)) &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">extractTimestamp</span><span class="params">(SensorReading element)</span> &#123;</span><br><span class="line">                    <span class="keyword">return</span> element.getTimestamp() * <span class="number">1000L</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 基于事件时间的开窗测试</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 统计15秒内温度最小值</span></span><br><span class="line">    SingleOutputStreamOperator&lt;SensorReading&gt; minTempStream = dataStream.keyBy(SensorReading::getId)</span><br><span class="line">            .timeWindow(Time.seconds(<span class="number">15</span>))  <span class="comment">// 创建滚动窗口</span></span><br><span class="line">            .minBy(<span class="string">&quot;temperature&quot;</span>);  <span class="comment">// 取窗口中最小值</span></span><br><span class="line"></span><br><span class="line">    minTempStream.print(<span class="string">&quot;minTemp&quot;</span>);</span><br><span class="line">    env.execute();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这段代码样例中，采用事件时间的滚动窗口机制，窗口大小为15秒，乱序延迟为3秒。</p>
<p><img src="/flink%E7%AC%94%E8%AE%B0.assets/image-20211118015453141.png" alt="image-20211118015453141"></p>
<p>这个测试中，当输入图中最后一条数据时，Flink程序输出了第一条结果，根据水印时间的计算方式可知</p>
<p>水印时间&#x3D;1636272738-3&#x3D;1636272735， 此时由于触发了窗口操作执行，因此可知当前的时间窗口大小为[1636272720,1636272735)</p>
<p>在该时间段之间的最小值的数据为 sensor_1,1636272732,28.9，而该条数据是在1636272737时刻之后到来的，可以看到这里使用水印的机制，就能避免错过正确的结果。</p>
<h2 id="处理延迟数据-Late-Data"><a href="#处理延迟数据-Late-Data" class="headerlink" title="处理延迟数据 Late Data"></a>处理延迟数据 Late Data</h2><p>​		水印机制在处理乱序达到的事件数据时，确实能够帮助程序执行时尽可能完整的接收到需要处理的数据，但是对于已经执行完而关闭的窗口来说， 在这之后到达的数据却无法再次被处理。</p>
<p>对于迟到的事件，有以下考虑:</p>
<ol>
<li>直接将迟到太久的数据丢弃；</li>
<li>迟到的数据重定向到另外的一条流中，Flink中的Side-Output-Data；</li>
<li>将迟到的数据与本应所属窗口的结果一起更新操作，这种情况需要保存计算完的窗口结果状态</li>
</ol>
<p>在Flink中针对EventTime模式的处理，默认会丢弃迟到的数据。</p>
<p>如果是需要将延迟的数据重定向到另外的流中，可以使用SideOutputLateData算子,如下代码所示:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">		<span class="comment">// 给迟到的数据指定一个标签</span></span><br><span class="line">        OutputTag&lt;SensorReading&gt; outputTagLate = <span class="keyword">new</span> <span class="title class_">OutputTag</span>&lt;SensorReading&gt;(<span class="string">&quot;late&quot;</span>)&#123;&#125;;</span><br><span class="line">        SingleOutputStreamOperator&lt;SensorReading&gt; minTempStream = dataStream.keyBy(SensorReading::getId)</span><br><span class="line">                .timeWindow(Time.seconds(<span class="number">15</span>))  <span class="comment">// 创建滚动窗口</span></span><br><span class="line">                .allowedLateness(Time.seconds(<span class="number">60</span>)) <span class="comment">// 设置允许延迟到达的时间阈值</span></span><br><span class="line">                .sideOutputLateData(outputTagLate) <span class="comment">// 将延迟到达的数据打上标签</span></span><br><span class="line">                .minBy(<span class="string">&quot;temperature&quot;</span>);  <span class="comment">// 取窗口中最小值</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取指定标签上的数据</span></span><br><span class="line">        minTempStream.getSideOutput(outputTagLate)</span><br><span class="line"><span class="comment">//                .addSink(...)  // 可以sink到其他的位置存放</span></span><br><span class="line">                .print(<span class="string">&quot;late&quot;</span>);</span><br></pre></td></tr></table></figure>





<h1 id="流状态State"><a href="#流状态State" class="headerlink" title="流状态State"></a>流状态State</h1><p>​		什么是 state，流式计算的数据往往是转瞬即逝， 当然，真实业务场景不可能说所有的数据都是进来之后就走掉，没有任何东西留下来，那么留下来的东西其实就是称之为 state，中文可以翻译成状态。</p>
<p>参考文档: <a target="_blank" rel="noopener" href="https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/fault-tolerance/state/">https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/fault-tolerance/state/</a></p>
<h2 id="Keyed-State"><a href="#Keyed-State" class="headerlink" title="Keyed State"></a>Keyed State</h2><p>​		使用Keyed State之前需要先给DataStream指定一个key，用来将流划分到不同的分区，在每个分区中都会有一个分区状态。</p>
<p>​		使用keyBy算子将DataStream转换成KeyedStream。</p>
<p>​		Flink中的KeyedStream数据模型并不是基于key-value的键值对，所以不需要用户来指定键值类型。在该模型中，keys是<em>虚拟的</em>，它们由实际数据上的分组算子来定义。</p>
<blockquote>
<p>The data model of Flink is not based on key-value pairs. Therefore, you do not need to physically pack the data set types into keys and values. Keys are “virtual”: they are defined as functions over the actual data to guide the grouping operator.</p>
</blockquote>
<p>keyBy方法的使用如下:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">        SingleOutputStreamOperator&lt;SensorReading&gt; dataStream = inputStream.map(v -&gt; &#123;</span><br><span class="line">            String[] fields = v.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">SensorReading</span>(fields[<span class="number">0</span>], Long.valueOf(fields[<span class="number">1</span>]), Double.valueOf(fields[<span class="number">2</span>]));</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用key selector function来</span></span><br><span class="line">        KeyedStream&lt;SensorReading, String&gt; keyedStream = dataStream.keyBy(SensorReading::getId);</span><br><span class="line">        <span class="comment">// 使用旧版的keyBy(String... fields)算子返回的key类型就是个Tuple</span></span><br><span class="line"><span class="comment">//        KeyedStream&lt;SensorReading, Tuple&gt; keyedStream = dataStream.keyBy(&quot;id&quot;);</span></span><br></pre></td></tr></table></figure>





<ul>
<li><code>ValueState&lt;T&gt;</code>: This keeps a value that can be updated and retrieved (scoped to key of the input element as mentioned above, so there will possibly be one value for each key that the operation sees). The value can be set using <code>update(T)</code> and retrieved using <code>T value()</code>.</li>
<li><code>ListState&lt;T&gt;</code>: This keeps a list of elements. You can append elements and retrieve an <code>Iterable</code> over all currently stored elements. Elements are added using <code>add(T)</code> or <code>addAll(List&lt;T&gt;)</code>, the Iterable can be retrieved using <code>Iterable&lt;T&gt; get()</code>. You can also override the existing list with <code>update(List&lt;T&gt;)</code></li>
<li><code>ReducingState&lt;T&gt;</code>: This keeps a single value that represents the aggregation of all values added to the state. The  interface is similar to <code>ListState</code> but elements added using <code>add(T)</code> are reduced to an aggregate using a specified <code>ReduceFunction</code>.</li>
<li><code>AggregatingState&lt;IN, OUT&gt;</code>: This keeps a single value that represents the aggregation of all values added to the state. Contrary to <code>ReducingState</code>, the aggregate type may be different from the type of elements that are added to the state. The interface is the same as for <code>ListState</code> but elements added using <code>add(IN)</code> are aggregated using a specified <code>AggregateFunction</code>.</li>
<li><code>MapState&lt;UK, UV&gt;</code>: This keeps a list of mappings. You can put key-value pairs into the state and retrieve an <code>Iterable</code> over all currently stored mappings. Mappings are added using <code>put(UK, UV)</code> or <code>putAll(Map&lt;UK, UV&gt;)</code>. The value associated with a user key can be retrieved using <code>get(UK)</code>. The iterable views for mappings, keys and values can be retrieved using <code>entries()</code>, <code>keys()</code> and <code>values()</code> respectively. You can also use <code>isEmpty()</code> to check whether this map contains any key-value mappings.</li>
</ul>
<p>以上类型的状态都有相同的方法clear()用来清除当前key对应的状态。</p>
<h2 id="Operator-State"><a href="#Operator-State" class="headerlink" title="Operator State"></a>Operator State</h2><p>​		在Flink中，状态始终与特定算子相关联，</p>
<p>Operator State 算子状态</p>
<p>算子状态限定为算子任务，由同一并行任务所处理的所有数据都可以访问到相同的状态</p>
<p>状态对于同一子任务是共享的</p>
<p>算子状态不能由相同或不同的算子的另一个子任务访问</p>
<h2 id="Broadcast-State"><a href="#Broadcast-State" class="headerlink" title="Broadcast State"></a>Broadcast State</h2><h1 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h1><ul>
<li>一致性检查点</li>
<li>从检查点恢复状态</li>
<li>Flink检查点算法</li>
<li>保存点（Save Points）</li>
</ul>
<p>Flink故障恢复机制的核心，就是应用状态的一致性检查点</p>
<p>有状态流应用的一致性检查点，就是所有任务的状态，在摸一个时间点的一份拷贝（快照）；在这个时间点，应该是所有的任务陡峭好处理完一个相同的输入数据的时候。</p>
<p>错误重启策略:</p>
<p>不重启</p>
<p>固定频率重启</p>
<p>失败率重启</p>
<p>保存点（Savepoint）</p>
<ul>
<li><p>Flink提供可以自定义的镜像保存功能，就是保存点。</p>
</li>
<li><p>原则上，创建保存点使用的算法与检查点完全相同，因此保存点可以认为是具有一些额外元数据的检查点。</p>
</li>
<li><p>Flink不会自动创建保存点，因此用户(或外部调用程序)必须明确地触发创建操作</p>
</li>
<li><p>保存点是一个强大的功能，除了故障恢复外，</p>
<p>  保存点可以用于：</p>
<ul>
<li><p>有计划的手动备份  程序运行到一定程度的时候，可以手动备份下比较重要的结果</p>
</li>
<li><p>更新应用程序   更新程序后，重新从保存点开始继续执行，而不需要在更新之后重新计算</p>
</li>
<li><p>版本迁移 例如升级Flink版本，程序版本等</p>
</li>
<li><p>暂停和重重启应用</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.enableCheckpointing(<span class="type">long</span> interval))</span><br></pre></td></tr></table></figure>





<h1 id="Table-API-Table-SQL"><a href="#Table-API-Table-SQL" class="headerlink" title="Table API &amp; Table SQL"></a>Table API &amp; Table SQL</h1><h2 id="入门代码样例"><a href="#入门代码样例" class="headerlink" title="入门代码样例"></a>入门代码样例</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableTest1_Example</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStream&lt;String&gt; inputStream = env.readTextFile(<span class="string">&quot;src/main/resources/sensor.txt&quot;</span>);</span><br><span class="line">        SingleOutputStreamOperator&lt;SensorReading&gt; dataStream = inputStream.map(v -&gt; &#123;</span><br><span class="line">            String[] fields = v.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">SensorReading</span>(fields[<span class="number">0</span>], Long.valueOf(fields[<span class="number">1</span>]), Double.valueOf(fields[<span class="number">2</span>]));</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建表环境</span></span><br><span class="line">        <span class="type">StreamTableEnvironment</span> <span class="variable">tableEnv</span> <span class="operator">=</span> StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 通过流创建建一张表</span></span><br><span class="line">        <span class="type">Table</span> <span class="variable">dataTable</span> <span class="operator">=</span> tableEnv.fromDataStream(dataStream);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用Table API进行算子操作</span></span><br><span class="line">        <span class="type">Table</span> <span class="variable">resultTable</span> <span class="operator">=</span> dataTable.select(<span class="string">&quot;id, temperature&quot;</span>).where(<span class="string">&quot;id = &#x27;sensor_1&#x27;&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用Table SQL进行操作</span></span><br><span class="line">        <span class="comment">// 创建一张表</span></span><br><span class="line">        tableEnv.createTemporaryView(<span class="string">&quot;sensor&quot;</span>, dataStream);</span><br><span class="line">        <span class="type">String</span> <span class="variable">sql</span> <span class="operator">=</span> <span class="string">&quot;select id, temperature from sensor where id = &#x27;sensor_1&#x27;&quot;</span>;</span><br><span class="line">        <span class="type">Table</span> <span class="variable">sqlResult</span> <span class="operator">=</span> tableEnv.sqlQuery(sql);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将Table转换成流stream</span></span><br><span class="line">        tableEnv.toAppendStream(resultTable, Row.class).print(<span class="string">&quot;result&quot;</span>);</span><br><span class="line">        tableEnv.toAppendStream(sqlResult,Row.class).print(<span class="string">&quot;sql&quot;</span>);</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<p>输出到文件</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableTest3_FileOutput</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 1. 创建环境</span></span><br><span class="line">        <span class="type">StreamExecutionEnvironment</span> <span class="variable">env</span> <span class="operator">=</span> StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamTableEnvironment</span> <span class="variable">tableEnv</span> <span class="operator">=</span> StreamTableEnvironment.create(env);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 表的创建：连接外部系统，读取数据</span></span><br><span class="line">        <span class="comment">// 读取文件</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">filePath</span> <span class="operator">=</span> <span class="string">&quot;src\\main\\resources\\sensor.txt&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 注册定义的表</span></span><br><span class="line">        tableEnv.connect( <span class="keyword">new</span> <span class="title class_">FileSystem</span>().path(filePath))</span><br><span class="line">                .withFormat( <span class="keyword">new</span> <span class="title class_">Csv</span>())</span><br><span class="line">                .withSchema( <span class="keyword">new</span> <span class="title class_">Schema</span>()</span><br><span class="line">                        .field(<span class="string">&quot;id&quot;</span>, DataTypes.STRING())</span><br><span class="line">                        .field(<span class="string">&quot;timestamp&quot;</span>, DataTypes.BIGINT())</span><br><span class="line">                        .field(<span class="string">&quot;temp&quot;</span>, DataTypes.DOUBLE())</span><br><span class="line">                )</span><br><span class="line">                .createTemporaryTable(<span class="string">&quot;inputTable&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">Table</span> <span class="variable">inputTable</span> <span class="operator">=</span> tableEnv.from(<span class="string">&quot;inputTable&quot;</span>);</span><br><span class="line"><span class="comment">//        inputTable.printSchema();</span></span><br><span class="line"><span class="comment">//        tableEnv.toAppendStream(inputTable, Row.class).print(&quot;inputTable&quot;);</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 查询转换</span></span><br><span class="line">        <span class="comment">// 3.1 Table API</span></span><br><span class="line">        <span class="comment">// 简单转换</span></span><br><span class="line">        <span class="type">Table</span> <span class="variable">resultTable</span> <span class="operator">=</span> inputTable.select(<span class="string">&quot;id, temp&quot;</span>)</span><br><span class="line">                .filter(<span class="string">&quot;id === &#x27;sensor_6&#x27;&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 聚合统计</span></span><br><span class="line">        <span class="type">Table</span> <span class="variable">aggTable</span> <span class="operator">=</span> inputTable.groupBy(<span class="string">&quot;id&quot;</span>)</span><br><span class="line">                .select(<span class="string">&quot;id, id.count as count, temp.avg as avgTemp&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.2 SQL</span></span><br><span class="line">        <span class="type">Table</span> <span class="variable">sqlResult</span> <span class="operator">=</span> tableEnv.sqlQuery(<span class="string">&quot;select id, temp from inputTable where id = &#x27;sensor_6&#x27;&quot;</span>);</span><br><span class="line">        <span class="type">Table</span> <span class="variable">sqlAggTable</span> <span class="operator">=</span> tableEnv.sqlQuery(<span class="string">&quot;select id, count(id) as cnt, avg(temp) as avgTemp from inputTable group by id&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 输出到文件</span></span><br><span class="line">        <span class="comment">// 连接外部文件注册输出表</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">outputPath</span> <span class="operator">=</span> <span class="string">&quot;src\\main\\resources\\out.txt&quot;</span>;</span><br><span class="line">        tableEnv.connect( <span class="keyword">new</span> <span class="title class_">FileSystem</span>().path(outputPath))</span><br><span class="line">                .withFormat( <span class="keyword">new</span> <span class="title class_">Csv</span>())</span><br><span class="line">                .withSchema( <span class="keyword">new</span> <span class="title class_">Schema</span>()</span><br><span class="line">                        .field(<span class="string">&quot;id&quot;</span>, DataTypes.STRING())</span><br><span class="line"><span class="comment">//                        .field(&quot;cnt&quot;, DataTypes.BIGINT())</span></span><br><span class="line">                        .field(<span class="string">&quot;temperature&quot;</span>, DataTypes.DOUBLE())</span><br><span class="line">                )</span><br><span class="line">                .createTemporaryTable(<span class="string">&quot;outputTable&quot;</span>);</span><br><span class="line"></span><br><span class="line">        tableEnv.toAppendStream(resultTable, Row.class).print(<span class="string">&quot;resultTable&quot;</span>);</span><br><span class="line">        resultTable.executeInsert(<span class="string">&quot;outputTable&quot;</span>);</span><br><span class="line"><span class="comment">//        aggTable.insertInto(&quot;outputTable&quot;);</span></span><br><span class="line"><span class="comment">//        tableEnv.toRetractStream(aggTable, Row.class).print();</span></span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>aggTable.insertInto(“outputTable”)这行代码会报错</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; org.apache.flink.table.api.TableException: AppendStreamTableSink doesn&#x27;t support consuming update changes which is produced by node GroupAggregate(groupBy=[id], select=[id, COUNT(id) AS cnt, AVG(temp) AS avgTemp])</span><br><span class="line">	at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.createNewNode(FlinkChangelogModeInferenceProgram.scala:357)</span><br><span class="line">	at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.visit(FlinkChangelogModeInferenceProgram.scala:174)</span><br><span class="line">	at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.visitChild(FlinkChangelogModeInferenceProgram.scala:314)</span><br><span class="line">	at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.$anonfun$visitChildren$1(FlinkChangelogModeInferenceProgram.scala:303)</span><br><span class="line">	at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyModifyKindSetTraitVisitor.$anonfun$visitChildren$1$adapted(FlinkChangelogModeInferenceProgram.scala:302)</span><br><span class="line">	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)</span><br><span class="line">	at scala.collection.immutable.Range.foreach(Range.scala:155)</span><br><span class="line">	...</span><br></pre></td></tr></table></figure>

<p>这段代码为什么会报错?</p>
<p>它具体的操作是需要对流中的数据做聚合操作然后更新计算结果，而流中的数据是无法回撤更新的，例如接收到同一个key的第二条数据时，计算的结果会发生变化，但却不能再回去修改了。</p>
<p>使用下面这个方法结果为一个二元组，它的第一个元素表示这条记录是否需要撤回</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tableEnv.toRetractStream(aggTable, Row.class).print();</span><br></pre></td></tr></table></figure>

<p><img src="/flink%E7%AC%94%E8%AE%B0.assets/image-20211208011201234.png" alt="image-20211208011201234"></p>
<p>第一次接收到sensor_4时，计算结果为(true, sensor_4,35.2)；</p>
<p>第二次接收到sensor_4时，需要更新计算结果哦，于是会将上次的结果变为(false, sensor_4,35.2)，表示这条记录需要回撤，并且给出新的计算结果(true,sensor_4,2,34.900000000000006)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/flink/flink%E7%AC%94%E8%AE%B0/" data-id="clumsd6x50007abr72zpc8w4a" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-docker/Docker部署使用mongo" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/04/05/docker/Docker%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8mongo/" class="article-date">
  <time class="dt-published" datetime="2024-04-05T14:53:22.840Z" itemprop="datePublished">2024-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Docker部署使用mongo"><a href="#Docker部署使用mongo" class="headerlink" title="Docker部署使用mongo"></a>Docker部署使用mongo</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 拉取mongo镜像</span><br><span class="line">docker pull mongo</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 运行容器</span><br><span class="line"># -v 指定挂载到镜像中的本地目录</span><br><span class="line"># -d 后台运行</span><br><span class="line">#  docker run --name 自定义名称 -v ~/docker/mongo:/data/db -p 27017:27017 -d 镜像名称</span><br><span class="line">docker run --name mongodb -v ~/docker/mongo:/data/db -p 27017:27017 -d mongo</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 进入容器</span><br><span class="line"># -it 交互方式</span><br><span class="line">docker exec -it mongodb bash</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/04/05/docker/Docker%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8mongo/" data-id="clumsd6x40003abr7fim89dmo" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/2/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/4/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">April 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/04/05/%E6%95%B0%E4%BB%93%E7%BB%8F%E9%AA%8C/%E9%9D%A2%E8%AF%95%E9%A2%98/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/04/05/zookeeper/Zookeeper/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/04/05/zookeeper/%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA(HA)/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/04/05/spark/spark%E7%BC%96%E7%A8%8B-RDD%E9%AB%98%E9%98%B6/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/04/05/spark/spark%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>